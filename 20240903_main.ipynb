{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to analyse the whole database of videos and process it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import json\n",
    "import imageio\n",
    "import re\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import colorsys\n",
    "import aiofiles\n",
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from scenedetect import VideoManager, SceneManager\n",
    "from scenedetect.detectors import ContentDetector\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load OpenAI API key from .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Apply nest_asyncio to handle the running event loop\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Concurrency limit\n",
    "semaphore = asyncio.Semaphore(5)\n",
    "\n",
    "# A dictionary to store characters across frames\n",
    "character_frames = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Video Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_video(video_path, threshold=27.0):\n",
    "    if not os.path.exists(video_path):\n",
    "        raise FileNotFoundError(f\"The video file {video_path} does not exist.\")\n",
    "    \n",
    "    video_manager = VideoManager([video_path])\n",
    "    scene_manager = SceneManager()\n",
    "    scene_manager.add_detector(ContentDetector(threshold=threshold))\n",
    "\n",
    "    video_manager.set_downscale_factor()\n",
    "    video_manager.start()\n",
    "\n",
    "    scene_manager.detect_scenes(frame_source=video_manager)\n",
    "    scene_list = scene_manager.get_scene_list()\n",
    "\n",
    "    video_manager.release()\n",
    "\n",
    "    logging.info(f'Detected {len(scene_list)} scenes:')\n",
    "    for i, scene in enumerate(scene_list):\n",
    "        logging.info(f'Scene {i + 1}: Start {scene[0].get_timecode()} / Frame {scene[0].get_frames()}, '\n",
    "              f'End {scene[1].get_timecode()} / Frame {scene[1].get_frames()}')\n",
    "\n",
    "    return scene_list\n",
    "\n",
    "def get_video_length(video_path):\n",
    "    # You can use a tool like OpenCV, ffmpeg, or similar to calculate video length\n",
    "    import cv2\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_length = frame_count / fps\n",
    "    cap.release()\n",
    "    return video_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Frame Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames_imageio(video_path, scenes, output_dir):\n",
    "    reader = imageio.get_reader(video_path)\n",
    "    for i, scene in enumerate(scenes):\n",
    "        start_frame, end_frame = scene\n",
    "        \n",
    "        # Convert FrameTimecode to integer frame numbers\n",
    "        start_frame_num = int(start_frame)\n",
    "        end_frame_num = int(end_frame)\n",
    "        \n",
    "        # Calculate the middle frame of the scene\n",
    "        middle_frame = (start_frame_num + end_frame_num) // 2\n",
    "        \n",
    "        # Set the reader to the middle frame and extract it\n",
    "        reader.set_image_index(middle_frame)\n",
    "        frame = reader.get_next_data()\n",
    "        \n",
    "        # Save the frame as an image with frame number in the filename\n",
    "        output_path = os.path.join(output_dir, f'scene_{i + 1}_frame_{middle_frame}.jpg')\n",
    "        imageio.imwrite(output_path, frame)\n",
    "        print(f\"Extracted and saved middle frame of scene {i + 1} as {output_path}\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Image Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def encode_image(image_path):\n",
    "    async with aiofiles.open(image_path, \"rb\") as image_file:\n",
    "        content = await image_file.read()\n",
    "        return base64.b64encode(content).decode('utf-8')\n",
    "\n",
    "def get_color_category(color):\n",
    "    r, g, b = [x / 255.0 for x in color]\n",
    "    h, l, s = colorsys.rgb_to_hls(r, g, b)\n",
    "\n",
    "    primary_hues = {\n",
    "        \"red\": (0.0, 0.1),  \n",
    "        \"yellow\": (0.1, 0.18),\n",
    "        \"green\": (0.25, 0.4),\n",
    "        \"blue\": (0.55, 0.75),\n",
    "    }\n",
    "\n",
    "    for color_name, hue_range in primary_hues.items():\n",
    "        if hue_range[0] <= h <= hue_range[1]:\n",
    "            return color_name\n",
    "\n",
    "    if (l >= 0.9 and s <= 0.1):\n",
    "        return \"white\"\n",
    "    if (l <= 0.1 and s <= 0.1):\n",
    "        return \"black\"\n",
    "\n",
    "    return \"non-primary\"\n",
    "\n",
    "def analyze_image_colors(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert('RGB')\n",
    "    data = np.array(image)\n",
    "\n",
    "    unique_colors, counts = np.unique(data.reshape(-1, data.shape[2]), axis=0, return_counts=True)\n",
    "    total_pixels = int(counts.sum())\n",
    "\n",
    "    color_counts = {\n",
    "        \"Red\": 0,\n",
    "        \"Yellow\": 0,\n",
    "        \"Green\": 0,\n",
    "        \"Blue\": 0,\n",
    "        \"White\": 0,\n",
    "        \"Black\": 0,\n",
    "        \"Non-primary\": 0\n",
    "    }\n",
    "\n",
    "    for color, count in zip(unique_colors, counts):\n",
    "        category = get_color_category(tuple(color))\n",
    "        color_counts[category.capitalize()] += int(count)\n",
    "\n",
    "    color_percentages = {color: (count / total_pixels) * 100 for color, count in color_counts.items()}\n",
    "    primary_total = color_counts[\"Red\"] + color_counts[\"Yellow\"] + color_counts[\"Blue\"]\n",
    "    color_dominance = \"Primary colors\" if primary_total > color_counts[\"Non-primary\"] else \"Non-primary colors\"\n",
    "\n",
    "    return {\n",
    "        \"Color Analysis\": {\n",
    "            \"Colors Found\": {\n",
    "                color: {\n",
    "                    \"Pixel Count\": count,\n",
    "                    \"Percentage\": f\"{color_percentages[color]:.2f}%\"\n",
    "                } for color, count in color_counts.items()\n",
    "            },\n",
    "            \"Dominance\": color_dominance\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) OpenAI API Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def send_image_to_openai(image_path, base64_image, retries=3):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"\"\"\n",
    "                        Analyze the following image and provide a detailed description in the format of JSON only. Ensure the output is strictly in JSON format without any additional text or code block formatting. The JSON should include the following standardized labels:\n",
    "\n",
    "                        1. **Image Analysis**: The root dictionary containing all analysis data.\n",
    "                        \n",
    "                        2. **Suitability**:\n",
    "                            - \"Nudity\": Boolean indicating the presence of nudity.\n",
    "                            - \"Obscene Gestures\": Boolean indicating the presence of obscene gestures.\n",
    "                            - \"Alcohol\": Boolean indicating the presence of alcohol.\n",
    "                            - \"Drugs\": Boolean indicating the presence of drugs.\n",
    "                            - \"Addictions\": Boolean indicating the presence of addictions.\n",
    "\n",
    "                        3. **Objects**:\n",
    "                            - \"Total Objects Identified\": Integer representing the total number of objects identified.\n",
    "                            - \"Average Features Per Object\": Float representing the average number of features per object.\n",
    "                            - \"Objects Details\": Dictionary containing details of each object, where each object is labeled as \"Object_1\", \"Object_2\", etc., with the following structure:\n",
    "                                - \"Name\": The name of the object - as simplest and descriptive mossible.\n",
    "                                - \"Portion Boolean\": 0-1 output indicating if the object is a portion of a larger object (1) or a complete object (0). For example, a leg is a portion of a human. However, if the object is just cropped but clearly identifiable as a complete object, it should be considered a complete object.\n",
    "                                - \"Color\": The color of the object.\n",
    "                                - \"Features\": List of features of the object.\n",
    "                                - \"Total Features\": Integer representing the number of features for the object.\n",
    "\n",
    "                        4. **Place**:\n",
    "                            - \"Name\": The name of the place - as simplest and descriptive mossible.\n",
    "                            - \"Certainty Boolean\": 0-1 output indicating if the place is clearly identifiable (1) or not (0).\n",
    "                            - \"Fantasy/Adventurous Place\": Boolean (0-1) indicating whether the place is classified as a fantasy/adventurous place or not.\n",
    "                            - \"Explanation\": Detailed explanation of why the place is classified as fantasy/adventurous or not. Fantasy places are those that do not exist in reality, and adventurous places are defined as those involving clear statements of traveling to space or another country.\n",
    "\n",
    "                        5. **Characters**:\n",
    "                            - \"Total Characters Identified\": Integer representing the total number of characters identified.\n",
    "                            - \"Average Features Per Character\": Float representing the average number of features per character.\n",
    "                            - \"Character Details\": Dictionary containing details of each character, where each character is labeled as \"Character_1\", \"Character_2\", etc., with the following structure:\n",
    "                                - \"Name\": The name of the character - as simplest and descriptive mossible.\n",
    "                                - \"Portion Boolean\": 0-1 output indicating if the character is a portion of a larger character (1) or a complete character (0). For example, a leg is a portion of a human. However, if the character is just cropped but clearly identifiable as a complete character, it should be considered a complete character.\n",
    "                                - \"Human or Non-Human\": 0-1 output indicating if the character appears human (1) or non-human (0). Anthropomorphized characters or any other combination not fully human are considered non-human.\n",
    "                                - \"Physical Features\": List of physical features of the character.\n",
    "                                - \"Explanation\": Explanation for why the character is classified as human or non-human, and why these physical features are inferred.\n",
    "                                - \"Age\": Expected age range of the character (a single number).\n",
    "                            **Note**: If the \"character\" consists of only a part of a body (such as a hand, leg, or face without enough distinguishing features to identify it as a complete character), do not count it as a \"character.\"\n",
    "\n",
    "                        Ensure that the structure of the JSON output strictly adheres to these standardized labels.\n",
    "                        \"\"\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 750\n",
    "    }\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload) as response:\n",
    "                    # Log the status code and full response for debugging\n",
    "                    status = response.status\n",
    "                    response_text = await response.text()\n",
    "                    \n",
    "                    # print(f\"Response Status Code: {status}\")\n",
    "                    # print(f\"Response Content: {response_text}\")\n",
    "\n",
    "                    if status == 429:\n",
    "                        print(\"Rate limit exceeded, retrying...\")\n",
    "                        await asyncio.sleep(2 ** attempt)\n",
    "                        continue\n",
    "                    elif status == 200:\n",
    "                        content = await response.json()\n",
    "                        \n",
    "                        # Log the full JSON content\n",
    "                        # print(f\"Full JSON Response for {image_path}: {content}\")\n",
    "                        \n",
    "                        if 'choices' in content:\n",
    "                            message_content = content['choices'][0].get('message', {}).get('content', '').strip()\n",
    "                            try:\n",
    "                                return json.loads(message_content)\n",
    "                            except json.JSONDecodeError as e:\n",
    "                                print(f\"Error decoding JSON from OpenAI response for {image_path}: {e}\")\n",
    "                                # print(f\"OpenAI Response Content: {message_content}\")\n",
    "                                return None\n",
    "                        else:\n",
    "                            print(f\"Unexpected response format from OpenAI API for {image_path}.\")\n",
    "                            return None\n",
    "                    else:\n",
    "                        print(f\"Request failed with status code {status} for {image_path}.\")\n",
    "                        # print(f\"Response Content: {response_text}\")\n",
    "                        return None\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Request failed due to a client error: {e}\")\n",
    "            await asyncio.sleep(2 ** attempt)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error occurred: {e}\")\n",
    "            await asyncio.sleep(2 ** attempt)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Scene Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_scenes_output(output_dir, json_output_dir):\n",
    "    os.makedirs(json_output_dir, exist_ok=True)\n",
    "    scenes = sorted([f for f in os.listdir(output_dir) if f.endswith('.jpg')], key=extract_scene_number)\n",
    "    total_scenes = len(scenes)\n",
    "    with tqdm(total=total_scenes, desc=\"Processing Scenes\", unit=\"scene\") as pbar:\n",
    "        tasks = [process_single_scene(i, scene, output_dir, json_output_dir, pbar) for i, scene in enumerate(scenes)]\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "async def process_single_scene(i, scene, output_dir, json_output_dir, pbar):\n",
    "    async with semaphore:  # Limit concurrent execution\n",
    "        scene_path = os.path.join(output_dir, scene)\n",
    "\n",
    "        # Encode image in base64\n",
    "        base64_image = await encode_image(scene_path)\n",
    "\n",
    "        # Perform color analysis\n",
    "        color_analysis_result = analyze_image_colors(scene_path)\n",
    "\n",
    "        # Send image to OpenAI for further analysis\n",
    "        openai_response = await send_image_to_openai(scene_path, base64_image)\n",
    "\n",
    "        # Check if openai_response is valid (not None or empty)\n",
    "        if not openai_response:\n",
    "            print(f\"Skipping {scene} due to invalid OpenAI response.\")\n",
    "            pbar.update(1)\n",
    "            return\n",
    "\n",
    "        # Combine both results, and include the reference to the image file\n",
    "        final_output = {\n",
    "            \"Image File\": scene,\n",
    "            \"Image Analysis\": {\n",
    "                **color_analysis_result[\"Color Analysis\"],\n",
    "                **openai_response.get(\"Image Analysis\", {})\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # The filename already includes the scene number and frame number\n",
    "        output_filename = os.path.splitext(scene)[0] + '_analysis.json'\n",
    "        output_path = os.path.join(json_output_dir, output_filename)\n",
    "\n",
    "        try:\n",
    "            async with aiofiles.open(output_path, 'w') as json_file:\n",
    "                await json_file.write(json.dumps(final_output, indent=4))\n",
    "                print(f\"Saved analysis for {scene} as {output_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save analysis for {scene}: {e}\")\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "def extract_scene_number(filename):\n",
    "    match = re.search(r'\\d+', filename)\n",
    "    return int(match.group()) if match else -1\n",
    "\n",
    "def extract_frame_number(filename):\n",
    "    match = re.search(r'_frame_(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Run whole analysis of each json output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Path Construction: get_image_path generates the correct path to the image file based on the JSON filename.\n",
    "\n",
    "Entity Extraction:extract_entities_from_json pulls characters, objects, and places from the JSON data.\n",
    "\n",
    "Image-to-Image Comparison:perform_image_to_image_comparison compares partial objects with full objects using the OpenAI API.\n",
    "\n",
    "Entity Comparison:compare_entities handles both name-based and image-based comparisons to decide whether two entities should be consolidated.\n",
    "\n",
    "Consolidation:Entities across frames are consolidated into a single summary file that tracks where each entity was found.\n",
    "\n",
    "Main Execution:The script runs through all JSON files, processes the entities, and saves the consolidated results to a summary JSON file.\n",
    "\n",
    "Key Features of This Implementation:\n",
    "Text-Based Comparison: The code first attempts to merge entities based on exact name matches. If no match is found, it uses the OpenAI API to determine if two entities with different names should be merged.\n",
    "\n",
    "Image-to-Image Comparison: If one of the entities is flagged as a portion, or if names don't match but the entities might still be the same, the code performs an image-to-image comparison using the OpenAI API.\n",
    "\n",
    "Efficient Processing: The code processes each frame sequentially and logs all merges into merged_entities_log, ensuring you have a record of what entities were merged, including their original names and frames.\n",
    "\n",
    "No Overwritten Functionality: The original image analysis functionality is preserved and integrated smoothly with the text-based comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Unique Characters, Objects, and Places: This can be done by counting the keys in the consolidated_data dictionary.\n",
    "Average Characters per Frame: This can be calculated by summing up all instances of characters found across frames and dividing by the total number of frames where characters appear.\n",
    "Average Features per Character/Object: Calculate this by summing the features of all characters/objects and dividing by the total number of characters/objects.\n",
    "Overall Color Analysis: Aggregate the color data from all JSON files.\n",
    "Filter Compliance: Check for any instances where the filters (e.g., nudity, drugs) are not compliant and log the frame numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import base64\n",
    "import re\n",
    "import time\n",
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Function to construct the image path based on the JSON file\n",
    "def get_image_path(json_filename, image_directory):\n",
    "    image_filename = json_filename.replace(\"_analysis.json\", \".jpg\")\n",
    "    return os.path.join(image_directory, image_filename)\n",
    "\n",
    "# Function to extract entities from a JSON file\n",
    "def extract_entities_from_json(json_data):\n",
    "    entities = {\n",
    "        \"characters\": [],\n",
    "        \"objects\": [],\n",
    "        \"places\": []\n",
    "    }\n",
    "\n",
    "    if \"Image Analysis\" in json_data:\n",
    "        if \"Characters\" in json_data[\"Image Analysis\"] and json_data[\"Image Analysis\"][\"Characters\"][\"Total Characters Identified\"] > 0:\n",
    "            for character in json_data[\"Image Analysis\"][\"Characters\"][\"Character Details\"].values():\n",
    "                entities[\"characters\"].append(character)\n",
    "        \n",
    "        if \"Objects\" in json_data[\"Image Analysis\"] and json_data[\"Image Analysis\"][\"Objects\"][\"Total Objects Identified\"] > 0:\n",
    "            for obj in json_data[\"Image Analysis\"][\"Objects\"][\"Objects Details\"].values():\n",
    "                entities[\"objects\"].append(obj)\n",
    "        \n",
    "        if \"Place\" in json_data[\"Image Analysis\"]:\n",
    "            entities[\"places\"].append(json_data[\"Image Analysis\"][\"Place\"])\n",
    "\n",
    "    return entities\n",
    "\n",
    "# Function to encode an image to base64\n",
    "def encode_image_to_base64(image_path):\n",
    "    if not image_path or not os.path.isfile(image_path):\n",
    "        raise ValueError(f\"Image path cannot be None or invalid: {image_path}\")\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# Function to perform image-to-image comparison using OpenAI API\n",
    "async def perform_image_to_image_comparison(entity1, entity2, image_path1, image_path2, api_key):\n",
    "    base64_image1 = encode_image_to_base64(image_path1)\n",
    "    base64_image2 = encode_image_to_base64(image_path2)\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    You are an expert in image analysis. Compare the two provided images and determine if they represent the same object or character, even if one is a partial view. Consider features, colors, and context. For instance, if the identified entity is a limb (e.g., a leg), you could contrast that limb with other full views of characters and see if it matches.\n",
    "\n",
    "    Return 'True' if the images depict the same object or character, 'False' if they are different, and 'Uncertain' if unsure.\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"images\": [{\"image\": base64_image1}, {\"image\": base64_image2}],\n",
    "        \"max_tokens\": 100\n",
    "    }\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload) as response:\n",
    "                response_json = await response.json()\n",
    "                answer = response_json.get('choices', [{}])[0].get('message', {}).get('content', '').strip().lower()\n",
    "                \n",
    "                if \"uncertain\" in answer:\n",
    "                    return \"uncertain\"\n",
    "                elif \"true\" in answer:\n",
    "                    return \"true\"\n",
    "                else:\n",
    "                    return \"false\"\n",
    "        except KeyError as e:\n",
    "            print(f\"Error accessing API response: {e}\")\n",
    "            return \"false\"\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            return \"false\"\n",
    "\n",
    "# Function to prompt OpenAI for entity consolidation\n",
    "async def consolidate_entities_with_openai(entity_type, entity_list, api_key):\n",
    "    if not entity_list:\n",
    "        return entity_list  # No entities to process\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert in entity recognition and consolidation. Here is a list of {entity_type}. The list may include variations in the names or descriptions that refer to the same entity. Please identify which entities refer to the same concept or character and suggest how they should be merged under a single, consistent name. For example, if 'Banana character', 'Banana Character 1', and 'Banana Man' refer to the same entity, suggest that they should be merged under one name.\n",
    "\n",
    "    List of {entity_type}:\n",
    "    {', '.join(entity_list)}\n",
    "\n",
    "    Please return a JSON object with the consolidated list of entities where similar entities are merged under a single name.\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": 500\n",
    "    }\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload) as response:\n",
    "                response_json = await response.json()\n",
    "                consolidated_list = response_json.get('choices', [{}])[0].get('message', {}).get('content', '').strip()\n",
    "                try:\n",
    "                    return json.loads(consolidated_list)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error decoding JSON from OpenAI response: {e}\")\n",
    "                    return entity_list  # Fallback to original list if parsing fails\n",
    "        except Exception as e:\n",
    "            print(f\"Error during entity consolidation: {e}\")\n",
    "            return entity_list  # Fallback to original list if API call fails\n",
    "\n",
    "# Function to apply the consolidation based on the OpenAI response\n",
    "def apply_consolidation(consolidated_data, entity_type, new_entity_list, merged_entities_log):\n",
    "    old_to_new_map = {}\n",
    "    for new_entity in new_entity_list:\n",
    "        if isinstance(new_entity, dict):\n",
    "            for old_entity in new_entity[\"merge\"]:\n",
    "                old_to_new_map[old_entity] = new_entity[\"name\"]\n",
    "\n",
    "    for old_entity, new_entity in old_to_new_map.items():\n",
    "        if old_entity in consolidated_data[entity_type]:\n",
    "            if new_entity not in consolidated_data[entity_type]:\n",
    "                consolidated_data[entity_type][new_entity] = consolidated_data[entity_type][old_entity]\n",
    "            else:\n",
    "                consolidated_data[entity_type][new_entity][\"frames_found\"].extend(consolidated_data[entity_type][old_entity][\"frames_found\"])\n",
    "                consolidated_data[entity_type][new_entity][\"frames_found\"] = list(set(consolidated_data[entity_type][new_entity][\"frames_found\"]))\n",
    "            \n",
    "            merged_entities_log.append({\n",
    "                \"merged_to\": new_entity,\n",
    "                \"merged_from\": old_entity,\n",
    "                \"frames\": consolidated_data[entity_type][old_entity][\"frames_found\"]\n",
    "            })\n",
    "            del consolidated_data[entity_type][old_entity]\n",
    "\n",
    "    return consolidated_data, merged_entities_log\n",
    "\n",
    "# Function to generate summary statistics\n",
    "def generate_summary_statistics(consolidated_data, all_json_files_data):\n",
    "    summary_statistics = {\n",
    "        \"Number of Unique Characters\": len(consolidated_data[\"characters\"]),\n",
    "        \"Number of Unique Objects\": len(consolidated_data[\"objects\"]),\n",
    "        \"Number of Unique Places\": len(consolidated_data[\"places\"]),\n",
    "    }\n",
    "\n",
    "    # Initialize variables for aggregation\n",
    "    total_characters_identified = 0\n",
    "    total_objects_identified = 0\n",
    "    total_frames = len(all_json_files_data)\n",
    "    total_character_features = 0\n",
    "    total_object_features = 0\n",
    "    color_totals = {\n",
    "        \"Red\": 0,\n",
    "        \"Yellow\": 0,\n",
    "        \"Green\": 0,\n",
    "        \"Blue\": 0,\n",
    "        \"White\": 0,\n",
    "        \"Black\": 0,\n",
    "        \"Non-primary\": 0\n",
    "    }\n",
    "    non_compliant_frames = []\n",
    "    fantasy_adventurous_count = 0\n",
    "\n",
    "    # Aggregate data from all JSON files\n",
    "    for json_data in all_json_files_data:\n",
    "        image_analysis = json_data[\"Image Analysis\"]\n",
    "\n",
    "        # Summing up total identified characters and objects\n",
    "        total_characters_identified += image_analysis[\"Characters\"][\"Total Characters Identified\"]\n",
    "        total_objects_identified += image_analysis[\"Objects\"][\"Total Objects Identified\"]\n",
    "\n",
    "        # Summing up features per character and object with a default value of 0 if \"Total Features\" is missing\n",
    "        total_character_features += sum([char.get(\"Total Features\", 0) for char in image_analysis[\"Characters\"][\"Character Details\"].values()])\n",
    "        total_object_features += sum([obj.get(\"Total Features\", 0) for obj in image_analysis[\"Objects\"][\"Objects Details\"].values()])\n",
    "\n",
    "        # Aggregating color data\n",
    "        for color, color_data in image_analysis[\"Colors Found\"].items():\n",
    "            color_totals[color] += color_data[\"Pixel Count\"]\n",
    "\n",
    "        # Check compliance and log non-compliant frames\n",
    "        for filter_name, is_compliant in image_analysis[\"Suitability\"].items():\n",
    "            if is_compliant:  # Log only if true, indicating non-compliance\n",
    "                non_compliant_frames.append({\n",
    "                    \"Frame\": json_data[\"Image File\"],\n",
    "                    \"Non-Compliant Filter\": filter_name\n",
    "                })\n",
    "\n",
    "        # Count fantasy/adventurous places\n",
    "        if image_analysis[\"Place\"].get(\"Fantasy/Adventurous Place\", 0) == 1:\n",
    "            fantasy_adventurous_count += 1\n",
    "\n",
    "    # Calculate averages\n",
    "    summary_statistics[\"Average Characters per Frame\"] = total_characters_identified / total_frames if total_frames > 0 else 0\n",
    "    summary_statistics[\"Average Features per Character\"] = total_character_features / total_characters_identified if total_characters_identified > 0 else 0\n",
    "    summary_statistics[\"Average Features per Object\"] = total_object_features / total_objects_identified if total_objects_identified > 0 else 0\n",
    "\n",
    "    # Add overall color percentages\n",
    "    total_pixels = sum(color_totals.values())\n",
    "    color_percentages = {color: (count / total_pixels) * 100 for color, count in color_totals.items() if total_pixels > 0}\n",
    "    summary_statistics[\"Overall Color Distribution\"] = color_percentages\n",
    "\n",
    "    # Add compliance information\n",
    "    summary_statistics[\"Non-Compliant Frames\"] = non_compliant_frames\n",
    "\n",
    "    # Calculate and add the percentage of fantasy/adventurous places\n",
    "    summary_statistics[\"Percentage of Fantasy/Adventurous Places\"] = (fantasy_adventurous_count / total_frames) * 100 if total_frames > 0 else 0\n",
    "\n",
    "    return summary_statistics\n",
    "\n",
    "\n",
    "# Function to print the consolidated entities\n",
    "def print_consolidated_data(consolidated_data, title=\"Consolidated\"):\n",
    "    print(f\"\\n{title} Characters:\")\n",
    "    for char_name, char_data in consolidated_data[\"characters\"].items():\n",
    "        print(f\"{char_name}: Found in frames {char_data['frames_found']}\")\n",
    "\n",
    "    print(f\"\\n{title} Objects:\")\n",
    "    for obj_name, obj_data in consolidated_data[\"objects\"].items():\n",
    "        print(f\"{obj_name}: Found in frames {obj_data['frames_found']}\")\n",
    "\n",
    "    print(f\"\\n{title} Places:\")\n",
    "    for place_name, place_data in consolidated_data[\"places\"].items():\n",
    "        print(f\"{place_name}: Found in frames {place_data['frames_found']}\")\n",
    "\n",
    "# Function to initially extract and consolidate all entities into lists\n",
    "async def initial_consolidation(json_output_dir, image_output_dir):\n",
    "    consolidated_data = {\n",
    "        \"characters\": {},\n",
    "        \"objects\": {},\n",
    "        \"places\": {}\n",
    "    }\n",
    "\n",
    "    json_files = [f for f in os.listdir(json_output_dir) if f.endswith('.json')]\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        json_path = os.path.join(json_output_dir, json_file)\n",
    "        with open(json_path, 'r') as file:\n",
    "            json_data = json.load(file)\n",
    "\n",
    "        frame_number = extract_frame_number(json_file)\n",
    "        entities = extract_entities_from_json(json_data)\n",
    "\n",
    "        for entity_type in [\"characters\", \"objects\", \"places\"]:\n",
    "            for new_entity in entities[entity_type]:\n",
    "                entity_name = new_entity[\"Name\"]\n",
    "                \n",
    "                # Generate the image path\n",
    "                image_path = get_image_path(json_file, image_output_dir)\n",
    "                new_entity[\"Image Path\"] = image_path\n",
    "                \n",
    "                if entity_name in consolidated_data[entity_type]:\n",
    "                    consolidated_data[entity_type][entity_name][\"frames_found\"].append(frame_number)\n",
    "                else:\n",
    "                    consolidated_data[entity_type][entity_name] = {\n",
    "                        \"entity\": new_entity,\n",
    "                        \"frames_found\": [frame_number]\n",
    "                    }\n",
    "\n",
    "    return consolidated_data\n",
    "\n",
    "# Function to process and consolidate all entities\n",
    "async def consolidate_all_entities(consolidated_data, api_key):\n",
    "    merged_entities_log = []\n",
    "\n",
    "    # Consolidate characters\n",
    "    characters = list(consolidated_data[\"characters\"].keys())\n",
    "    new_characters = await consolidate_entities_with_openai(\"characters\", characters, api_key)\n",
    "    consolidated_data, merged_entities_log = apply_consolidation(consolidated_data, \"characters\", new_characters, merged_entities_log)\n",
    "\n",
    "    # Consolidate objects\n",
    "    objects = list(consolidated_data[\"objects\"].keys())\n",
    "    new_objects = await consolidate_entities_with_openai(\"objects\", objects, api_key)\n",
    "    consolidated_data, merged_entities_log = apply_consolidation(consolidated_data, \"objects\", new_objects, merged_entities_log)\n",
    "\n",
    "    # Consolidate places\n",
    "    places = list(consolidated_data[\"places\"].keys())\n",
    "    new_places = await consolidate_entities_with_openai(\"places\", places, api_key)\n",
    "    consolidated_data, merged_entities_log = apply_consolidation(consolidated_data, \"places\", new_places, merged_entities_log)\n",
    "\n",
    "    return consolidated_data, merged_entities_log\n",
    "\n",
    "# Function to handle the image-to-image comparisons for portions\n",
    "async def handle_portion_comparisons(consolidated_data, api_key):\n",
    "    merged_entities_log = []\n",
    "\n",
    "    for entity_type in [\"characters\", \"objects\", \"places\"]:\n",
    "        for entity_name, entity_data in consolidated_data[entity_type].items():\n",
    "            if entity_data[\"entity\"].get(\"Portion Boolean\") == 1:\n",
    "                # Perform image-to-image comparisons for portions\n",
    "                for other_entity_name, other_entity_data in consolidated_data[entity_type].items():\n",
    "                    if other_entity_name != entity_name and other_entity_data[\"entity\"].get(\"Portion Boolean\") != 1:\n",
    "                        comparison_result = await perform_image_to_image_comparison(\n",
    "                            entity_data[\"entity\"], other_entity_data[\"entity\"],\n",
    "                            entity_data[\"entity\"][\"Image Path\"], other_entity_data[\"entity\"][\"Image Path\"],\n",
    "                            api_key\n",
    "                        )\n",
    "                        if comparison_result == \"true\":\n",
    "                            if entity_name not in other_entity_data[\"frames_found\"]:\n",
    "                                other_entity_data[\"frames_found\"].extend(entity_data[\"frames_found\"])\n",
    "                                other_entity_data[\"frames_found\"] = list(set(other_entity_data[\"frames_found\"]))\n",
    "\n",
    "                            merged_entities_log.append({\n",
    "                                \"merged_to\": other_entity_name,\n",
    "                                \"merged_from\": entity_name,\n",
    "                                \"frames\": entity_data[\"frames_found\"]\n",
    "                            })\n",
    "                            del consolidated_data[entity_type][entity_name]\n",
    "                            break\n",
    "\n",
    "    return consolidated_data, merged_entities_log\n",
    "\n",
    "# Function to perform a final image-to-image comparison for characters\n",
    "async def final_image_to_image_comparison_for_characters(consolidated_data, api_key):\n",
    "    character_names = list(consolidated_data[\"characters\"].keys())\n",
    "    merged_entities_log = []\n",
    "\n",
    "    # Progress bar for character comparisons\n",
    "    with tqdm(total=len(character_names) * (len(character_names) - 1) // 2, desc=\"Final Character Comparisons\", unit=\"comparison\") as pbar:\n",
    "        # Compare each character with every other character\n",
    "        for i in range(len(character_names)):\n",
    "            for j in range(i + 1, len(character_names)):\n",
    "                name1 = character_names[i]\n",
    "                name2 = character_names[j]\n",
    "\n",
    "                entity1 = consolidated_data[\"characters\"][name1][\"entity\"]\n",
    "                entity2 = consolidated_data[\"characters\"][name2][\"entity\"]\n",
    "\n",
    "                # Perform image comparison\n",
    "                comparison_result = await perform_image_to_image_comparison(\n",
    "                    entity1, entity2,\n",
    "                    entity1[\"Image Path\"], entity2[\"Image Path\"],\n",
    "                    api_key\n",
    "                )\n",
    "\n",
    "                if comparison_result == \"true\":\n",
    "                    # Merge name2 into name1\n",
    "                    consolidated_data[\"characters\"][name1][\"frames_found\"].extend(consolidated_data[\"characters\"][name2][\"frames_found\"])\n",
    "                    consolidated_data[\"characters\"][name1][\"frames_found\"] = list(set(consolidated_data[\"characters\"][name1][\"frames_found\"]))\n",
    "\n",
    "                    # Log the merge\n",
    "                    merged_entities_log.append({\n",
    "                        \"merged_to\": name1,\n",
    "                        \"merged_from\": name2,\n",
    "                        \"frames\": consolidated_data[\"characters\"][name2][\"frames_found\"]\n",
    "                    })\n",
    "\n",
    "                    # Remove the merged entity\n",
    "                    del consolidated_data[\"characters\"][name2]\n",
    "\n",
    "                pbar.update(1)  # Update the progress bar\n",
    "\n",
    "    return consolidated_data, merged_entities_log\n",
    "\n",
    "# Function to save the summary to a JSON file\n",
    "def save_summary_to_json(consolidated_data, output_path, video_title, file_size, processing_time, merged_entities_log, all_json_files_data):\n",
    "    # Generate additional summary statistics\n",
    "    summary_statistics = generate_summary_statistics(consolidated_data, all_json_files_data)\n",
    "\n",
    "    summary = {\n",
    "        \"Video Title\": video_title,\n",
    "        \"File Size (bytes)\": file_size,\n",
    "        \"Processing Time (seconds)\": processing_time,\n",
    "        \"Summary Statistics\": summary_statistics,  # Add the generated statistics here\n",
    "        \"Consolidated Data\": consolidated_data,\n",
    "        \"Merged Entities Log\": merged_entities_log\n",
    "    }\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=4)\n",
    "\n",
    "# Extract frame number from file name\n",
    "def extract_frame_number(filename):\n",
    "    match = re.search(r'_frame_(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Characters:\n",
      "Banana character: Found in frames [2995]\n",
      "Mouse Character: Found in frames [2747]\n",
      "Cartoon character: Found in frames [2517]\n",
      "Banana Character 1: Found in frames [542, 642]\n",
      "Banana Character 2: Found in frames [542, 642]\n",
      "Cartoon Character: Found in frames [610]\n",
      "\n",
      "Initial Objects:\n",
      "Cloud: Found in frames [690]\n",
      "Box: Found in frames [690]\n",
      "Banana character: Found in frames [2995]\n",
      "Mouse Character: Found in frames [2747]\n",
      "Foot: Found in frames [2517]\n",
      "Chair: Found in frames [2517]\n",
      "Grass: Found in frames [2517]\n",
      "Door: Found in frames [732]\n",
      "Tree: Found in frames [542, 610, 642]\n",
      "Banana Characters: Found in frames [542, 642]\n",
      "Blue Box: Found in frames [542]\n",
      "Feet: Found in frames [610]\n",
      "Ground: Found in frames [642]\n",
      "\n",
      "Initial Places:\n",
      "Sky: Found in frames [690]\n",
      "Blue room: Found in frames [2995]\n",
      "Cartoon Nature Background: Found in frames [2747]\n",
      "Cartoon setting: Found in frames [2517]\n",
      "Cartoon Garden: Found in frames [732, 642]\n",
      "Backyard: Found in frames [542]\n",
      "Grass Area: Found in frames [610]\n",
      "\n",
      "After OpenAI Consolidation Characters:\n",
      "Banana character: Found in frames [2995]\n",
      "Mouse Character: Found in frames [2747]\n",
      "Cartoon character: Found in frames [2517]\n",
      "Banana Character 1: Found in frames [542, 642]\n",
      "Banana Character 2: Found in frames [542, 642]\n",
      "Cartoon Character: Found in frames [610]\n",
      "\n",
      "After OpenAI Consolidation Objects:\n",
      "Cloud: Found in frames [690]\n",
      "Box: Found in frames [690]\n",
      "Banana character: Found in frames [2995]\n",
      "Mouse Character: Found in frames [2747]\n",
      "Foot: Found in frames [2517]\n",
      "Chair: Found in frames [2517]\n",
      "Grass: Found in frames [2517]\n",
      "Door: Found in frames [732]\n",
      "Tree: Found in frames [542, 610, 642]\n",
      "Banana Characters: Found in frames [542, 642]\n",
      "Blue Box: Found in frames [542]\n",
      "Feet: Found in frames [610]\n",
      "Ground: Found in frames [642]\n",
      "\n",
      "After OpenAI Consolidation Places:\n",
      "Sky: Found in frames [690]\n",
      "Blue room: Found in frames [2995]\n",
      "Cartoon Nature Background: Found in frames [2747]\n",
      "Cartoon setting: Found in frames [2517]\n",
      "Cartoon Garden: Found in frames [732, 642]\n",
      "Backyard: Found in frames [542]\n",
      "Grass Area: Found in frames [610]\n",
      "\n",
      "After Portion Comparisons Characters:\n",
      "Banana character: Found in frames [2995]\n",
      "Mouse Character: Found in frames [2747]\n",
      "Cartoon character: Found in frames [2517]\n",
      "Banana Character 1: Found in frames [542, 642]\n",
      "Banana Character 2: Found in frames [542, 642]\n",
      "Cartoon Character: Found in frames [610]\n",
      "\n",
      "After Portion Comparisons Objects:\n",
      "Cloud: Found in frames [690]\n",
      "Box: Found in frames [690]\n",
      "Banana character: Found in frames [2995]\n",
      "Mouse Character: Found in frames [2747]\n",
      "Foot: Found in frames [2517]\n",
      "Chair: Found in frames [2517]\n",
      "Grass: Found in frames [2517]\n",
      "Door: Found in frames [732]\n",
      "Tree: Found in frames [542, 610, 642]\n",
      "Banana Characters: Found in frames [542, 642]\n",
      "Blue Box: Found in frames [542]\n",
      "Feet: Found in frames [610]\n",
      "Ground: Found in frames [642]\n",
      "\n",
      "After Portion Comparisons Places:\n",
      "Sky: Found in frames [690]\n",
      "Blue room: Found in frames [2995]\n",
      "Cartoon Nature Background: Found in frames [2747]\n",
      "Cartoon setting: Found in frames [2517]\n",
      "Cartoon Garden: Found in frames [732, 642]\n",
      "Backyard: Found in frames [542]\n",
      "Grass Area: Found in frames [610]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Character Comparisons: 100%|██████████| 15/15 [00:08<00:00,  1.86comparison/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final After Image Comparisons Characters:\n",
      "Banana character: Found in frames [2995]\n",
      "Mouse Character: Found in frames [2747]\n",
      "Cartoon character: Found in frames [2517]\n",
      "Banana Character 1: Found in frames [542, 642]\n",
      "Banana Character 2: Found in frames [542, 642]\n",
      "Cartoon Character: Found in frames [610]\n",
      "\n",
      "Final After Image Comparisons Objects:\n",
      "Cloud: Found in frames [690]\n",
      "Box: Found in frames [690]\n",
      "Banana character: Found in frames [2995]\n",
      "Mouse Character: Found in frames [2747]\n",
      "Foot: Found in frames [2517]\n",
      "Chair: Found in frames [2517]\n",
      "Grass: Found in frames [2517]\n",
      "Door: Found in frames [732]\n",
      "Tree: Found in frames [542, 610, 642]\n",
      "Banana Characters: Found in frames [542, 642]\n",
      "Blue Box: Found in frames [542]\n",
      "Feet: Found in frames [610]\n",
      "Ground: Found in frames [642]\n",
      "\n",
      "Final After Image Comparisons Places:\n",
      "Sky: Found in frames [690]\n",
      "Blue room: Found in frames [2995]\n",
      "Cartoon Nature Background: Found in frames [2747]\n",
      "Cartoon setting: Found in frames [2517]\n",
      "Cartoon Garden: Found in frames [732, 642]\n",
      "Backyard: Found in frames [542]\n",
      "Grass Area: Found in frames [610]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ############################\n",
    "# # TEXT FOR ANALYSIS ,... FEEL FREE TO COMMENT\n",
    "\n",
    "# import os\n",
    "# import json\n",
    "# import nest_asyncio\n",
    "# import asyncio\n",
    "\n",
    "# # Set your directories and variables\n",
    "# json_output_dir = '/Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/json_output'\n",
    "# image_output_dir = '/Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output'\n",
    "# summary_json_path = '/Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas_summary.json'\n",
    "# video_title = \"Bananas_in_pyjamas.mp4\"\n",
    "# api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# # Apply nest_asyncio to handle the running event loop\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# async def test_entity_consolidation():\n",
    "#     # Step 1: Initial extraction and consolidation\n",
    "#     consolidated_data = await initial_consolidation(json_output_dir, image_output_dir)\n",
    "#     print_consolidated_data(consolidated_data, title=\"Initial\")\n",
    "\n",
    "#     # Load all JSON files data\n",
    "#     all_json_files_data = []\n",
    "#     json_files = [f for f in os.listdir(json_output_dir) if f.endswith('.json')]\n",
    "#     for json_file in json_files:\n",
    "#         with open(os.path.join(json_output_dir, json_file), 'r') as file:\n",
    "#             all_json_files_data.append(json.load(file))\n",
    "\n",
    "#     # Step 2: Consolidate entities using OpenAI\n",
    "#     consolidated_data, merged_entities_log = await consolidate_all_entities(consolidated_data, api_key)\n",
    "#     print_consolidated_data(consolidated_data, title=\"After OpenAI Consolidation\")\n",
    "\n",
    "#     # Step 3: Handle image-to-image comparisons for portions\n",
    "#     consolidated_data, portion_merged_log = await handle_portion_comparisons(consolidated_data, api_key)\n",
    "#     merged_entities_log.extend(portion_merged_log)\n",
    "#     print_consolidated_data(consolidated_data, title=\"After Portion Comparisons\")\n",
    "\n",
    "#     # Step 4: Final image-to-image comparison for characters\n",
    "#     consolidated_data, final_character_merged_log = await final_image_to_image_comparison_for_characters(consolidated_data, api_key)\n",
    "#     merged_entities_log.extend(final_character_merged_log)\n",
    "#     print_consolidated_data(consolidated_data, title=\"Final After Image Comparisons\")\n",
    "\n",
    "#     # Save the final summary\n",
    "#     processing_time = time.time()  # This should be the actual processing time from the whole process\n",
    "#     file_size = os.path.getsize(summary_json_path) if os.path.isfile(summary_json_path) else 0\n",
    "#     save_summary_to_json(consolidated_data, summary_json_path, video_title, file_size, processing_time, merged_entities_log, all_json_files_data)\n",
    "\n",
    "# # Run the test\n",
    "# asyncio.get_event_loop().run_until_complete(test_entity_consolidation())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Main Function Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Videos:   0%|          | 0/1 [00:00<?, ?video/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video 1/1: Bananas_in_pyjamas.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:pyscenedetect:VideoManager is deprecated and will be removed.\n",
      "INFO:pyscenedetect:Loaded 1 video, framerate: 25.000 FPS, resolution: 640 x 360\n",
      "INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 320 x 180\n",
      "INFO:pyscenedetect:Detecting scenes...\n",
      "INFO:root:Detected 159 scenes:\n",
      "INFO:root:Scene 1: Start 00:00:00.000 / Frame 0, End 00:00:08.360 / Frame 209\n",
      "INFO:root:Scene 2: Start 00:00:08.360 / Frame 209, End 00:00:19.320 / Frame 483\n",
      "INFO:root:Scene 3: Start 00:00:19.320 / Frame 483, End 00:00:24.040 / Frame 601\n",
      "INFO:root:Scene 4: Start 00:00:24.040 / Frame 601, End 00:00:24.800 / Frame 620\n",
      "INFO:root:Scene 5: Start 00:00:24.800 / Frame 620, End 00:00:26.600 / Frame 665\n",
      "INFO:root:Scene 6: Start 00:00:26.600 / Frame 665, End 00:00:28.600 / Frame 715\n",
      "INFO:root:Scene 7: Start 00:00:28.600 / Frame 715, End 00:00:29.960 / Frame 749\n",
      "INFO:root:Scene 8: Start 00:00:29.960 / Frame 749, End 00:00:31.880 / Frame 797\n",
      "INFO:root:Scene 9: Start 00:00:31.880 / Frame 797, End 00:00:35.240 / Frame 881\n",
      "INFO:root:Scene 10: Start 00:00:35.240 / Frame 881, End 00:00:37.920 / Frame 948\n",
      "INFO:root:Scene 11: Start 00:00:37.920 / Frame 948, End 00:00:42.320 / Frame 1058\n",
      "INFO:root:Scene 12: Start 00:00:42.320 / Frame 1058, End 00:00:42.960 / Frame 1074\n",
      "INFO:root:Scene 13: Start 00:00:42.960 / Frame 1074, End 00:00:50.040 / Frame 1251\n",
      "INFO:root:Scene 14: Start 00:00:50.040 / Frame 1251, End 00:00:54.480 / Frame 1362\n",
      "INFO:root:Scene 15: Start 00:00:54.480 / Frame 1362, End 00:00:58.400 / Frame 1460\n",
      "INFO:root:Scene 16: Start 00:00:58.400 / Frame 1460, End 00:01:08.560 / Frame 1714\n",
      "INFO:root:Scene 17: Start 00:01:08.560 / Frame 1714, End 00:01:10.480 / Frame 1762\n",
      "INFO:root:Scene 18: Start 00:01:10.480 / Frame 1762, End 00:01:14.520 / Frame 1863\n",
      "INFO:root:Scene 19: Start 00:01:14.520 / Frame 1863, End 00:01:18.040 / Frame 1951\n",
      "INFO:root:Scene 20: Start 00:01:18.040 / Frame 1951, End 00:01:21.960 / Frame 2049\n",
      "INFO:root:Scene 21: Start 00:01:21.960 / Frame 2049, End 00:01:28.400 / Frame 2210\n",
      "INFO:root:Scene 22: Start 00:01:28.400 / Frame 2210, End 00:01:34.480 / Frame 2362\n",
      "INFO:root:Scene 23: Start 00:01:34.480 / Frame 2362, End 00:01:39.880 / Frame 2497\n",
      "INFO:root:Scene 24: Start 00:01:39.880 / Frame 2497, End 00:01:41.520 / Frame 2538\n",
      "INFO:root:Scene 25: Start 00:01:41.520 / Frame 2538, End 00:01:48.280 / Frame 2707\n",
      "INFO:root:Scene 26: Start 00:01:48.280 / Frame 2707, End 00:01:51.480 / Frame 2787\n",
      "INFO:root:Scene 27: Start 00:01:51.480 / Frame 2787, End 00:01:58.000 / Frame 2950\n",
      "INFO:root:Scene 28: Start 00:01:58.000 / Frame 2950, End 00:02:01.640 / Frame 3041\n",
      "INFO:root:Scene 29: Start 00:02:01.640 / Frame 3041, End 00:02:08.440 / Frame 3211\n",
      "INFO:root:Scene 30: Start 00:02:08.440 / Frame 3211, End 00:02:10.560 / Frame 3264\n",
      "INFO:root:Scene 31: Start 00:02:10.560 / Frame 3264, End 00:02:16.840 / Frame 3421\n",
      "INFO:root:Scene 32: Start 00:02:16.840 / Frame 3421, End 00:02:20.360 / Frame 3509\n",
      "INFO:root:Scene 33: Start 00:02:20.360 / Frame 3509, End 00:02:23.520 / Frame 3588\n",
      "INFO:root:Scene 34: Start 00:02:23.520 / Frame 3588, End 00:02:28.280 / Frame 3707\n",
      "INFO:root:Scene 35: Start 00:02:28.280 / Frame 3707, End 00:02:34.120 / Frame 3853\n",
      "INFO:root:Scene 36: Start 00:02:34.120 / Frame 3853, End 00:02:37.640 / Frame 3941\n",
      "INFO:root:Scene 37: Start 00:02:37.640 / Frame 3941, End 00:02:40.280 / Frame 4007\n",
      "INFO:root:Scene 38: Start 00:02:40.280 / Frame 4007, End 00:02:47.560 / Frame 4189\n",
      "INFO:root:Scene 39: Start 00:02:47.560 / Frame 4189, End 00:02:50.080 / Frame 4252\n",
      "INFO:root:Scene 40: Start 00:02:50.080 / Frame 4252, End 00:02:55.240 / Frame 4381\n",
      "INFO:root:Scene 41: Start 00:02:55.240 / Frame 4381, End 00:02:59.200 / Frame 4480\n",
      "INFO:root:Scene 42: Start 00:02:59.200 / Frame 4480, End 00:03:01.080 / Frame 4527\n",
      "INFO:root:Scene 43: Start 00:03:01.080 / Frame 4527, End 00:03:03.680 / Frame 4592\n",
      "INFO:root:Scene 44: Start 00:03:03.680 / Frame 4592, End 00:03:05.360 / Frame 4634\n",
      "INFO:root:Scene 45: Start 00:03:05.360 / Frame 4634, End 00:03:06.800 / Frame 4670\n",
      "INFO:root:Scene 46: Start 00:03:06.800 / Frame 4670, End 00:03:09.760 / Frame 4744\n",
      "INFO:root:Scene 47: Start 00:03:09.760 / Frame 4744, End 00:03:11.280 / Frame 4782\n",
      "INFO:root:Scene 48: Start 00:03:11.280 / Frame 4782, End 00:03:12.880 / Frame 4822\n",
      "INFO:root:Scene 49: Start 00:03:12.880 / Frame 4822, End 00:03:14.640 / Frame 4866\n",
      "INFO:root:Scene 50: Start 00:03:14.640 / Frame 4866, End 00:03:18.520 / Frame 4963\n",
      "INFO:root:Scene 51: Start 00:03:18.520 / Frame 4963, End 00:03:29.560 / Frame 5239\n",
      "INFO:root:Scene 52: Start 00:03:29.560 / Frame 5239, End 00:03:31.640 / Frame 5291\n",
      "INFO:root:Scene 53: Start 00:03:31.640 / Frame 5291, End 00:03:33.760 / Frame 5344\n",
      "INFO:root:Scene 54: Start 00:03:33.760 / Frame 5344, End 00:03:35.360 / Frame 5384\n",
      "INFO:root:Scene 55: Start 00:03:35.360 / Frame 5384, End 00:03:41.440 / Frame 5536\n",
      "INFO:root:Scene 56: Start 00:03:41.440 / Frame 5536, End 00:03:42.840 / Frame 5571\n",
      "INFO:root:Scene 57: Start 00:03:42.840 / Frame 5571, End 00:03:44.880 / Frame 5622\n",
      "INFO:root:Scene 58: Start 00:03:44.880 / Frame 5622, End 00:03:50.080 / Frame 5752\n",
      "INFO:root:Scene 59: Start 00:03:50.080 / Frame 5752, End 00:03:52.080 / Frame 5802\n",
      "INFO:root:Scene 60: Start 00:03:52.080 / Frame 5802, End 00:03:57.000 / Frame 5925\n",
      "INFO:root:Scene 61: Start 00:03:57.000 / Frame 5925, End 00:04:04.120 / Frame 6103\n",
      "INFO:root:Scene 62: Start 00:04:04.120 / Frame 6103, End 00:04:06.880 / Frame 6172\n",
      "INFO:root:Scene 63: Start 00:04:06.880 / Frame 6172, End 00:04:08.840 / Frame 6221\n",
      "INFO:root:Scene 64: Start 00:04:08.840 / Frame 6221, End 00:04:12.880 / Frame 6322\n",
      "INFO:root:Scene 65: Start 00:04:12.880 / Frame 6322, End 00:04:16.360 / Frame 6409\n",
      "INFO:root:Scene 66: Start 00:04:16.360 / Frame 6409, End 00:04:19.000 / Frame 6475\n",
      "INFO:root:Scene 67: Start 00:04:19.000 / Frame 6475, End 00:04:23.040 / Frame 6576\n",
      "INFO:root:Scene 68: Start 00:04:23.040 / Frame 6576, End 00:04:25.440 / Frame 6636\n",
      "INFO:root:Scene 69: Start 00:04:25.440 / Frame 6636, End 00:04:31.120 / Frame 6778\n",
      "INFO:root:Scene 70: Start 00:04:31.120 / Frame 6778, End 00:04:37.880 / Frame 6947\n",
      "INFO:root:Scene 71: Start 00:04:37.880 / Frame 6947, End 00:04:41.880 / Frame 7047\n",
      "INFO:root:Scene 72: Start 00:04:41.880 / Frame 7047, End 00:04:45.240 / Frame 7131\n",
      "INFO:root:Scene 73: Start 00:04:45.240 / Frame 7131, End 00:04:49.640 / Frame 7241\n",
      "INFO:root:Scene 74: Start 00:04:49.640 / Frame 7241, End 00:04:54.840 / Frame 7371\n",
      "INFO:root:Scene 75: Start 00:04:54.840 / Frame 7371, End 00:04:58.240 / Frame 7456\n",
      "INFO:root:Scene 76: Start 00:04:58.240 / Frame 7456, End 00:05:00.600 / Frame 7515\n",
      "INFO:root:Scene 77: Start 00:05:00.600 / Frame 7515, End 00:05:16.600 / Frame 7915\n",
      "INFO:root:Scene 78: Start 00:05:16.600 / Frame 7915, End 00:05:24.520 / Frame 8113\n",
      "INFO:root:Scene 79: Start 00:05:24.520 / Frame 8113, End 00:05:26.360 / Frame 8159\n",
      "INFO:root:Scene 80: Start 00:05:26.360 / Frame 8159, End 00:05:34.880 / Frame 8372\n",
      "INFO:root:Scene 81: Start 00:05:34.880 / Frame 8372, End 00:05:44.920 / Frame 8623\n",
      "INFO:root:Scene 82: Start 00:05:44.920 / Frame 8623, End 00:05:46.040 / Frame 8651\n",
      "INFO:root:Scene 83: Start 00:05:46.040 / Frame 8651, End 00:05:47.800 / Frame 8695\n",
      "INFO:root:Scene 84: Start 00:05:47.800 / Frame 8695, End 00:05:51.840 / Frame 8796\n",
      "INFO:root:Scene 85: Start 00:05:51.840 / Frame 8796, End 00:05:53.880 / Frame 8847\n",
      "INFO:root:Scene 86: Start 00:05:53.880 / Frame 8847, End 00:05:57.000 / Frame 8925\n",
      "INFO:root:Scene 87: Start 00:05:57.000 / Frame 8925, End 00:05:59.200 / Frame 8980\n",
      "INFO:root:Scene 88: Start 00:05:59.200 / Frame 8980, End 00:06:01.880 / Frame 9047\n",
      "INFO:root:Scene 89: Start 00:06:01.880 / Frame 9047, End 00:06:10.520 / Frame 9263\n",
      "INFO:root:Scene 90: Start 00:06:10.520 / Frame 9263, End 00:06:11.680 / Frame 9292\n",
      "INFO:root:Scene 91: Start 00:06:11.680 / Frame 9292, End 00:06:19.600 / Frame 9490\n",
      "INFO:root:Scene 92: Start 00:06:19.600 / Frame 9490, End 00:06:22.760 / Frame 9569\n",
      "INFO:root:Scene 93: Start 00:06:22.760 / Frame 9569, End 00:06:28.000 / Frame 9700\n",
      "INFO:root:Scene 94: Start 00:06:28.000 / Frame 9700, End 00:06:30.640 / Frame 9766\n",
      "INFO:root:Scene 95: Start 00:06:30.640 / Frame 9766, End 00:06:40.280 / Frame 10007\n",
      "INFO:root:Scene 96: Start 00:06:40.280 / Frame 10007, End 00:06:43.400 / Frame 10085\n",
      "INFO:root:Scene 97: Start 00:06:43.400 / Frame 10085, End 00:06:52.440 / Frame 10311\n",
      "INFO:root:Scene 98: Start 00:06:52.440 / Frame 10311, End 00:06:55.560 / Frame 10389\n",
      "INFO:root:Scene 99: Start 00:06:55.560 / Frame 10389, End 00:06:56.360 / Frame 10409\n",
      "INFO:root:Scene 100: Start 00:06:56.360 / Frame 10409, End 00:06:58.680 / Frame 10467\n",
      "INFO:root:Scene 101: Start 00:06:58.680 / Frame 10467, End 00:07:00.720 / Frame 10518\n",
      "INFO:root:Scene 102: Start 00:07:00.720 / Frame 10518, End 00:07:02.240 / Frame 10556\n",
      "INFO:root:Scene 103: Start 00:07:02.240 / Frame 10556, End 00:07:07.040 / Frame 10676\n",
      "INFO:root:Scene 104: Start 00:07:07.040 / Frame 10676, End 00:07:09.720 / Frame 10743\n",
      "INFO:root:Scene 105: Start 00:07:09.720 / Frame 10743, End 00:07:12.040 / Frame 10801\n",
      "INFO:root:Scene 106: Start 00:07:12.040 / Frame 10801, End 00:07:21.680 / Frame 11042\n",
      "INFO:root:Scene 107: Start 00:07:21.680 / Frame 11042, End 00:07:25.080 / Frame 11127\n",
      "INFO:root:Scene 108: Start 00:07:25.080 / Frame 11127, End 00:07:29.200 / Frame 11230\n",
      "INFO:root:Scene 109: Start 00:07:29.200 / Frame 11230, End 00:07:36.840 / Frame 11421\n",
      "INFO:root:Scene 110: Start 00:07:36.840 / Frame 11421, End 00:07:39.840 / Frame 11496\n",
      "INFO:root:Scene 111: Start 00:07:39.840 / Frame 11496, End 00:07:46.720 / Frame 11668\n",
      "INFO:root:Scene 112: Start 00:07:46.720 / Frame 11668, End 00:07:49.480 / Frame 11737\n",
      "INFO:root:Scene 113: Start 00:07:49.480 / Frame 11737, End 00:07:55.400 / Frame 11885\n",
      "INFO:root:Scene 114: Start 00:07:55.400 / Frame 11885, End 00:08:13.640 / Frame 12341\n",
      "INFO:root:Scene 115: Start 00:08:13.640 / Frame 12341, End 00:08:16.960 / Frame 12424\n",
      "INFO:root:Scene 116: Start 00:08:16.960 / Frame 12424, End 00:08:21.880 / Frame 12547\n",
      "INFO:root:Scene 117: Start 00:08:21.880 / Frame 12547, End 00:08:24.840 / Frame 12621\n",
      "INFO:root:Scene 118: Start 00:08:24.840 / Frame 12621, End 00:08:36.600 / Frame 12915\n",
      "INFO:root:Scene 119: Start 00:08:36.600 / Frame 12915, End 00:08:38.240 / Frame 12956\n",
      "INFO:root:Scene 120: Start 00:08:38.240 / Frame 12956, End 00:08:41.640 / Frame 13041\n",
      "INFO:root:Scene 121: Start 00:08:41.640 / Frame 13041, End 00:08:46.240 / Frame 13156\n",
      "INFO:root:Scene 122: Start 00:08:46.240 / Frame 13156, End 00:08:52.960 / Frame 13324\n",
      "INFO:root:Scene 123: Start 00:08:52.960 / Frame 13324, End 00:08:58.840 / Frame 13471\n",
      "INFO:root:Scene 124: Start 00:08:58.840 / Frame 13471, End 00:09:03.960 / Frame 13599\n",
      "INFO:root:Scene 125: Start 00:09:03.960 / Frame 13599, End 00:09:05.000 / Frame 13625\n",
      "INFO:root:Scene 126: Start 00:09:05.000 / Frame 13625, End 00:09:08.240 / Frame 13706\n",
      "INFO:root:Scene 127: Start 00:09:08.240 / Frame 13706, End 00:09:11.520 / Frame 13788\n",
      "INFO:root:Scene 128: Start 00:09:11.520 / Frame 13788, End 00:09:21.000 / Frame 14025\n",
      "INFO:root:Scene 129: Start 00:09:21.000 / Frame 14025, End 00:09:23.280 / Frame 14082\n",
      "INFO:root:Scene 130: Start 00:09:23.280 / Frame 14082, End 00:09:26.440 / Frame 14161\n",
      "INFO:root:Scene 131: Start 00:09:26.440 / Frame 14161, End 00:09:27.960 / Frame 14199\n",
      "INFO:root:Scene 132: Start 00:09:27.960 / Frame 14199, End 00:09:37.160 / Frame 14429\n",
      "INFO:root:Scene 133: Start 00:09:37.160 / Frame 14429, End 00:09:40.680 / Frame 14517\n",
      "INFO:root:Scene 134: Start 00:09:40.680 / Frame 14517, End 00:09:43.400 / Frame 14585\n",
      "INFO:root:Scene 135: Start 00:09:43.400 / Frame 14585, End 00:09:48.840 / Frame 14721\n",
      "INFO:root:Scene 136: Start 00:09:48.840 / Frame 14721, End 00:09:49.800 / Frame 14745\n",
      "INFO:root:Scene 137: Start 00:09:49.800 / Frame 14745, End 00:09:56.920 / Frame 14923\n",
      "INFO:root:Scene 138: Start 00:09:56.920 / Frame 14923, End 00:09:58.840 / Frame 14971\n",
      "INFO:root:Scene 139: Start 00:09:58.840 / Frame 14971, End 00:10:01.880 / Frame 15047\n",
      "INFO:root:Scene 140: Start 00:10:01.880 / Frame 15047, End 00:10:03.120 / Frame 15078\n",
      "INFO:root:Scene 141: Start 00:10:03.120 / Frame 15078, End 00:10:05.240 / Frame 15131\n",
      "INFO:root:Scene 142: Start 00:10:05.240 / Frame 15131, End 00:10:09.760 / Frame 15244\n",
      "INFO:root:Scene 143: Start 00:10:09.760 / Frame 15244, End 00:10:15.600 / Frame 15390\n",
      "INFO:root:Scene 144: Start 00:10:15.600 / Frame 15390, End 00:10:22.240 / Frame 15556\n",
      "INFO:root:Scene 145: Start 00:10:22.240 / Frame 15556, End 00:10:30.800 / Frame 15770\n",
      "INFO:root:Scene 146: Start 00:10:30.800 / Frame 15770, End 00:10:33.840 / Frame 15846\n",
      "INFO:root:Scene 147: Start 00:10:33.840 / Frame 15846, End 00:10:42.600 / Frame 16065\n",
      "INFO:root:Scene 148: Start 00:10:42.600 / Frame 16065, End 00:10:44.240 / Frame 16106\n",
      "INFO:root:Scene 149: Start 00:10:44.240 / Frame 16106, End 00:10:46.440 / Frame 16161\n",
      "INFO:root:Scene 150: Start 00:10:46.440 / Frame 16161, End 00:10:47.160 / Frame 16179\n",
      "INFO:root:Scene 151: Start 00:10:47.160 / Frame 16179, End 00:10:52.320 / Frame 16308\n",
      "INFO:root:Scene 152: Start 00:10:52.320 / Frame 16308, End 00:10:53.600 / Frame 16340\n",
      "INFO:root:Scene 153: Start 00:10:53.600 / Frame 16340, End 00:10:54.960 / Frame 16374\n",
      "INFO:root:Scene 154: Start 00:10:54.960 / Frame 16374, End 00:10:56.240 / Frame 16406\n",
      "INFO:root:Scene 155: Start 00:10:56.240 / Frame 16406, End 00:11:02.360 / Frame 16559\n",
      "INFO:root:Scene 156: Start 00:11:02.360 / Frame 16559, End 00:11:06.960 / Frame 16674\n",
      "INFO:root:Scene 157: Start 00:11:06.960 / Frame 16674, End 00:11:08.800 / Frame 16720\n",
      "INFO:root:Scene 158: Start 00:11:08.800 / Frame 16720, End 00:11:20.000 / Frame 17000\n",
      "INFO:root:Scene 159: Start 00:11:20.000 / Frame 17000, End 00:11:20.360 / Frame 17009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted and saved middle frame of scene 1 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_1_frame_104.jpg\n",
      "Extracted and saved middle frame of scene 2 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_2_frame_346.jpg\n",
      "Extracted and saved middle frame of scene 3 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_3_frame_542.jpg\n",
      "Extracted and saved middle frame of scene 4 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_4_frame_610.jpg\n",
      "Extracted and saved middle frame of scene 5 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_5_frame_642.jpg\n",
      "Extracted and saved middle frame of scene 6 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_6_frame_690.jpg\n",
      "Extracted and saved middle frame of scene 7 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_7_frame_732.jpg\n",
      "Extracted and saved middle frame of scene 8 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_8_frame_773.jpg\n",
      "Extracted and saved middle frame of scene 9 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_9_frame_839.jpg\n",
      "Extracted and saved middle frame of scene 10 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_10_frame_914.jpg\n",
      "Extracted and saved middle frame of scene 11 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_11_frame_1003.jpg\n",
      "Extracted and saved middle frame of scene 12 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_12_frame_1066.jpg\n",
      "Extracted and saved middle frame of scene 13 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_13_frame_1162.jpg\n",
      "Extracted and saved middle frame of scene 14 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_14_frame_1306.jpg\n",
      "Extracted and saved middle frame of scene 15 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_15_frame_1411.jpg\n",
      "Extracted and saved middle frame of scene 16 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_16_frame_1587.jpg\n",
      "Extracted and saved middle frame of scene 17 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_17_frame_1738.jpg\n",
      "Extracted and saved middle frame of scene 18 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_18_frame_1812.jpg\n",
      "Extracted and saved middle frame of scene 19 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_19_frame_1907.jpg\n",
      "Extracted and saved middle frame of scene 20 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_20_frame_2000.jpg\n",
      "Extracted and saved middle frame of scene 21 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_21_frame_2129.jpg\n",
      "Extracted and saved middle frame of scene 22 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_22_frame_2286.jpg\n",
      "Extracted and saved middle frame of scene 23 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_23_frame_2429.jpg\n",
      "Extracted and saved middle frame of scene 24 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_24_frame_2517.jpg\n",
      "Extracted and saved middle frame of scene 25 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_25_frame_2622.jpg\n",
      "Extracted and saved middle frame of scene 26 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_26_frame_2747.jpg\n",
      "Extracted and saved middle frame of scene 27 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_27_frame_2868.jpg\n",
      "Extracted and saved middle frame of scene 28 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_28_frame_2995.jpg\n",
      "Extracted and saved middle frame of scene 29 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_29_frame_3126.jpg\n",
      "Extracted and saved middle frame of scene 30 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_30_frame_3237.jpg\n",
      "Extracted and saved middle frame of scene 31 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_31_frame_3342.jpg\n",
      "Extracted and saved middle frame of scene 32 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_32_frame_3465.jpg\n",
      "Extracted and saved middle frame of scene 33 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_33_frame_3548.jpg\n",
      "Extracted and saved middle frame of scene 34 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_34_frame_3647.jpg\n",
      "Extracted and saved middle frame of scene 35 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_35_frame_3780.jpg\n",
      "Extracted and saved middle frame of scene 36 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_36_frame_3897.jpg\n",
      "Extracted and saved middle frame of scene 37 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_37_frame_3974.jpg\n",
      "Extracted and saved middle frame of scene 38 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_38_frame_4098.jpg\n",
      "Extracted and saved middle frame of scene 39 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_39_frame_4220.jpg\n",
      "Extracted and saved middle frame of scene 40 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_40_frame_4316.jpg\n",
      "Extracted and saved middle frame of scene 41 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_41_frame_4430.jpg\n",
      "Extracted and saved middle frame of scene 42 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_42_frame_4503.jpg\n",
      "Extracted and saved middle frame of scene 43 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_43_frame_4559.jpg\n",
      "Extracted and saved middle frame of scene 44 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_44_frame_4613.jpg\n",
      "Extracted and saved middle frame of scene 45 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_45_frame_4652.jpg\n",
      "Extracted and saved middle frame of scene 46 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_46_frame_4707.jpg\n",
      "Extracted and saved middle frame of scene 47 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_47_frame_4763.jpg\n",
      "Extracted and saved middle frame of scene 48 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_48_frame_4802.jpg\n",
      "Extracted and saved middle frame of scene 49 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_49_frame_4844.jpg\n",
      "Extracted and saved middle frame of scene 50 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_50_frame_4914.jpg\n",
      "Extracted and saved middle frame of scene 51 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_51_frame_5101.jpg\n",
      "Extracted and saved middle frame of scene 52 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_52_frame_5265.jpg\n",
      "Extracted and saved middle frame of scene 53 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_53_frame_5317.jpg\n",
      "Extracted and saved middle frame of scene 54 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_54_frame_5364.jpg\n",
      "Extracted and saved middle frame of scene 55 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_55_frame_5460.jpg\n",
      "Extracted and saved middle frame of scene 56 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_56_frame_5553.jpg\n",
      "Extracted and saved middle frame of scene 57 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_57_frame_5596.jpg\n",
      "Extracted and saved middle frame of scene 58 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_58_frame_5687.jpg\n",
      "Extracted and saved middle frame of scene 59 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_59_frame_5777.jpg\n",
      "Extracted and saved middle frame of scene 60 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_60_frame_5863.jpg\n",
      "Extracted and saved middle frame of scene 61 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_61_frame_6014.jpg\n",
      "Extracted and saved middle frame of scene 62 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_62_frame_6137.jpg\n",
      "Extracted and saved middle frame of scene 63 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_63_frame_6196.jpg\n",
      "Extracted and saved middle frame of scene 64 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_64_frame_6271.jpg\n",
      "Extracted and saved middle frame of scene 65 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_65_frame_6365.jpg\n",
      "Extracted and saved middle frame of scene 66 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_66_frame_6442.jpg\n",
      "Extracted and saved middle frame of scene 67 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_67_frame_6525.jpg\n",
      "Extracted and saved middle frame of scene 68 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_68_frame_6606.jpg\n",
      "Extracted and saved middle frame of scene 69 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_69_frame_6707.jpg\n",
      "Extracted and saved middle frame of scene 70 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_70_frame_6862.jpg\n",
      "Extracted and saved middle frame of scene 71 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_71_frame_6997.jpg\n",
      "Extracted and saved middle frame of scene 72 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_72_frame_7089.jpg\n",
      "Extracted and saved middle frame of scene 73 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_73_frame_7186.jpg\n",
      "Extracted and saved middle frame of scene 74 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_74_frame_7306.jpg\n",
      "Extracted and saved middle frame of scene 75 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_75_frame_7413.jpg\n",
      "Extracted and saved middle frame of scene 76 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_76_frame_7485.jpg\n",
      "Extracted and saved middle frame of scene 77 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_77_frame_7715.jpg\n",
      "Extracted and saved middle frame of scene 78 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_78_frame_8014.jpg\n",
      "Extracted and saved middle frame of scene 79 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_79_frame_8136.jpg\n",
      "Extracted and saved middle frame of scene 80 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_80_frame_8265.jpg\n",
      "Extracted and saved middle frame of scene 81 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_81_frame_8497.jpg\n",
      "Extracted and saved middle frame of scene 82 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_82_frame_8637.jpg\n",
      "Extracted and saved middle frame of scene 83 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_83_frame_8673.jpg\n",
      "Extracted and saved middle frame of scene 84 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_84_frame_8745.jpg\n",
      "Extracted and saved middle frame of scene 85 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_85_frame_8821.jpg\n",
      "Extracted and saved middle frame of scene 86 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_86_frame_8886.jpg\n",
      "Extracted and saved middle frame of scene 87 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_87_frame_8952.jpg\n",
      "Extracted and saved middle frame of scene 88 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_88_frame_9013.jpg\n",
      "Extracted and saved middle frame of scene 89 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_89_frame_9155.jpg\n",
      "Extracted and saved middle frame of scene 90 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_90_frame_9277.jpg\n",
      "Extracted and saved middle frame of scene 91 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_91_frame_9391.jpg\n",
      "Extracted and saved middle frame of scene 92 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_92_frame_9529.jpg\n",
      "Extracted and saved middle frame of scene 93 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_93_frame_9634.jpg\n",
      "Extracted and saved middle frame of scene 94 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_94_frame_9733.jpg\n",
      "Extracted and saved middle frame of scene 95 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_95_frame_9886.jpg\n",
      "Extracted and saved middle frame of scene 96 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_96_frame_10046.jpg\n",
      "Extracted and saved middle frame of scene 97 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_97_frame_10198.jpg\n",
      "Extracted and saved middle frame of scene 98 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_98_frame_10350.jpg\n",
      "Extracted and saved middle frame of scene 99 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_99_frame_10399.jpg\n",
      "Extracted and saved middle frame of scene 100 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_100_frame_10438.jpg\n",
      "Extracted and saved middle frame of scene 101 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_101_frame_10492.jpg\n",
      "Extracted and saved middle frame of scene 102 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_102_frame_10537.jpg\n",
      "Extracted and saved middle frame of scene 103 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_103_frame_10616.jpg\n",
      "Extracted and saved middle frame of scene 104 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_104_frame_10709.jpg\n",
      "Extracted and saved middle frame of scene 105 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_105_frame_10772.jpg\n",
      "Extracted and saved middle frame of scene 106 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_106_frame_10921.jpg\n",
      "Extracted and saved middle frame of scene 107 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_107_frame_11084.jpg\n",
      "Extracted and saved middle frame of scene 108 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_108_frame_11178.jpg\n",
      "Extracted and saved middle frame of scene 109 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_109_frame_11325.jpg\n",
      "Extracted and saved middle frame of scene 110 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_110_frame_11458.jpg\n",
      "Extracted and saved middle frame of scene 111 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_111_frame_11582.jpg\n",
      "Extracted and saved middle frame of scene 112 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_112_frame_11702.jpg\n",
      "Extracted and saved middle frame of scene 113 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_113_frame_11811.jpg\n",
      "Extracted and saved middle frame of scene 114 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_114_frame_12113.jpg\n",
      "Extracted and saved middle frame of scene 115 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_115_frame_12382.jpg\n",
      "Extracted and saved middle frame of scene 116 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_116_frame_12485.jpg\n",
      "Extracted and saved middle frame of scene 117 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_117_frame_12584.jpg\n",
      "Extracted and saved middle frame of scene 118 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_118_frame_12768.jpg\n",
      "Extracted and saved middle frame of scene 119 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_119_frame_12935.jpg\n",
      "Extracted and saved middle frame of scene 120 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_120_frame_12998.jpg\n",
      "Extracted and saved middle frame of scene 121 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_121_frame_13098.jpg\n",
      "Extracted and saved middle frame of scene 122 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_122_frame_13240.jpg\n",
      "Extracted and saved middle frame of scene 123 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_123_frame_13397.jpg\n",
      "Extracted and saved middle frame of scene 124 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_124_frame_13535.jpg\n",
      "Extracted and saved middle frame of scene 125 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_125_frame_13612.jpg\n",
      "Extracted and saved middle frame of scene 126 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_126_frame_13665.jpg\n",
      "Extracted and saved middle frame of scene 127 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_127_frame_13747.jpg\n",
      "Extracted and saved middle frame of scene 128 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_128_frame_13906.jpg\n",
      "Extracted and saved middle frame of scene 129 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_129_frame_14053.jpg\n",
      "Extracted and saved middle frame of scene 130 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_130_frame_14121.jpg\n",
      "Extracted and saved middle frame of scene 131 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_131_frame_14180.jpg\n",
      "Extracted and saved middle frame of scene 132 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_132_frame_14314.jpg\n",
      "Extracted and saved middle frame of scene 133 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_133_frame_14473.jpg\n",
      "Extracted and saved middle frame of scene 134 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_134_frame_14551.jpg\n",
      "Extracted and saved middle frame of scene 135 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_135_frame_14653.jpg\n",
      "Extracted and saved middle frame of scene 136 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_136_frame_14733.jpg\n",
      "Extracted and saved middle frame of scene 137 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_137_frame_14834.jpg\n",
      "Extracted and saved middle frame of scene 138 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_138_frame_14947.jpg\n",
      "Extracted and saved middle frame of scene 139 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_139_frame_15009.jpg\n",
      "Extracted and saved middle frame of scene 140 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_140_frame_15062.jpg\n",
      "Extracted and saved middle frame of scene 141 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_141_frame_15104.jpg\n",
      "Extracted and saved middle frame of scene 142 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_142_frame_15187.jpg\n",
      "Extracted and saved middle frame of scene 143 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_143_frame_15317.jpg\n",
      "Extracted and saved middle frame of scene 144 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_144_frame_15473.jpg\n",
      "Extracted and saved middle frame of scene 145 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_145_frame_15663.jpg\n",
      "Extracted and saved middle frame of scene 146 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_146_frame_15808.jpg\n",
      "Extracted and saved middle frame of scene 147 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_147_frame_15955.jpg\n",
      "Extracted and saved middle frame of scene 148 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_148_frame_16085.jpg\n",
      "Extracted and saved middle frame of scene 149 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_149_frame_16133.jpg\n",
      "Extracted and saved middle frame of scene 150 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_150_frame_16170.jpg\n",
      "Extracted and saved middle frame of scene 151 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_151_frame_16243.jpg\n",
      "Extracted and saved middle frame of scene 152 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_152_frame_16324.jpg\n",
      "Extracted and saved middle frame of scene 153 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_153_frame_16357.jpg\n",
      "Extracted and saved middle frame of scene 154 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_154_frame_16390.jpg\n",
      "Extracted and saved middle frame of scene 155 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_155_frame_16482.jpg\n",
      "Extracted and saved middle frame of scene 156 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_156_frame_16616.jpg\n",
      "Extracted and saved middle frame of scene 157 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_157_frame_16697.jpg\n",
      "Extracted and saved middle frame of scene 158 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_158_frame_16860.jpg\n",
      "Extracted and saved middle frame of scene 159 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output/scene_159_frame_17004.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_1_frame_104.jpg as scene_1_frame_104_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_4_frame_610.jpg as scene_4_frame_610_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_5_frame_642.jpg as scene_5_frame_642_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_2_frame_346.jpg as scene_2_frame_346_analysis.json\n",
      "Saved analysis for scene_3_frame_542.jpg as scene_3_frame_542_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_6_frame_690.jpg as scene_6_frame_690_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_7_frame_732.jpg as scene_7_frame_732_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_11_frame_1003.jpg as scene_11_frame_1003_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_9_frame_839.jpg as scene_9_frame_839_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_12_frame_1066.jpg as scene_12_frame_1066_analysis.json\n",
      "Saved analysis for scene_8_frame_773.jpg as scene_8_frame_773_analysis.json\n",
      "Saved analysis for scene_10_frame_914.jpg as scene_10_frame_914_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_13_frame_1162.jpg as scene_13_frame_1162_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_14_frame_1306.jpg as scene_14_frame_1306_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_15_frame_1411.jpg as scene_15_frame_1411_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_17_frame_1738.jpg as scene_17_frame_1738_analysis.json\n",
      "Saved analysis for scene_16_frame_1587.jpg as scene_16_frame_1587_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_18_frame_1812.jpg as scene_18_frame_1812_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_19_frame_1907.jpg as scene_19_frame_1907_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_21_frame_2129.jpg as scene_21_frame_2129_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_22_frame_2286.jpg as scene_22_frame_2286_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_20_frame_2000.jpg as scene_20_frame_2000_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_23_frame_2429.jpg as scene_23_frame_2429_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_24_frame_2517.jpg as scene_24_frame_2517_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_26_frame_2747.jpg as scene_26_frame_2747_analysis.json\n",
      "Saved analysis for scene_28_frame_2995.jpg as scene_28_frame_2995_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Scenes:  16%|█▋        | 26/159 [00:55<04:43,  2.13s/scene]\n",
      "Processing Videos:   0%|          | 0/1 [01:38<?, ?video/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m video_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/02_Video_DB\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     47\u001b[0m output_base_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 49\u001b[0m process_videos_in_directory(video_directory, output_base_directory)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFINISHED PROCESSING ALL VIDEOS.\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[51], line 26\u001b[0m, in \u001b[0;36mprocess_videos_in_directory\u001b[0;34m(directory_path, output_base_dir)\u001b[0m\n\u001b[1;32m     24\u001b[0m scenes \u001b[38;5;241m=\u001b[39m analyze_video(video_path)\n\u001b[1;32m     25\u001b[0m extract_frames_imageio(video_path, scenes, scenes_output_dir)\n\u001b[0;32m---> 26\u001b[0m asyncio\u001b[38;5;241m.\u001b[39mrun(process_scenes_output(scenes_output_dir, json_output_dir))  \u001b[38;5;66;03m# Run async scene processing\u001b[39;00m\n\u001b[1;32m     28\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     29\u001b[0m processing_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nest_asyncio.py:35\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     33\u001b[0m task\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(asyncio\u001b[38;5;241m.\u001b[39mCancelledError):\n\u001b[0;32m---> 35\u001b[0m     loop\u001b[38;5;241m.\u001b[39mrun_until_complete(task)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_once()\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nest_asyncio.py:133\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m curr_task \u001b[38;5;241m=\u001b[39m curr_tasks\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m     handle\u001b[38;5;241m.\u001b[39m_run()\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# restore the current task\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/asyncio/events.py:80\u001b[0m, in \u001b[0;36mHandle._run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args)\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/asyncio/tasks.py:342\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    339\u001b[0m     future\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;66;03m# Don't pass the value of `future.result()` explicitly,\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;66;03m# as `Future.__iter__` and `Future.__await__` don't need it.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;66;03m# instead of `__next__()`, which is slower for futures\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;66;03m# that return non-generator iterators from their `__iter__`.\u001b[39;00m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/asyncio/tasks.py:269\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    267\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[49], line 7\u001b[0m, in \u001b[0;36mprocess_scenes_output\u001b[0;34m(output_dir, json_output_dir)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mtotal_scenes, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Scenes\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscene\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m      6\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m [process_single_scene(i, scene, output_dir, json_output_dir, pbar) \u001b[38;5;28;01mfor\u001b[39;00m i, scene \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(scenes)]\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/asyncio/tasks.py:339\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 339\u001b[0m         future\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_until_complete(task)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_once()\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/nest_asyncio.py:133\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m curr_task \u001b[38;5;241m=\u001b[39m curr_tasks\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m     handle\u001b[38;5;241m.\u001b[39m_run()\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# restore the current task\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/asyncio/events.py:80\u001b[0m, in \u001b[0;36mHandle._run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args)\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/asyncio/tasks.py:350\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;66;03m# Don't pass the value of `future.result()` explicitly,\u001b[39;00m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;66;03m# as `Future.__iter__` and `Future.__await__` don't need it.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;66;03m# instead of `__next__()`, which is slower for futures\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;66;03m# that return non-generator iterators from their `__iter__`.\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step()\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/asyncio/tasks.py:267\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[0;32mIn[49], line 18\u001b[0m, in \u001b[0;36mprocess_single_scene\u001b[0;34m(i, scene, output_dir, json_output_dir, pbar)\u001b[0m\n\u001b[1;32m     15\u001b[0m base64_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m encode_image(scene_path)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Perform color analysis\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m color_analysis_result \u001b[38;5;241m=\u001b[39m analyze_image_colors(scene_path)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Send image to OpenAI for further analysis\u001b[39;00m\n\u001b[1;32m     21\u001b[0m openai_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m send_image_to_openai(scene_path, base64_image)\n",
      "Cell \u001b[0;32mIn[37], line 33\u001b[0m, in \u001b[0;36manalyze_image_colors\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     30\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(image)\n\u001b[0;32m---> 33\u001b[0m unique_colors, counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(data\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     34\u001b[0m total_pixels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(counts\u001b[38;5;241m.\u001b[39msum())\n\u001b[1;32m     36\u001b[0m color_counts \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRed\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYellow\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-primary\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     44\u001b[0m }\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/lib/arraysetops.py:317\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    314\u001b[0m     uniq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmoveaxis(uniq, \u001b[38;5;241m0\u001b[39m, axis)\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m uniq\n\u001b[0;32m--> 317\u001b[0m output \u001b[38;5;241m=\u001b[39m _unique1d(consolidated, return_index,\n\u001b[1;32m    318\u001b[0m                    return_inverse, return_counts, equal_nan\u001b[38;5;241m=\u001b[39mequal_nan)\n\u001b[1;32m    319\u001b[0m output \u001b[38;5;241m=\u001b[39m (reshape_uniq(output[\u001b[38;5;241m0\u001b[39m]),) \u001b[38;5;241m+\u001b[39m output[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(output)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/lib/arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    334\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar[perm]\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     ar\u001b[38;5;241m.\u001b[39msort()\n\u001b[1;32m    337\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar\n\u001b[1;32m    338\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(aux\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mbool_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def process_videos_in_directory(directory_path, output_base_dir):\n",
    "    video_files = [f for f in os.listdir(directory_path) if f.endswith(('.mp4', '.avi', '.mkv'))]\n",
    "\n",
    "    if not video_files:\n",
    "        print(\"No video files found in the directory.\", flush=True)\n",
    "        return\n",
    "\n",
    "    with tqdm(total=len(video_files), desc=\"Processing Videos\", unit=\"video\") as pbar:\n",
    "        for i, video_file in enumerate(video_files):\n",
    "            start_time = time.time()\n",
    "\n",
    "            video_path = os.path.join(directory_path, video_file)\n",
    "            video_name = os.path.splitext(video_file)[0]\n",
    "            video_size = os.path.getsize(video_path)\n",
    "\n",
    "            video_output_dir = os.path.join(output_base_dir, video_name)\n",
    "            scenes_output_dir = os.path.join(video_output_dir, 'scenes_output')\n",
    "            json_output_dir = os.path.join(video_output_dir, 'json_output')\n",
    "\n",
    "            os.makedirs(scenes_output_dir, exist_ok=True)\n",
    "\n",
    "            print(f\"Processing video {i + 1}/{len(video_files)}: {video_file}\", flush=True)\n",
    "            \n",
    "            scenes = analyze_video(video_path)\n",
    "            extract_frames_imageio(video_path, scenes, scenes_output_dir)\n",
    "            asyncio.run(process_scenes_output(scenes_output_dir, json_output_dir))  # Run async scene processing\n",
    "\n",
    "            end_time = time.time()\n",
    "            processing_time = end_time - start_time\n",
    "            \n",
    "            # Calculate video length (in seconds)\n",
    "            video_length = get_video_length(video_path)\n",
    "\n",
    "            # Calculate scenes per minute\n",
    "            scenes_per_minute = len(scenes) / (video_length / 60)\n",
    "\n",
    "            summary = {\n",
    "                \"Video Title\": video_file,\n",
    "                \"File Size (bytes)\": video_size,\n",
    "                \"Video Length (seconds)\": video_length,\n",
    "                \"Number of Scenes\": len(scenes),\n",
    "                \"Scenes per Minute\": scenes_per_minute,\n",
    "                \"Processing Time (seconds)\": processing_time\n",
    "            }\n",
    "\n",
    "            summary_output_path = os.path.join(video_output_dir, f\"{video_name}_summary.json\")\n",
    "            with open(summary_output_path, 'w') as summary_file:\n",
    "                json.dump(summary, summary_file, indent=4)\n",
    "\n",
    "            # Additional processing (entity consolidation and comparison)\n",
    "            asyncio.run(run_additional_processing(json_output_dir, scenes_output_dir, summary_output_path, video_file, video_size, processing_time))\n",
    "\n",
    "            print(f\"Finished processing video: {video_file}\", flush=True)\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "async def run_additional_processing(json_output_dir, image_output_dir, summary_output_path, video_file, video_size, processing_time):\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load all JSON files data\n",
    "    all_json_files_data = []\n",
    "    json_files = [f for f in os.listdir(json_output_dir) if f.endswith('.json')]\n",
    "    for json_file in json_files:\n",
    "        with open(os.path.join(json_output_dir, json_file), 'r') as file:\n",
    "            all_json_files_data.append(json.load(file))\n",
    "\n",
    "    with tqdm(total=4, desc=\"Overall Progress\", unit=\"step\") as overall_pbar:\n",
    "        # Step 1: Initial extraction and consolidation\n",
    "        consolidated_data = await initial_consolidation(json_output_dir, image_output_dir)\n",
    "        print_consolidated_data(consolidated_data, title=\"Initial\")\n",
    "        overall_pbar.update(1)\n",
    "\n",
    "        # Step 2: Consolidate entities using OpenAI\n",
    "        consolidated_data, merged_entities_log = await consolidate_all_entities(consolidated_data, api_key)\n",
    "        print_consolidated_data(consolidated_data, title=\"After OpenAI Consolidation\")\n",
    "        overall_pbar.update(1)\n",
    "\n",
    "        # Step 3: Handle image-to-image comparisons for portions\n",
    "        consolidated_data, portion_merged_log = await handle_portion_comparisons(consolidated_data, api_key)\n",
    "        merged_entities_log.extend(portion_merged_log)\n",
    "        print_consolidated_data(consolidated_data, title=\"After Portion Comparisons\")\n",
    "        overall_pbar.update(1)\n",
    "\n",
    "        # Step 4: Final image-to-image comparison for characters\n",
    "        consolidated_data, final_character_merged_log = await final_image_to_image_comparison_for_characters(consolidated_data, api_key)\n",
    "        merged_entities_log.extend(final_character_merged_log)\n",
    "        print_consolidated_data(consolidated_data, title=\"Final After Image Comparisons\")\n",
    "        overall_pbar.update(1)\n",
    "\n",
    "    end_time = time.time()\n",
    "    processing_time += (end_time - start_time)\n",
    "\n",
    "    save_summary_to_json(consolidated_data, summary_output_path, video_file, video_size, processing_time, merged_entities_log, all_json_files_data)\n",
    "\n",
    "\n",
    "# Main script execution\n",
    "video_directory = '/Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/02_Video_DB'\n",
    "output_base_directory = '/Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB'\n",
    "\n",
    "process_videos_in_directory(video_directory, output_base_directory)\n",
    "print(\"FINISHED PROCESSING ALL VIDEOS.\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10) Analysing all of outputs for FINAL ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Breakdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Path Construction: get_image_path generates the correct path to the image file based on the JSON filename.\n",
    "\n",
    "Entity Extraction:extract_entities_from_json pulls characters, objects, and places from the JSON data.\n",
    "\n",
    "Image-to-Image Comparison:perform_image_to_image_comparison compares partial objects with full objects using the OpenAI API.\n",
    "\n",
    "Entity Comparison:compare_entities handles both name-based and image-based comparisons to decide whether two entities should be consolidated.\n",
    "\n",
    "Consolidation:Entities across frames are consolidated into a single summary file that tracks where each entity was found.\n",
    "\n",
    "Main Execution:The script runs through all JSON files, processes the entities, and saves the consolidated results to a summary JSON file.\n",
    "\n",
    "Key Features of This Implementation:\n",
    "Text-Based Comparison: The code first attempts to merge entities based on exact name matches. If no match is found, it uses the OpenAI API to determine if two entities with different names should be merged.\n",
    "\n",
    "Image-to-Image Comparison: If one of the entities is flagged as a portion, or if names don't match but the entities might still be the same, the code performs an image-to-image comparison using the OpenAI API.\n",
    "\n",
    "Efficient Processing: The code processes each frame sequentially and logs all merges into merged_entities_log, ensuring you have a record of what entities were merged, including their original names and frames.\n",
    "\n",
    "No Overwritten Functionality: The original image analysis functionality is preserved and integrated smoothly with the text-based comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santiagowon/anaconda3/lib/python3.11/site-packages/IPython/core/compilerop.py:86: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  return compile(source, filename, symbol, self.flags | PyCF_ONLY_AST, 1)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Overall Progress:   0%|          | 0/4 [00:00<?, ?step/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Characters:\n",
      "Banana character: Found in frames [2995]\n",
      "Mouse Character: Found in frames [2747]\n",
      "Cartoon character: Found in frames [2517]\n",
      "Banana Character 1: Found in frames [542, 642]\n",
      "Banana Character 2: Found in frames [542, 642]\n",
      "Cartoon Character: Found in frames [610]\n",
      "\n",
      "Initial Objects:\n",
      "Cloud: Found in frames [690]\n",
      "Box: Found in frames [690]\n",
      "Banana character: Found in frames [2995]\n",
      "Mouse Character: Found in frames [2747]\n",
      "Foot: Found in frames [2517]\n",
      "Chair: Found in frames [2517]\n",
      "Grass: Found in frames [2517]\n",
      "Door: Found in frames [732]\n",
      "Tree: Found in frames [542, 610, 642]\n",
      "Banana Characters: Found in frames [542, 642]\n",
      "Blue Box: Found in frames [542]\n",
      "Feet: Found in frames [610]\n",
      "Ground: Found in frames [642]\n",
      "\n",
      "Initial Places:\n",
      "Sky: Found in frames [690]\n",
      "Blue room: Found in frames [2995]\n",
      "Cartoon Nature Background: Found in frames [2747]\n",
      "Cartoon setting: Found in frames [2517]\n",
      "Cartoon Garden: Found in frames [732, 642]\n",
      "Backyard: Found in frames [542]\n",
      "Grass Area: Found in frames [610]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  50%|█████     | 2/4 [00:24<00:24, 12.15s/step]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After OpenAI Consolidation Characters:\n",
      "Banana character: Found in frames [2995]\n",
      "Mouse Character: Found in frames [2747]\n",
      "Cartoon character: Found in frames [2517]\n",
      "Banana Character 1: Found in frames [542, 642]\n",
      "Banana Character 2: Found in frames [542, 642]\n",
      "Cartoon Character: Found in frames [610]\n",
      "\n",
      "After OpenAI Consolidation Objects:\n",
      "Cloud: Found in frames [690]\n",
      "Box: Found in frames [690]\n",
      "Banana character: Found in frames [2995]\n",
      "Mouse Character: Found in frames [2747]\n",
      "Foot: Found in frames [2517]\n",
      "Chair: Found in frames [2517]\n",
      "Grass: Found in frames [2517]\n",
      "Door: Found in frames [732]\n",
      "Tree: Found in frames [542, 610, 642]\n",
      "Banana Characters: Found in frames [542, 642]\n",
      "Blue Box: Found in frames [542]\n",
      "Feet: Found in frames [610]\n",
      "Ground: Found in frames [642]\n",
      "\n",
      "After OpenAI Consolidation Places:\n",
      "Sky: Found in frames [690]\n",
      "Blue room: Found in frames [2995]\n",
      "Cartoon Nature Background: Found in frames [2747]\n",
      "Cartoon setting: Found in frames [2517]\n",
      "Cartoon Garden: Found in frames [732, 642]\n",
      "Backyard: Found in frames [542]\n",
      "Grass Area: Found in frames [610]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  75%|███████▌  | 3/4 [00:32<00:10, 10.66s/step]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Portion Comparisons Characters:\n",
      "Banana character: Found in frames [2995]\n",
      "Mouse Character: Found in frames [2747]\n",
      "Cartoon character: Found in frames [2517]\n",
      "Banana Character 1: Found in frames [542, 642]\n",
      "Banana Character 2: Found in frames [542, 642]\n",
      "Cartoon Character: Found in frames [610]\n",
      "\n",
      "After Portion Comparisons Objects:\n",
      "Cloud: Found in frames [690]\n",
      "Box: Found in frames [690]\n",
      "Banana character: Found in frames [2995]\n",
      "Mouse Character: Found in frames [2747]\n",
      "Foot: Found in frames [2517]\n",
      "Chair: Found in frames [2517]\n",
      "Grass: Found in frames [2517]\n",
      "Door: Found in frames [732]\n",
      "Tree: Found in frames [542, 610, 642]\n",
      "Banana Characters: Found in frames [542, 642]\n",
      "Blue Box: Found in frames [542]\n",
      "Feet: Found in frames [610]\n",
      "Ground: Found in frames [642]\n",
      "\n",
      "After Portion Comparisons Places:\n",
      "Sky: Found in frames [690]\n",
      "Blue room: Found in frames [2995]\n",
      "Cartoon Nature Background: Found in frames [2747]\n",
      "Cartoon setting: Found in frames [2517]\n",
      "Cartoon Garden: Found in frames [732, 642]\n",
      "Backyard: Found in frames [542]\n",
      "Grass Area: Found in frames [610]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Character Comparisons: 100%|██████████| 15/15 [00:06<00:00,  2.41comparison/s]\n",
      "Overall Progress: 100%|██████████| 4/4 [00:39<00:00,  9.78s/step]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final After Image Comparisons Characters:\n",
      "Banana character: Found in frames [2995]\n",
      "Mouse Character: Found in frames [2747]\n",
      "Cartoon character: Found in frames [2517]\n",
      "Banana Character 1: Found in frames [542, 642]\n",
      "Banana Character 2: Found in frames [542, 642]\n",
      "Cartoon Character: Found in frames [610]\n",
      "\n",
      "Final After Image Comparisons Objects:\n",
      "Cloud: Found in frames [690]\n",
      "Box: Found in frames [690]\n",
      "Banana character: Found in frames [2995]\n",
      "Mouse Character: Found in frames [2747]\n",
      "Foot: Found in frames [2517]\n",
      "Chair: Found in frames [2517]\n",
      "Grass: Found in frames [2517]\n",
      "Door: Found in frames [732]\n",
      "Tree: Found in frames [542, 610, 642]\n",
      "Banana Characters: Found in frames [542, 642]\n",
      "Blue Box: Found in frames [542]\n",
      "Feet: Found in frames [610]\n",
      "Ground: Found in frames [642]\n",
      "\n",
      "Final After Image Comparisons Places:\n",
      "Sky: Found in frames [690]\n",
      "Blue room: Found in frames [2995]\n",
      "Cartoon Nature Background: Found in frames [2747]\n",
      "Cartoon setting: Found in frames [2517]\n",
      "Cartoon Garden: Found in frames [732, 642]\n",
      "Backyard: Found in frames [542]\n",
      "Grass Area: Found in frames [610]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import aiohttp\n",
    "# import asyncio\n",
    "# import base64\n",
    "# import re\n",
    "# import time\n",
    "# import nest_asyncio\n",
    "# from tqdm.asyncio import tqdm\n",
    "\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# # Function to construct the image path based on the JSON file\n",
    "# def get_image_path(json_filename, image_directory):\n",
    "#     image_filename = json_filename.replace(\"_analysis.json\", \".jpg\")\n",
    "#     return os.path.join(image_directory, image_filename)\n",
    "\n",
    "# # Function to extract entities from a JSON file\n",
    "# def extract_entities_from_json(json_data):\n",
    "#     entities = {\n",
    "#         \"characters\": [],\n",
    "#         \"objects\": [],\n",
    "#         \"places\": []\n",
    "#     }\n",
    "\n",
    "#     if \"Image Analysis\" in json_data:\n",
    "#         if \"Characters\" in json_data[\"Image Analysis\"] and json_data[\"Image Analysis\"][\"Characters\"][\"Total Characters Identified\"] > 0:\n",
    "#             for character in json_data[\"Image Analysis\"][\"Characters\"][\"Character Details\"].values():\n",
    "#                 entities[\"characters\"].append(character)\n",
    "        \n",
    "#         if \"Objects\" in json_data[\"Image Analysis\"] and json_data[\"Image Analysis\"][\"Objects\"][\"Total Objects Identified\"] > 0:\n",
    "#             for obj in json_data[\"Image Analysis\"][\"Objects\"][\"Objects Details\"].values():\n",
    "#                 entities[\"objects\"].append(obj)\n",
    "        \n",
    "#         if \"Place\" in json_data[\"Image Analysis\"]:\n",
    "#             entities[\"places\"].append(json_data[\"Image Analysis\"][\"Place\"])\n",
    "\n",
    "#     return entities\n",
    "\n",
    "# # Function to encode an image to base64\n",
    "# def encode_image_to_base64(image_path):\n",
    "#     if not image_path or not os.path.isfile(image_path):\n",
    "#         raise ValueError(f\"Image path cannot be None or invalid: {image_path}\")\n",
    "#     with open(image_path, \"rb\") as image_file:\n",
    "#         return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# # Function to perform image-to-image comparison using OpenAI API\n",
    "# async def perform_image_to_image_comparison(entity1, entity2, image_path1, image_path2, api_key):\n",
    "#     base64_image1 = encode_image_to_base64(image_path1)\n",
    "#     base64_image2 = encode_image_to_base64(image_path2)\n",
    "\n",
    "#     prompt = \"\"\"\n",
    "#     You are an expert in image analysis. Compare the two provided images and determine if they represent the same object or character, even if one is a partial view. Consider features, colors, and context. For instance, if the identified entity is a limb (e.g., a leg), you could contrast that limb with other full views of characters and see if it matches.\n",
    "\n",
    "#     Return 'True' if the images depict the same object or character, 'False' if they are different, and 'Uncertain' if unsure.\n",
    "#     \"\"\"\n",
    "\n",
    "#     headers = {\n",
    "#         \"Content-Type\": \"application/json\",\n",
    "#         \"Authorization\": f\"Bearer {api_key}\"\n",
    "#     }\n",
    "\n",
    "#     payload = {\n",
    "#         \"model\": \"gpt-4\",\n",
    "#         \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "#         \"images\": [{\"image\": base64_image1}, {\"image\": base64_image2}],\n",
    "#         \"max_tokens\": 100\n",
    "#     }\n",
    "\n",
    "#     async with aiohttp.ClientSession() as session:\n",
    "#         try:\n",
    "#             async with session.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload) as response:\n",
    "#                 response_json = await response.json()\n",
    "#                 answer = response_json.get('choices', [{}])[0].get('message', {}).get('content', '').strip().lower()\n",
    "                \n",
    "#                 if \"uncertain\" in answer:\n",
    "#                     return \"uncertain\"\n",
    "#                 elif \"true\" in answer:\n",
    "#                     return \"true\"\n",
    "#                 else:\n",
    "#                     return \"false\"\n",
    "#         except KeyError as e:\n",
    "#             print(f\"Error accessing API response: {e}\")\n",
    "#             return \"false\"\n",
    "#         except Exception as e:\n",
    "#             print(f\"Unexpected error: {e}\")\n",
    "#             return \"false\"\n",
    "\n",
    "\n",
    "# # Function to prompt OpenAI for entity consolidation\n",
    "# async def consolidate_entities_with_openai(entity_type, entity_list, api_key):\n",
    "#     if not entity_list:\n",
    "#         return entity_list  # No entities to process\n",
    "\n",
    "#     prompt = f\"\"\"\n",
    "#     You are an expert in entity recognition and consolidation. Here is a list of {entity_type}. The list may include variations in the names or descriptions that refer to the same entity. Please identify which entities refer to the same concept or character and suggest how they should be merged under a single, consistent name. For example, if 'Banana character', 'Banana Character 1', and 'Banana Man' refer to the same entity, suggest that they should be merged under one name.\n",
    "\n",
    "#     List of {entity_type}:\n",
    "#     {', '.join(entity_list)}\n",
    "\n",
    "#     Please return a JSON object with the consolidated list of entities where similar entities are merged under a single name.\n",
    "#     \"\"\"\n",
    "\n",
    "    \n",
    "#     headers = {\n",
    "#         \"Content-Type\": \"application/json\",\n",
    "#         \"Authorization\": f\"Bearer {api_key}\"\n",
    "#     }\n",
    "\n",
    "#     payload = {\n",
    "#         \"model\": \"gpt-4\",\n",
    "#         \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "#         \"max_tokens\": 500\n",
    "#     }\n",
    "\n",
    "#     async with aiohttp.ClientSession() as session:\n",
    "#         try:\n",
    "#             async with session.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload) as response:\n",
    "#                 response_json = await response.json()\n",
    "#                 consolidated_list = response_json.get('choices', [{}])[0].get('message', {}).get('content', '').strip()\n",
    "#                 try:\n",
    "#                     return json.loads(consolidated_list)\n",
    "#                 except json.JSONDecodeError as e:\n",
    "#                     print(f\"Error decoding JSON from OpenAI response: {e}\")\n",
    "#                     return entity_list  # Fallback to original list if parsing fails\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error during entity consolidation: {e}\")\n",
    "#             return entity_list  # Fallback to original list if API call fails\n",
    "\n",
    "# # Function to apply the consolidation based on the OpenAI response\n",
    "# def apply_consolidation(consolidated_data, entity_type, new_entity_list, merged_entities_log):\n",
    "#     # Mapping old entities to new ones based on OpenAI suggestions\n",
    "#     old_to_new_map = {}\n",
    "#     for new_entity in new_entity_list:\n",
    "#         if isinstance(new_entity, dict):\n",
    "#             for old_entity in new_entity[\"merge\"]:\n",
    "#                 old_to_new_map[old_entity] = new_entity[\"name\"]\n",
    "\n",
    "#     for old_entity, new_entity in old_to_new_map.items():\n",
    "#         if old_entity in consolidated_data[entity_type]:\n",
    "#             if new_entity not in consolidated_data[entity_type]:\n",
    "#                 consolidated_data[entity_type][new_entity] = consolidated_data[entity_type][old_entity]\n",
    "#             else:\n",
    "#                 consolidated_data[entity_type][new_entity][\"frames_found\"].extend(consolidated_data[entity_type][old_entity][\"frames_found\"])\n",
    "#                 consolidated_data[entity_type][new_entity][\"frames_found\"] = list(set(consolidated_data[entity_type][new_entity][\"frames_found\"]))\n",
    "            \n",
    "#             merged_entities_log.append({\n",
    "#                 \"merged_to\": new_entity,\n",
    "#                 \"merged_from\": old_entity,\n",
    "#                 \"frames\": consolidated_data[entity_type][old_entity][\"frames_found\"]\n",
    "#             })\n",
    "#             del consolidated_data[entity_type][old_entity]\n",
    "\n",
    "#     return consolidated_data, merged_entities_log\n",
    "\n",
    "# # Function to print the consolidated entities\n",
    "# def print_consolidated_data(consolidated_data, title=\"Consolidated\"):\n",
    "#     print(f\"\\n{title} Characters:\")\n",
    "#     for char_name, char_data in consolidated_data[\"characters\"].items():\n",
    "#         print(f\"{char_name}: Found in frames {char_data['frames_found']}\")\n",
    "\n",
    "#     print(f\"\\n{title} Objects:\")\n",
    "#     for obj_name, obj_data in consolidated_data[\"objects\"].items():\n",
    "#         print(f\"{obj_name}: Found in frames {obj_data['frames_found']}\")\n",
    "\n",
    "#     print(f\"\\n{title} Places:\")\n",
    "#     for place_name, place_data in consolidated_data[\"places\"].items():\n",
    "#         print(f\"{place_name}: Found in frames {place_data['frames_found']}\")\n",
    "\n",
    "# # Function to initially extract and consolidate all entities into lists\n",
    "# async def initial_consolidation(json_output_dir, image_output_dir):\n",
    "#     consolidated_data = {\n",
    "#         \"characters\": {},\n",
    "#         \"objects\": {},\n",
    "#         \"places\": {}\n",
    "#     }\n",
    "\n",
    "#     json_files = [f for f in os.listdir(json_output_dir) if f.endswith('.json')]\n",
    "    \n",
    "#     for json_file in json_files:\n",
    "#         json_path = os.path.join(json_output_dir, json_file)\n",
    "#         with open(json_path, 'r') as file:\n",
    "#             json_data = json.load(file)\n",
    "\n",
    "#         frame_number = extract_frame_number(json_file)\n",
    "#         entities = extract_entities_from_json(json_data)\n",
    "\n",
    "#         for entity_type in [\"characters\", \"objects\", \"places\"]:\n",
    "#             for new_entity in entities[entity_type]:\n",
    "#                 entity_name = new_entity[\"Name\"]\n",
    "                \n",
    "#                 # Generate the image path\n",
    "#                 image_path = get_image_path(json_file, image_output_dir)\n",
    "#                 new_entity[\"Image Path\"] = image_path\n",
    "                \n",
    "#                 if entity_name in consolidated_data[entity_type]:\n",
    "#                     consolidated_data[entity_type][entity_name][\"frames_found\"].append(frame_number)\n",
    "#                 else:\n",
    "#                     consolidated_data[entity_type][entity_name] = {\n",
    "#                         \"entity\": new_entity,\n",
    "#                         \"frames_found\": [frame_number]\n",
    "#                     }\n",
    "\n",
    "#     return consolidated_data\n",
    "\n",
    "# # Function to process and consolidate all entities\n",
    "# async def consolidate_all_entities(consolidated_data, api_key):\n",
    "#     merged_entities_log = []\n",
    "\n",
    "#     # Consolidate characters\n",
    "#     characters = list(consolidated_data[\"characters\"].keys())\n",
    "#     new_characters = await consolidate_entities_with_openai(\"characters\", characters, api_key)\n",
    "#     consolidated_data, merged_entities_log = apply_consolidation(consolidated_data, \"characters\", new_characters, merged_entities_log)\n",
    "\n",
    "#     # Consolidate objects\n",
    "#     objects = list(consolidated_data[\"objects\"].keys())\n",
    "#     new_objects = await consolidate_entities_with_openai(\"objects\", objects, api_key)\n",
    "#     consolidated_data, merged_entities_log = apply_consolidation(consolidated_data, \"objects\", new_objects, merged_entities_log)\n",
    "\n",
    "#     # Consolidate places\n",
    "#     places = list(consolidated_data[\"places\"].keys())\n",
    "#     new_places = await consolidate_entities_with_openai(\"places\", places, api_key)\n",
    "#     consolidated_data, merged_entities_log = apply_consolidation(consolidated_data, \"places\", new_places, merged_entities_log)\n",
    "\n",
    "#     return consolidated_data, merged_entities_log\n",
    "\n",
    "# # Function to handle the image-to-image comparisons for portions\n",
    "# async def handle_portion_comparisons(consolidated_data, api_key):\n",
    "#     merged_entities_log = []\n",
    "\n",
    "#     for entity_type in [\"characters\", \"objects\", \"places\"]:\n",
    "#         for entity_name, entity_data in consolidated_data[entity_type].items():\n",
    "#             if entity_data[\"entity\"].get(\"Portion Boolean\") == 1:\n",
    "#                 # Perform image-to-image comparisons for portions\n",
    "#                 for other_entity_name, other_entity_data in consolidated_data[entity_type].items():\n",
    "#                     if other_entity_name != entity_name and other_entity_data[\"entity\"].get(\"Portion Boolean\") != 1:\n",
    "#                         comparison_result = await perform_image_to_image_comparison(\n",
    "#                             entity_data[\"entity\"], other_entity_data[\"entity\"],\n",
    "#                             entity_data[\"entity\"][\"Image Path\"], other_entity_data[\"entity\"][\"Image Path\"],\n",
    "#                             api_key\n",
    "#                         )\n",
    "#                         if comparison_result == \"true\":\n",
    "#                             if entity_name not in other_entity_data[\"frames_found\"]:\n",
    "#                                 other_entity_data[\"frames_found\"].extend(entity_data[\"frames_found\"])\n",
    "#                                 other_entity_data[\"frames_found\"] = list(set(other_entity_data[\"frames_found\"]))\n",
    "\n",
    "#                             merged_entities_log.append({\n",
    "#                                 \"merged_to\": other_entity_name,\n",
    "#                                 \"merged_from\": entity_name,\n",
    "#                                 \"frames\": entity_data[\"frames_found\"]\n",
    "#                             })\n",
    "#                             del consolidated_data[entity_type][entity_name]\n",
    "#                             break\n",
    "\n",
    "#     return consolidated_data, merged_entities_log\n",
    "\n",
    "# # Function to perform a final image-to-image comparison for characters\n",
    "# async def final_image_to_image_comparison_for_characters(consolidated_data, api_key):\n",
    "#     character_names = list(consolidated_data[\"characters\"].keys())\n",
    "#     merged_entities_log = []\n",
    "\n",
    "#     # Progress bar for character comparisons\n",
    "#     with tqdm(total=len(character_names) * (len(character_names) - 1) // 2, desc=\"Final Character Comparisons\", unit=\"comparison\") as pbar:\n",
    "#         # Compare each character with every other character\n",
    "#         for i in range(len(character_names)):\n",
    "#             for j in range(i + 1, len(character_names)):\n",
    "#                 name1 = character_names[i]\n",
    "#                 name2 = character_names[j]\n",
    "\n",
    "#                 entity1 = consolidated_data[\"characters\"][name1][\"entity\"]\n",
    "#                 entity2 = consolidated_data[\"characters\"][name2][\"entity\"]\n",
    "\n",
    "#                 # Perform image comparison\n",
    "#                 comparison_result = await perform_image_to_image_comparison(\n",
    "#                     entity1, entity2,\n",
    "#                     entity1[\"Image Path\"], entity2[\"Image Path\"],\n",
    "#                     api_key\n",
    "#                 )\n",
    "\n",
    "#                 if comparison_result == \"true\":\n",
    "#                     # Merge name2 into name1\n",
    "#                     consolidated_data[\"characters\"][name1][\"frames_found\"].extend(consolidated_data[\"characters\"][name2][\"frames_found\"])\n",
    "#                     consolidated_data[\"characters\"][name1][\"frames_found\"] = list(set(consolidated_data[\"characters\"][name1][\"frames_found\"]))\n",
    "\n",
    "#                     # Log the merge\n",
    "#                     merged_entities_log.append({\n",
    "#                         \"merged_to\": name1,\n",
    "#                         \"merged_from\": name2,\n",
    "#                         \"frames\": consolidated_data[\"characters\"][name2][\"frames_found\"]\n",
    "#                     })\n",
    "\n",
    "#                     # Remove the merged entity\n",
    "#                     del consolidated_data[\"characters\"][name2]\n",
    "\n",
    "#                 pbar.update(1)  # Update the progress bar\n",
    "\n",
    "#     return consolidated_data, merged_entities_log\n",
    "\n",
    "# # Function to save the summary to a JSON file\n",
    "# def save_summary_to_json(consolidated_data, output_path, video_title, file_size, processing_time, merged_entities_log):\n",
    "#     summary = {\n",
    "#         \"Video Title\": video_title,\n",
    "#         \"File Size (bytes)\": file_size,\n",
    "#         \"Processing Time (seconds)\": processing_time,\n",
    "#         \"Consolidated Data\": consolidated_data,\n",
    "#         \"Merged Entities Log\": merged_entities_log\n",
    "#     }\n",
    "    \n",
    "#     with open(output_path, 'w') as f:\n",
    "#         json.dump(summary, f, indent=4)\n",
    "\n",
    "# # Extract frame number from file name\n",
    "# def extract_frame_number(filename):\n",
    "#     match = re.search(r'_frame_(\\d+)', filename)\n",
    "#     return int(match.group(1)) if match else None\n",
    "\n",
    "# # Main function to run the consolidation and processing\n",
    "# async def main():\n",
    "#     json_output_dir = \"/Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/json_output\"\n",
    "#     image_output_dir = \"/Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas/scenes_output\"\n",
    "#     summary_json_path = \"/Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas_summary.json\"\n",
    "#     api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "#     start_time = time.time()  # Start timing\n",
    "\n",
    "#     # Overall progress bar for 4 steps\n",
    "#     with tqdm(total=4, desc=\"Overall Progress\", unit=\"step\") as overall_pbar:\n",
    "#         # Step 1: Initial extraction and consolidation\n",
    "#         consolidated_data = await initial_consolidation(json_output_dir, image_output_dir)\n",
    "#         print_consolidated_data(consolidated_data, title=\"Initial\")\n",
    "#         overall_pbar.update(1)\n",
    "\n",
    "#         # Step 2: Consolidate entities using OpenAI\n",
    "#         consolidated_data, merged_entities_log = await consolidate_all_entities(consolidated_data, api_key)\n",
    "#         print_consolidated_data(consolidated_data, title=\"After OpenAI Consolidation\")\n",
    "#         overall_pbar.update(1)\n",
    "\n",
    "#         # Step 3: Handle image-to-image comparisons for portions\n",
    "#         consolidated_data, portion_merged_log = await handle_portion_comparisons(consolidated_data, api_key)\n",
    "#         merged_entities_log.extend(portion_merged_log)\n",
    "#         print_consolidated_data(consolidated_data, title=\"After Portion Comparisons\")\n",
    "#         overall_pbar.update(1)\n",
    "\n",
    "#         # Step 4: Final image-to-image comparison for characters\n",
    "#         consolidated_data, final_character_merged_log = await final_image_to_image_comparison_for_characters(consolidated_data, api_key)\n",
    "#         merged_entities_log.extend(final_character_merged_log)\n",
    "#         print_consolidated_data(consolidated_data, title=\"Final After Image Comparisons\")\n",
    "#         overall_pbar.update(1)\n",
    "\n",
    "#     end_time = time.time()  # End timing\n",
    "#     processing_time = end_time - start_time\n",
    "\n",
    "#     # Calculate file size of JSON files\n",
    "#     file_size = os.path.getsize(summary_json_path) if os.path.isfile(summary_json_path) else 0\n",
    "\n",
    "#     save_summary_to_json(consolidated_data, summary_json_path, \"Bananas_in_pyjamas.mp4\", file_size, processing_time, merged_entities_log)\n",
    "\n",
    "# # Run the main function\n",
    "# if __name__ == \"__main__\":\n",
    "#     asyncio.get_event_loop().run_until_complete(main())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
