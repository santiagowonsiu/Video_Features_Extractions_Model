{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to analyse the whole database of videos and process it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import json\n",
    "import imageio\n",
    "import re\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import colorsys\n",
    "import aiofiles\n",
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from scenedetect import VideoManager, SceneManager\n",
    "from scenedetect.detectors import ContentDetector\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import csv\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load OpenAI API key from .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Apply nest_asyncio to handle the running event loop\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Concurrency limit\n",
    "semaphore = asyncio.Semaphore(5)\n",
    "\n",
    "# A dictionary to store characters across frames\n",
    "character_frames = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRACK API USAGE CALLS\n",
    "\n",
    "# Initialize API usage tracking\n",
    "api_usage = {\n",
    "    \"total_api_calls\": 0,\n",
    "    \"total_tokens_used\": 0,\n",
    "    \"model_used\": \"gpt-4\"  # Assuming you're using GPT-4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Video Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_video(video_path, threshold=27.0):\n",
    "    if not os.path.exists(video_path):\n",
    "        raise FileNotFoundError(f\"The video file {video_path} does not exist.\")\n",
    "    \n",
    "    video_manager = VideoManager([video_path])\n",
    "    scene_manager = SceneManager()\n",
    "    scene_manager.add_detector(ContentDetector(threshold=threshold))\n",
    "\n",
    "    video_manager.set_downscale_factor()\n",
    "    video_manager.start()\n",
    "\n",
    "    scene_manager.detect_scenes(frame_source=video_manager)\n",
    "    scene_list = scene_manager.get_scene_list()\n",
    "\n",
    "    video_manager.release()\n",
    "\n",
    "    logging.info(f'Detected {len(scene_list)} scenes:')\n",
    "    for i, scene in enumerate(scene_list):\n",
    "        logging.info(f'Scene {i + 1}: Start {scene[0].get_timecode()} / Frame {scene[0].get_frames()}, '\n",
    "              f'End {scene[1].get_timecode()} / Frame {scene[1].get_frames()}')\n",
    "\n",
    "    return scene_list\n",
    "\n",
    "def get_video_length(video_path):\n",
    "    # You can use a tool like OpenCV, ffmpeg, or similar to calculate video length\n",
    "    import cv2\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_length = frame_count / fps\n",
    "    cap.release()\n",
    "    return video_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Frame Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames_imageio(video_path, scenes, output_dir):\n",
    "    reader = imageio.get_reader(video_path)\n",
    "    for i, scene in enumerate(scenes):\n",
    "        start_frame, end_frame = scene\n",
    "        \n",
    "        # Convert FrameTimecode to integer frame numbers\n",
    "        start_frame_num = int(start_frame)\n",
    "        end_frame_num = int(end_frame)\n",
    "        \n",
    "        # Calculate the middle frame of the scene\n",
    "        middle_frame = (start_frame_num + end_frame_num) // 2\n",
    "        \n",
    "        # Set the reader to the middle frame and extract it\n",
    "        reader.set_image_index(middle_frame)\n",
    "        frame = reader.get_next_data()\n",
    "        \n",
    "        # Save the frame as an image with frame number in the filename\n",
    "        output_path = os.path.join(output_dir, f'scene_{i + 1}_frame_{middle_frame}.jpg')\n",
    "        imageio.imwrite(output_path, frame)\n",
    "        print(f\"Extracted and saved middle frame of scene {i + 1} as {output_path}\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Image Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def encode_image(image_path):\n",
    "    async with aiofiles.open(image_path, \"rb\") as image_file:\n",
    "        content = await image_file.read()\n",
    "        return base64.b64encode(content).decode('utf-8')\n",
    "\n",
    "def get_color_category(color):\n",
    "    r, g, b = [x / 255.0 for x in color]\n",
    "    h, l, s = colorsys.rgb_to_hls(r, g, b)\n",
    "\n",
    "    primary_hues = {\n",
    "        \"red\": (0.0, 0.1),  \n",
    "        \"yellow\": (0.1, 0.18),\n",
    "        \"green\": (0.25, 0.4),\n",
    "        \"blue\": (0.55, 0.75),\n",
    "    }\n",
    "\n",
    "    for color_name, hue_range in primary_hues.items():\n",
    "        if hue_range[0] <= h <= hue_range[1]:\n",
    "            return color_name\n",
    "\n",
    "    if (l >= 0.9 and s <= 0.1):\n",
    "        return \"white\"\n",
    "    if (l <= 0.1 and s <= 0.1):\n",
    "        return \"black\"\n",
    "\n",
    "    return \"non-primary\"\n",
    "\n",
    "def analyze_image_colors(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert('RGB')\n",
    "    data = np.array(image)\n",
    "\n",
    "    unique_colors, counts = np.unique(data.reshape(-1, data.shape[2]), axis=0, return_counts=True)\n",
    "    total_pixels = int(counts.sum())\n",
    "\n",
    "    color_counts = {\n",
    "        \"Red\": 0,\n",
    "        \"Yellow\": 0,\n",
    "        \"Green\": 0,\n",
    "        \"Blue\": 0,\n",
    "        \"White\": 0,\n",
    "        \"Black\": 0,\n",
    "        \"Non-primary\": 0\n",
    "    }\n",
    "\n",
    "    for color, count in zip(unique_colors, counts):\n",
    "        category = get_color_category(tuple(color))\n",
    "        color_counts[category.capitalize()] += int(count)\n",
    "\n",
    "    color_percentages = {color: (count / total_pixels) * 100 for color, count in color_counts.items()}\n",
    "    primary_total = color_counts[\"Red\"] + color_counts[\"Yellow\"] + color_counts[\"Blue\"]\n",
    "    color_dominance = \"Primary colors\" if primary_total > color_counts[\"Non-primary\"] else \"Non-primary colors\"\n",
    "\n",
    "    return {\n",
    "        \"Color Analysis\": {\n",
    "            \"Colors Found\": {\n",
    "                color: {\n",
    "                    \"Pixel Count\": count,\n",
    "                    \"Percentage\": f\"{color_percentages[color]:.2f}%\"\n",
    "                } for color, count in color_counts.items()\n",
    "            },\n",
    "            \"Dominance\": color_dominance\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) OpenAI API Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def send_image_to_openai(image_path, base64_image, retries=3):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"\"\"\n",
    "                        Analyze the following image and provide a detailed description in the format of JSON only. Ensure the output is strictly in JSON format without any additional text or code block formatting. The JSON should include the following standardized labels:\n",
    "\n",
    "                        1. **Image Analysis**: The root dictionary containing all analysis data.\n",
    "                        \n",
    "                        2. **Suitability**:\n",
    "                            - \"Nudity\": Boolean indicating the presence of nudity.\n",
    "                            - \"Obscene Gestures\": Boolean indicating the presence of obscene gestures.\n",
    "                            - \"Alcohol\": Boolean indicating the presence of alcohol.\n",
    "                            - \"Drugs\": Boolean indicating the presence of drugs.\n",
    "                            - \"Addictions\": Boolean indicating the presence of addictions.\n",
    "\n",
    "                        3. **Objects**:\n",
    "                            - \"Total Objects Identified\": Integer representing the total number of objects identified.\n",
    "                            - \"Average Features Per Object\": Float representing the average number of features per object.\n",
    "                            - \"Objects Details\": Dictionary containing details of each object, where each object is labeled as \"Object_1\", \"Object_2\", etc., with the following structure:\n",
    "                                - \"Name\": The name of the object - as simplest and descriptive possible.\n",
    "                                - \"Portion Boolean\": 0-1 output indicating if the object is a portion of a larger object (1) or a complete object (0). For example, a leg is a portion of a human. However, if the object is just cropped but clearly identifiable as a complete object, it should be considered a complete object.\n",
    "                                - \"Color\": The color of the object.\n",
    "                                - \"Features\": List of features of the object.\n",
    "                                - \"Total Features\": Integer representing the number of features for the object.\n",
    "\n",
    "                        4. **Place**:\n",
    "                            - \"Name\": The name of the place - as simplest and descriptive as possible.\n",
    "                            - \"Certainty Boolean\": 0-1 output indicating if the place is clearly identifiable (1) or not (0).\n",
    "                            - \"Fantasy/Adventurous Place\": Boolean (0-1) indicating whether the place is classified as a fantasy/adventurous place or not.\n",
    "                            - \"Explanation\": Detailed explanation of why the place is classified as fantasy/adventurous or not. Fantasy places are those that do not exist in reality, and adventurous places are defined as those involving clear statements of traveling to space or another country.\n",
    "\n",
    "                        5. **Characters**:\n",
    "                            - \"Total Characters Identified\": Integer representing the total number of characters identified.\n",
    "                            - \"Average Features Per Character\": Float representing the average number of features per character.\n",
    "                            - \"Character Details\": Dictionary containing details of each character, where each character is labeled as \"Character_1\", \"Character_2\", etc., with the following structure:\n",
    "                                - \"Name\": The name of the character - as simplest and descriptive as possible.\n",
    "                                - \"Portion Boolean\": 0-1 output indicating if the character is a portion of a larger character (1) or a complete character (0). For example, a leg is a portion of a human. However, if the character is just cropped but clearly identifiable as a complete character, it should be considered a complete character.\n",
    "                                - \"Human or Non-Human\": 0-1 output indicating if the character appears human (1) or non-human (0). Anthropomorphized characters or any other combination not fully human are considered non-human.\n",
    "                                - \"Physical Features\": List of physical features of the character.\n",
    "                                - \"Explanation\": Explanation for why the character is classified as human or non-human, and why these physical features are inferred.\n",
    "                                - \"Age\": Expected age range of the character (a single number).\n",
    "                            **Note**: If the \"character\" consists of only a part of a body (such as a hand, leg, or face without enough distinguishing features to identify it as a complete character), do not count it as a \"character.\"\n",
    "\n",
    "                        Ensure that the structure of the JSON output strictly adheres to these standardized labels.\n",
    "                        \"\"\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 750\n",
    "    }\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload) as response:\n",
    "                    status = response.status\n",
    "                    response_text = await response.text()\n",
    "\n",
    "                    if status == 429:\n",
    "                        print(\"Rate limit exceeded, retrying...\")\n",
    "                        await asyncio.sleep(2 ** attempt)\n",
    "                        continue\n",
    "                    elif status == 200:\n",
    "                        content = await response.json()\n",
    "\n",
    "                        # Track API usage\n",
    "                        api_usage['total_api_calls'] += 1\n",
    "                        api_usage['total_tokens_used'] += content.get('usage', {}).get('total_tokens', 0)\n",
    "                        api_usage['model_used'] = content.get('model', 'gpt-4o-mini')\n",
    "\n",
    "                        # Handle and return JSON content\n",
    "                        if 'choices' in content:\n",
    "                            message_content = content['choices'][0].get('message', {}).get('content', '').strip()\n",
    "                            try:\n",
    "                                return json.loads(message_content)\n",
    "                            except json.JSONDecodeError as e:\n",
    "                                print(f\"Error decoding JSON from OpenAI response for {image_path}: {e}\")\n",
    "                                return None\n",
    "                        else:\n",
    "                            print(f\"Unexpected response format from OpenAI API for {image_path}.\")\n",
    "                            return None\n",
    "                    else:\n",
    "                        print(f\"Request failed with status code {status} for {image_path}.\")\n",
    "                        return None\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Request failed due to a client error: {e}\")\n",
    "            await asyncio.sleep(2 ** attempt)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error occurred: {e}\")\n",
    "            await asyncio.sleep(2 ** attempt)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Scene Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_scenes_output(output_dir, json_output_dir):\n",
    "    os.makedirs(json_output_dir, exist_ok=True)\n",
    "    scenes = sorted([f for f in os.listdir(output_dir) if f.endswith('.jpg')], key=extract_scene_number)\n",
    "    total_scenes = len(scenes)\n",
    "    with tqdm(total=total_scenes, desc=\"Processing Scenes\", unit=\"scene\") as pbar:\n",
    "        tasks = [process_single_scene(i, scene, output_dir, json_output_dir, pbar) for i, scene in enumerate(scenes)]\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "async def process_single_scene(i, scene, output_dir, json_output_dir, pbar):\n",
    "    async with semaphore:  # Limit concurrent execution\n",
    "        scene_path = os.path.join(output_dir, scene)\n",
    "\n",
    "        # Encode image in base64\n",
    "        base64_image = await encode_image(scene_path)\n",
    "\n",
    "        # Perform color analysis\n",
    "        color_analysis_result = analyze_image_colors(scene_path)\n",
    "\n",
    "        # Send image to OpenAI for further analysis\n",
    "        openai_response = await send_image_to_openai(scene_path, base64_image)\n",
    "\n",
    "        # Check if openai_response is valid (not None or empty)\n",
    "        if not openai_response:\n",
    "            print(f\"Skipping {scene} due to invalid OpenAI response.\")\n",
    "            pbar.update(1)\n",
    "            return\n",
    "\n",
    "        # Combine both results, and include the reference to the image file\n",
    "        final_output = {\n",
    "            \"Image File\": scene,\n",
    "            \"Image Analysis\": {\n",
    "                **color_analysis_result[\"Color Analysis\"],\n",
    "                **openai_response.get(\"Image Analysis\", {})\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # The filename already includes the scene number and frame number\n",
    "        output_filename = os.path.splitext(scene)[0] + '_analysis.json'\n",
    "        output_path = os.path.join(json_output_dir, output_filename)\n",
    "\n",
    "        try:\n",
    "            async with aiofiles.open(output_path, 'w') as json_file:\n",
    "                await json_file.write(json.dumps(final_output, indent=4))\n",
    "                print(f\"Saved analysis for {scene} as {output_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save analysis for {scene}: {e}\")\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "def extract_scene_number(filename):\n",
    "    match = re.search(r'\\d+', filename)\n",
    "    return int(match.group()) if match else -1\n",
    "\n",
    "def extract_frame_number(filename):\n",
    "    match = re.search(r'_frame_(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Run whole analysis of each json output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Path Construction: get_image_path generates the correct path to the image file based on the JSON filename.\n",
    "\n",
    "Entity Extraction:extract_entities_from_json pulls characters, objects, and places from the JSON data.\n",
    "\n",
    "Image-to-Image Comparison:perform_image_to_image_comparison compares partial objects with full objects using the OpenAI API.\n",
    "\n",
    "Entity Comparison:compare_entities handles both name-based and image-based comparisons to decide whether two entities should be consolidated.\n",
    "\n",
    "Consolidation:Entities across frames are consolidated into a single summary file that tracks where each entity was found.\n",
    "\n",
    "Main Execution:The script runs through all JSON files, processes the entities, and saves the consolidated results to a summary JSON file.\n",
    "\n",
    "Key Features of This Implementation:\n",
    "Text-Based Comparison: The code first attempts to merge entities based on exact name matches. If no match is found, it uses the OpenAI API to determine if two entities with different names should be merged.\n",
    "\n",
    "Image-to-Image Comparison: If one of the entities is flagged as a portion, or if names don't match but the entities might still be the same, the code performs an image-to-image comparison using the OpenAI API.\n",
    "\n",
    "Efficient Processing: The code processes each frame sequentially and logs all merges into merged_entities_log, ensuring you have a record of what entities were merged, including their original names and frames.\n",
    "\n",
    "No Overwritten Functionality: The original image analysis functionality is preserved and integrated smoothly with the text-based comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Unique Characters, Objects, and Places: This can be done by counting the keys in the consolidated_data dictionary.\n",
    "Average Characters per Frame: This can be calculated by summing up all instances of characters found across frames and dividing by the total number of frames where characters appear.\n",
    "Average Features per Character/Object: Calculate this by summing the features of all characters/objects and dividing by the total number of characters/objects.\n",
    "Overall Color Analysis: Aggregate the color data from all JSON files.\n",
    "Filter Compliance: Check for any instances where the filters (e.g., nudity, drugs) are not compliant and log the frame numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import openai\n",
    "\n",
    "# Apply nest_asyncio to handle the running event loop\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize OpenAI API\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OpenAI API key is not set.\")\n",
    "openai.api_key = api_key\n",
    "\n",
    "#Initial consolidation of entities\n",
    "def initial_consolidation(json_files):\n",
    "    consolidated_data = {\"characters\": {}, \"objects\": {}, \"places\": {}}\n",
    "    merge_tracking = {\"characters\": {}, \"objects\": {}, \"places\": {}}\n",
    "\n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            consolidate_from_json(json_data, consolidated_data, merge_tracking, os.path.basename(json_file))\n",
    "\n",
    "    return consolidated_data\n",
    "\n",
    "\n",
    "# Function to save entities to JSON\n",
    "def save_entities_to_json(entities, path):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(entities, f, indent=4)\n",
    "\n",
    "# Function to consolidate entities from a JSON file\n",
    "# Function to consolidate entities from a JSON file\n",
    "def consolidate_from_json(json_data, consolidated_data, merge_tracking, json_file_name):\n",
    "    frame_number = json_file_name.split('_')[3]  # Extract frame number\n",
    "\n",
    "    if \"Image Analysis\" in json_data:\n",
    "        characters = json_data[\"Image Analysis\"].get(\"Characters\", {}).get(\"Character Details\", {})\n",
    "        for key, details in characters.items():\n",
    "            name = details.get(\"Name\")\n",
    "            if name:\n",
    "                if name not in consolidated_data[\"characters\"]:\n",
    "                    consolidated_data[\"characters\"][name] = details\n",
    "                    consolidated_data[\"characters\"][name][\"merged_from\"] = []\n",
    "                    merge_tracking[\"characters\"][name] = {\"merged_from\": []}\n",
    "                if frame_number not in consolidated_data[\"characters\"][name][\"merged_from\"]:\n",
    "                    consolidated_data[\"characters\"][name][\"merged_from\"].append(frame_number)\n",
    "                    merge_tracking[\"characters\"][name][\"merged_from\"].append(frame_number)\n",
    "\n",
    "        objects = json_data[\"Image Analysis\"].get(\"Objects\", {}).get(\"Objects Details\", {})\n",
    "        for key, details in objects.items():\n",
    "            name = details.get(\"Name\")\n",
    "            if name:\n",
    "                if name not in consolidated_data[\"objects\"]:\n",
    "                    consolidated_data[\"objects\"][name] = details\n",
    "                    consolidated_data[\"objects\"][name][\"merged_from\"] = []\n",
    "                    merge_tracking[\"objects\"][name] = {\"merged_from\": []}\n",
    "                if frame_number not in consolidated_data[\"objects\"][name][\"merged_from\"]:\n",
    "                    consolidated_data[\"objects\"][name][\"merged_from\"].append(frame_number)\n",
    "                    merge_tracking[\"objects\"][name][\"merged_from\"].append(frame_number)\n",
    "\n",
    "        place = json_data[\"Image Analysis\"].get(\"Place\", {})\n",
    "        place_name = place.get(\"Name\")\n",
    "        if place_name:\n",
    "            if place_name not in consolidated_data[\"places\"]:\n",
    "                consolidated_data[\"places\"][place_name] = place\n",
    "                consolidated_data[\"places\"][place_name][\"merged_from\"] = []\n",
    "                merge_tracking[\"places\"][place_name] = {\"merged_from\": []}\n",
    "            if frame_number not in consolidated_data[\"places\"][place_name][\"merged_from\"]:\n",
    "                consolidated_data[\"places\"][place_name][\"merged_from\"].append(frame_number)\n",
    "                merge_tracking[\"places\"][place_name][\"merged_from\"].append(frame_number)\n",
    "\n",
    "\n",
    "# Function to cluster entities using OpenAI API and name the clusters\n",
    "# Initialize API usage tracking\n",
    "api_usage = {\n",
    "    \"total_api_calls\": 0,\n",
    "    \"total_tokens_used\": 0,\n",
    "    \"model_used\": \"gpt-4\",  # Set the default model name here\n",
    "}\n",
    "\n",
    "# Function to cluster entities using OpenAI API and track token usage\n",
    "async def cluster_entities(api_key, entities):\n",
    "    # Generate lists for characters, objects, and places from entities\n",
    "    character_list = ', '.join(entities['characters'].keys())\n",
    "    object_list = ', '.join(entities['objects'].keys())\n",
    "    place_list = ', '.join(entities['places'].keys())\n",
    "\n",
    "    if not character_list and not object_list and not place_list:\n",
    "        return \"No entities available to cluster.\"\n",
    "\n",
    "    # Adjusted OpenAI prompt to return structured output (dictionaries)\n",
    "    prompt = f\"\"\"\n",
    "    You are tasked with clustering and naming entities from a TV show. Below are lists of characters, objects, and places extracted from different scenes. These lists sometimes contain multiple labels for the same entity.\n",
    "\n",
    "    **Instructions:**\n",
    "\n",
    "    1. Group the characters, objects, and places that refer to the same entity and suggest a single **final name** for each group (be smart what could be the same individual/object in the show one and what couldn't).\n",
    "    2. Return the result as a dictionary where each cluster (key) contains the entities (values) that belong to that cluster.\n",
    "    3. Use this format:\n",
    "\n",
    "    {{\n",
    "      \"Characters Clusters\": {{\n",
    "        \"Final Name 1\": [\"Character 1\", \"Character 2\", ...],\n",
    "        \"Final Name 2\": [\"Character 3\", \"Character 4\", ...]\n",
    "      }},\n",
    "      \"Objects Clusters\": {{\n",
    "        \"Final Name 1\": [\"Object 1\", \"Object 2\", ...],\n",
    "        \"Final Name 2\": [\"Object 3\", \"Object 4\", ...]\n",
    "      }},\n",
    "      \"Places Clusters\": {{\n",
    "        \"Final Name 1\": [\"Place 1\", \"Place 2\", ...],\n",
    "        \"Final Name 2\": [\"Place 3\", \"Place 4\", ...]\n",
    "      }}\n",
    "    }}\n",
    "\n",
    "    **Characters:**\n",
    "    {character_list}\n",
    "\n",
    "    **Objects:**\n",
    "    {object_list}\n",
    "\n",
    "    **Places:**\n",
    "    {place_list}\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4\",  # Ensure you set the correct model here\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": 2000\n",
    "    }\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload) as response:\n",
    "                response_json = await response.json()\n",
    "                clusters = response_json.get('choices', [{}])[0].get('message', {}).get('content', '')\n",
    "\n",
    "                # Track API usage\n",
    "                api_usage['total_api_calls'] += 1\n",
    "                api_usage['total_tokens_used'] += response_json.get('usage', {}).get('total_tokens', 0)\n",
    "                api_usage['model_used'] = response_json.get('model', 'gpt-4')\n",
    "\n",
    "                return json.loads(clusters)  # Convert the response to JSON\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing API response: {e}\")\n",
    "            return {}\n",
    "\n",
    "\n",
    "# Function to merge clusters with final_final_entities.json, keeping names and frame numbers\n",
    "def merge_clusters_with_entities(final_entities, clusters):\n",
    "    merged_entities = {\"characters\": {}, \"objects\": {}, \"places\": {}}\n",
    "\n",
    "    # Process characters\n",
    "    character_clusters = clusters.get(\"Characters Clusters\", {})\n",
    "    for final_name, cluster_items in character_clusters.items():\n",
    "        merged_entities[\"characters\"][final_name] = {\"merged_from\": [], \"merged_names\": []}\n",
    "        for item in cluster_items:\n",
    "            if item in final_entities[\"characters\"]:\n",
    "                entity_data = final_entities[\"characters\"][item]\n",
    "                merged_entities[\"characters\"][final_name] = {\n",
    "                    **entity_data,\n",
    "                    \"Name\": final_name,\n",
    "                    \"merged_from\": list(set(merged_entities[\"characters\"][final_name][\"merged_from\"] + entity_data[\"merged_from\"])),\n",
    "                    \"merged_names\": list(set(merged_entities[\"characters\"][final_name][\"merged_names\"] + [item]))\n",
    "                }\n",
    "\n",
    "    # Process objects\n",
    "    object_clusters = clusters.get(\"Objects Clusters\", {})\n",
    "    for final_name, cluster_items in object_clusters.items():\n",
    "        merged_entities[\"objects\"][final_name] = {\"merged_from\": [], \"merged_names\": []}\n",
    "        for item in cluster_items:\n",
    "            if item in final_entities[\"objects\"]:\n",
    "                entity_data = final_entities[\"objects\"][item]\n",
    "                merged_entities[\"objects\"][final_name] = {\n",
    "                    **entity_data,\n",
    "                    \"Name\": final_name,\n",
    "                    \"merged_from\": list(set(merged_entities[\"objects\"][final_name][\"merged_from\"] + entity_data[\"merged_from\"])),\n",
    "                    \"merged_names\": list(set(merged_entities[\"objects\"][final_name][\"merged_names\"] + [item]))\n",
    "                }\n",
    "\n",
    "    # Process places\n",
    "    place_clusters = clusters.get(\"Places Clusters\", {})\n",
    "    for final_name, cluster_items in place_clusters.items():\n",
    "        merged_entities[\"places\"][final_name] = {\"merged_from\": [], \"merged_names\": []}\n",
    "        for item in cluster_items:\n",
    "            if item in final_entities[\"places\"]:\n",
    "                entity_data = final_entities[\"places\"][item]\n",
    "                merged_entities[\"places\"][final_name] = {\n",
    "                    **entity_data,\n",
    "                    \"Name\": final_name,\n",
    "                    \"merged_from\": list(set(merged_entities[\"places\"][final_name][\"merged_from\"] + entity_data[\"merged_from\"])),\n",
    "                    \"merged_names\": list(set(merged_entities[\"places\"][final_name][\"merged_names\"] + [item]))\n",
    "                }\n",
    "\n",
    "    return merged_entities\n",
    "\n",
    "# Main async function for clustering and merging entities\n",
    "# Main async function for clustering and merging entities\n",
    "async def cluster_and_merge_entities(api_key, json_output_dir, video_folder_path):\n",
    "    json_files = [os.path.join(json_output_dir, f) for f in os.listdir(json_output_dir) if f.endswith('.json')]\n",
    "\n",
    "    if not json_files:\n",
    "        print(\"No JSON files found for consolidation.\")\n",
    "        return None\n",
    "\n",
    "    # Consolidate entities from all JSON files\n",
    "    final_entities = initial_consolidation(json_files)  # Ensure only json_files is passed here\n",
    "\n",
    "    # Cluster entities using OpenAI API\n",
    "    clusters = await cluster_entities(api_key, final_entities)\n",
    "\n",
    "    if not clusters:\n",
    "        print(\"No clusters returned from OpenAI API.\")\n",
    "        return None\n",
    "\n",
    "    # Merge clusters into final final JSON\n",
    "    merged_final_entities = merge_clusters_with_entities(final_entities, clusters)\n",
    "\n",
    "    # Save the merged entities to JSON\n",
    "    merged_final_entities_path = os.path.join(video_folder_path, 'final_summary.json')\n",
    "    save_entities_to_json(merged_final_entities, merged_final_entities_path)\n",
    "    print(f\"Merged entities saved to {merged_final_entities_path}\")\n",
    "    \n",
    "    return merged_final_entities  # Return merged entities for further use\n",
    "\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Define paths\n",
    "#     final_final_json_path = \"path/to/final_final_entities.json\"\n",
    "#     video_folder_path = \"path/to/video_folder\"\n",
    "    \n",
    "#     # Run the async function\n",
    "#     asyncio.run(cluster_and_merge_entities(api_key, final_final_json_path, video_folder_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summary_statistics(entities):\n",
    "    statistics = {\n",
    "        \"number_of_characters\": len(entities.get(\"characters\", {})),\n",
    "        \"number_of_objects\": len(entities.get(\"objects\", {})),\n",
    "        \"number_of_places\": len(entities.get(\"places\", {})),\n",
    "        \"avg_features_per_character\": 0,\n",
    "        \"avg_features_per_object\": 0,\n",
    "        \"avg_appearances_per_character\": 0,\n",
    "        \"avg_appearances_per_object\": 0,\n",
    "    }\n",
    "\n",
    "    # Calculate average features per character\n",
    "    total_character_features = 0\n",
    "    total_character_appearances = 0\n",
    "    for char, details in entities.get(\"characters\", {}).items():\n",
    "        total_character_features += len(details.get(\"Physical Features\", []))\n",
    "        total_character_appearances += len(details.get(\"merged_from\", []))\n",
    "    \n",
    "    if statistics[\"number_of_characters\"] > 0:\n",
    "        statistics[\"avg_features_per_character\"] = total_character_features / statistics[\"number_of_characters\"]\n",
    "        statistics[\"avg_appearances_per_character\"] = total_character_appearances / statistics[\"number_of_characters\"]\n",
    "\n",
    "    # Calculate average features per object\n",
    "    total_object_features = 0\n",
    "    total_object_appearances = 0\n",
    "    for obj, details in entities.get(\"objects\", {}).items():\n",
    "        total_object_features += details.get(\"Total Features\", 0)\n",
    "        total_object_appearances += len(details.get(\"merged_from\", []))\n",
    "\n",
    "    if statistics[\"number_of_objects\"] > 0:\n",
    "        statistics[\"avg_features_per_object\"] = total_object_features / statistics[\"number_of_objects\"]\n",
    "        statistics[\"avg_appearances_per_object\"] = total_object_appearances / statistics[\"number_of_objects\"]\n",
    "\n",
    "    return statistics\n",
    "\n",
    "def modify_summary_json_with_statistics(entities, path):\n",
    "    # Compute statistics\n",
    "    statistics = compute_summary_statistics(entities)\n",
    "    \n",
    "    # Load the original JSON data\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Create a new dictionary with statistics at the top\n",
    "    modified_data = {\n",
    "        \"summary_statistics\": statistics,\n",
    "        **data  # This merges the existing data under the statistics\n",
    "    }\n",
    "\n",
    "    # Save the modified data back to the JSON\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(modified_data, f, indent=4)\n",
    "\n",
    "    print(f\"Summary statistics added and saved to {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_additional_stats(video_path, scenes_output_dir, start_time, end_time, video_output_dir, json_output_dir):\n",
    "    # Initialize additional information\n",
    "    additional_info = {\n",
    "        \"video_title\": None,\n",
    "        \"video_size_bytes\": None,\n",
    "        \"video_length_seconds\": None,\n",
    "        \"number_of_scenes\": None,\n",
    "        \"processing_time_seconds\": None\n",
    "    }\n",
    "\n",
    "    # Extract video title\n",
    "    try:\n",
    "        additional_info[\"video_title\"] = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        print(f\"Video title: {additional_info['video_title']}\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting video title: {e}\", flush=True)\n",
    "\n",
    "    # Extract video size\n",
    "    try:\n",
    "        additional_info[\"video_size_bytes\"] = os.path.getsize(video_path)\n",
    "        print(f\"Video size: {additional_info['video_size_bytes']} bytes\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting video size: {e}\", flush=True)\n",
    "\n",
    "    # Get video length\n",
    "    try:\n",
    "        additional_info[\"video_length_seconds\"] = get_video_length(video_path)\n",
    "        print(f\"Video length: {additional_info['video_length_seconds']} seconds\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating video length: {e}\", flush=True)\n",
    "\n",
    "    # Count the number of scenes\n",
    "    try:\n",
    "        additional_info[\"number_of_scenes\"] = len([f for f in os.listdir(scenes_output_dir) if f.endswith('.jpg')])\n",
    "        print(f\"Number of scenes: {additional_info['number_of_scenes']}\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error counting scenes: {e}\", flush=True)\n",
    "\n",
    "    # Calculate processing time\n",
    "    try:\n",
    "        additional_info[\"processing_time_seconds\"] = end_time - start_time\n",
    "        print(f\"Processing time: {additional_info['processing_time_seconds']} seconds\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating processing time: {e}\", flush=True)\n",
    "\n",
    "    # Analyze all JSON files in the json_output folder\n",
    "    try:\n",
    "        json_stats = analyze_json_files(json_output_dir)\n",
    "        additional_info.update(json_stats)\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing JSON files: {e}\", flush=True)\n",
    "\n",
    "    # Include API usage stats\n",
    "    additional_info[\"api_usage\"] = {\n",
    "        \"total_api_calls\": api_usage[\"total_api_calls\"],\n",
    "        \"total_tokens_used\": api_usage[\"total_tokens_used\"],\n",
    "        \"model_used\": api_usage[\"model_used\"]\n",
    "    }\n",
    "\n",
    "    # Save the additional stats in a new JSON file inside the video folder\n",
    "    stats_output_path = os.path.join(video_output_dir, f'{additional_info[\"video_title\"]}_stats.json')\n",
    "\n",
    "    try:\n",
    "        with open(stats_output_path, 'w') as f:\n",
    "            json.dump(additional_info, f, indent=4)\n",
    "        print(f\"Additional stats saved to {stats_output_path}\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving stats to JSON: {e}\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_json_files(json_output_dir):\n",
    "    color_distribution = {\n",
    "        \"Red\": 0,\n",
    "        \"Yellow\": 0,\n",
    "        \"Green\": 0,\n",
    "        \"Blue\": 0,\n",
    "        \"White\": 0,\n",
    "        \"Black\": 0,\n",
    "        \"Non-primary\": 0\n",
    "    }\n",
    "    total_frames = 0\n",
    "    non_compliant_frames = []\n",
    "    total_fantasy_places = 0\n",
    "    total_places = 0\n",
    "\n",
    "    # Iterate over all JSON files in the json_output folder\n",
    "    for json_file in os.listdir(json_output_dir):\n",
    "        if not json_file.endswith('.json'):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(os.path.join(json_output_dir, json_file), 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # Get color data\n",
    "            image_analysis = data.get(\"Image Analysis\", {})\n",
    "            colors_found = image_analysis.get(\"Colors Found\", {})\n",
    "\n",
    "            # Ensure \"Colors Found\" contains the expected structure\n",
    "            for color in [\"Red\", \"Yellow\", \"Green\", \"Blue\", \"White\", \"Black\", \"Non-primary\"]:\n",
    "                color_info = colors_found.get(color, {})\n",
    "                percentage_str = color_info.get(\"Percentage\", \"0%\").replace('%', '')\n",
    "                try:\n",
    "                    color_percentage = float(percentage_str)\n",
    "                    color_distribution[color] += color_percentage\n",
    "                except (ValueError, TypeError):\n",
    "                    print(f\"Invalid color percentage in file {json_file} for color {color}\")\n",
    "                    continue\n",
    "\n",
    "            total_frames += 1\n",
    "\n",
    "            # Check for compliance issues (Suitability)\n",
    "            suitability = image_analysis.get(\"Suitability\", {})\n",
    "            if suitability:  # Make sure suitability exists and is not empty\n",
    "                non_compliant_features = [feature for feature, value in suitability.items() if value]\n",
    "                if non_compliant_features:\n",
    "                    non_compliant_frames.append({\n",
    "                        \"frame\": json_file,\n",
    "                        \"features\": non_compliant_features\n",
    "                    })\n",
    "\n",
    "            # Check for fantasy/adventurous places\n",
    "            place = image_analysis.get(\"Place\", {})\n",
    "            if place.get(\"Certainty Boolean\") == 1:\n",
    "                total_places += 1\n",
    "                if place.get(\"Fantasy/Adventurous Place\") == 1:\n",
    "                    total_fantasy_places += 1\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON in file {json_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {json_file}: {e}\")\n",
    "\n",
    "    # Calculate final stats\n",
    "    overall_color_distribution = {color: (value / total_frames) if total_frames > 0 else 0\n",
    "                                  for color, value in color_distribution.items()}\n",
    "    percentage_fantasy_places = (total_fantasy_places / total_places * 100) if total_places > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"Overall Color Distribution\": overall_color_distribution,\n",
    "        \"Non-Compliant Frames\": non_compliant_frames,\n",
    "        \"Percentage of Fantasy/Adventurous Places\": percentage_fantasy_places\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Main Function Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Videos:   0%|          | 0/1 [00:00<?, ?video/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: Bananas_in_pyjamas copy.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:pyscenedetect:VideoManager is deprecated and will be removed.\n",
      "INFO:pyscenedetect:Loaded 1 video, framerate: 24.969 FPS, resolution: 640 x 360\n",
      "INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 320 x 180\n",
      "INFO:pyscenedetect:Detecting scenes...\n",
      "INFO:root:Detected 3 scenes:\n",
      "INFO:root:Scene 1: Start 00:00:00.000 / Frame 0, End 00:00:08.370 / Frame 209\n",
      "INFO:root:Scene 2: Start 00:00:08.370 / Frame 209, End 00:00:19.344 / Frame 483\n",
      "INFO:root:Scene 3: Start 00:00:19.344 / Frame 483, End 00:00:22.147 / Frame 553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted and saved middle frame of scene 1 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/scenes_output/scene_1_frame_104.jpg\n",
      "Extracted and saved middle frame of scene 2 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/scenes_output/scene_2_frame_346.jpg\n",
      "Extracted and saved middle frame of scene 3 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/scenes_output/scene_3_frame_518.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_1_frame_104.jpg as scene_1_frame_104_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_3_frame_518.jpg as scene_3_frame_518_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Scenes: 100%|██████████| 3/3 [00:11<00:00,  3.79s/scene]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_2_frame_346.jpg as scene_2_frame_346_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged entities saved to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/final_summary.json\n",
      "Summary statistics added and saved to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/final_summary.json\n",
      "Video title: Bananas_in_pyjamas copy\n",
      "Video size: 784472 bytes\n",
      "Video length: 22.147466666666666 seconds\n",
      "Number of scenes: 3\n",
      "Processing time: 19.105642080307007 seconds\n",
      "Additional stats saved to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/Bananas_in_pyjamas copy_stats.json\n",
      "Finished processing video: Bananas_in_pyjamas copy.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Videos: 100%|██████████| 1/1 [00:19<00:00, 19.13s/video]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED PROCESSING ALL VIDEOS.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "\n",
    "async def process_video(video_file, directory_path, output_base_dir, api_key):\n",
    "    try:\n",
    "        # Track the start time\n",
    "        start_time = time.time()\n",
    "\n",
    "        video_path = os.path.join(directory_path, video_file)\n",
    "        video_name = os.path.splitext(video_file)[0]\n",
    "        video_size = os.path.getsize(video_path)\n",
    "\n",
    "        video_output_dir = os.path.join(output_base_dir, video_name)\n",
    "        scenes_output_dir = os.path.join(video_output_dir, 'scenes_output')\n",
    "        json_output_dir = os.path.join(video_output_dir, 'json_output')\n",
    "\n",
    "        os.makedirs(scenes_output_dir, exist_ok=True)\n",
    "        os.makedirs(json_output_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"Processing video: {video_file}\", flush=True)\n",
    "\n",
    "        # Analyze video for scenes\n",
    "        scenes = analyze_video(video_path)\n",
    "        extract_frames_imageio(video_path, scenes, scenes_output_dir)\n",
    "        await process_scenes_output(scenes_output_dir, json_output_dir)  # Run async scene processing\n",
    "\n",
    "        final_json_path = os.path.join(json_output_dir, 'final_entities.json')\n",
    "        \n",
    "        # Await cluster_and_merge_entities and assign returned value to merged_final_entities\n",
    "        merged_final_entities = await cluster_and_merge_entities(api_key, json_output_dir, video_output_dir)\n",
    "\n",
    "        if merged_final_entities is None:\n",
    "            print(f\"Error: Failed to merge entities for video {video_file}\")\n",
    "            return\n",
    "\n",
    "        # Save the final summary as it was before\n",
    "        merged_final_entities_path = os.path.join(video_output_dir, 'final_summary.json')\n",
    "        modify_summary_json_with_statistics(merged_final_entities, merged_final_entities_path)\n",
    "\n",
    "        # Track the end time\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Save additional stats in a new JSON file inside the video folder\n",
    "        save_additional_stats(video_path, scenes_output_dir, start_time, end_time, video_output_dir, json_output_dir)\n",
    "\n",
    "        print(f\"Finished processing video: {video_file}\", flush=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video {video_file}: {e}\", flush=True)\n",
    "\n",
    "\n",
    "async def process_videos_in_directory(directory_path, output_base_dir, api_key):\n",
    "    video_files = [f for f in os.listdir(directory_path) if f.endswith(('.mp4', '.avi', '.mkv'))]\n",
    "\n",
    "    if not video_files:\n",
    "        print(\"No video files found in the directory.\", flush=True)\n",
    "        return\n",
    "\n",
    "    with tqdm(total=len(video_files), desc=\"Processing Videos\", unit=\"video\") as pbar:\n",
    "        for video_file in video_files:\n",
    "            await process_video(video_file, directory_path, output_base_dir, api_key)\n",
    "            pbar.update(1)\n",
    "\n",
    "# Ensure the main script has appropriate paths\n",
    "video_directory = '/Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/02_Video_DB'\n",
    "output_base_directory = '/Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB'\n",
    "\n",
    "# Run the async function (sequential processing)\n",
    "asyncio.run(process_videos_in_directory(video_directory, output_base_directory, api_key))\n",
    "print(\"FINISHED PROCESSING ALL VIDEOS.\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
