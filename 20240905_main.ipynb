{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to analyse the whole database of videos and process it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import json\n",
    "import imageio\n",
    "import re\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import colorsys\n",
    "import aiofiles\n",
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from scenedetect import VideoManager, SceneManager\n",
    "from scenedetect.detectors import ContentDetector\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import csv\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load OpenAI API key from .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Apply nest_asyncio to handle the running event loop\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Concurrency limit\n",
    "semaphore = asyncio.Semaphore(5)\n",
    "\n",
    "# A dictionary to store characters across frames\n",
    "character_frames = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRACK API USAGE CALLS\n",
    "\n",
    "# Initialize API usage tracking\n",
    "api_usage = {\n",
    "    \"total_api_calls\": 0,\n",
    "    \"total_tokens_used\": 0,\n",
    "    \"model_used\": \"gpt-4\"  # Assuming you're using GPT-4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Video Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_video(video_path, threshold=27.0):\n",
    "    if not os.path.exists(video_path):\n",
    "        raise FileNotFoundError(f\"The video file {video_path} does not exist.\")\n",
    "    \n",
    "    video_manager = VideoManager([video_path])\n",
    "    scene_manager = SceneManager()\n",
    "    scene_manager.add_detector(ContentDetector(threshold=threshold))\n",
    "\n",
    "    video_manager.set_downscale_factor()\n",
    "    video_manager.start()\n",
    "\n",
    "    scene_manager.detect_scenes(frame_source=video_manager)\n",
    "    scene_list = scene_manager.get_scene_list()\n",
    "\n",
    "    video_manager.release()\n",
    "\n",
    "    logging.info(f'Detected {len(scene_list)} scenes:')\n",
    "    for i, scene in enumerate(scene_list):\n",
    "        logging.info(f'Scene {i + 1}: Start {scene[0].get_timecode()} / Frame {scene[0].get_frames()}, '\n",
    "              f'End {scene[1].get_timecode()} / Frame {scene[1].get_frames()}')\n",
    "\n",
    "    return scene_list\n",
    "\n",
    "def get_video_length(video_path):\n",
    "    # You can use a tool like OpenCV, ffmpeg, or similar to calculate video length\n",
    "    import cv2\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_length = frame_count / fps\n",
    "    cap.release()\n",
    "    return video_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Frame Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames_imageio(video_path, scenes, output_dir):\n",
    "    reader = imageio.get_reader(video_path)\n",
    "    for i, scene in enumerate(scenes):\n",
    "        start_frame, end_frame = scene\n",
    "        \n",
    "        # Convert FrameTimecode to integer frame numbers\n",
    "        start_frame_num = int(start_frame)\n",
    "        end_frame_num = int(end_frame)\n",
    "        \n",
    "        # Calculate the middle frame of the scene\n",
    "        middle_frame = (start_frame_num + end_frame_num) // 2\n",
    "        \n",
    "        # Set the reader to the middle frame and extract it\n",
    "        reader.set_image_index(middle_frame)\n",
    "        frame = reader.get_next_data()\n",
    "        \n",
    "        # Save the frame as an image with frame number in the filename\n",
    "        output_path = os.path.join(output_dir, f'scene_{i + 1}_frame_{middle_frame}.jpg')\n",
    "        imageio.imwrite(output_path, frame)\n",
    "        print(f\"Extracted and saved middle frame of scene {i + 1} as {output_path}\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Image Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def encode_image(image_path):\n",
    "    async with aiofiles.open(image_path, \"rb\") as image_file:\n",
    "        content = await image_file.read()\n",
    "        return base64.b64encode(content).decode('utf-8')\n",
    "\n",
    "def get_color_category(color):\n",
    "    r, g, b = [x / 255.0 for x in color]\n",
    "    h, l, s = colorsys.rgb_to_hls(r, g, b)\n",
    "\n",
    "    primary_hues = {\n",
    "        \"red\": (0.0, 0.1),  \n",
    "        \"yellow\": (0.1, 0.18),\n",
    "        \"green\": (0.25, 0.4),\n",
    "        \"blue\": (0.55, 0.75),\n",
    "    }\n",
    "\n",
    "    for color_name, hue_range in primary_hues.items():\n",
    "        if hue_range[0] <= h <= hue_range[1]:\n",
    "            return color_name\n",
    "\n",
    "    if (l >= 0.9 and s <= 0.1):\n",
    "        return \"white\"\n",
    "    if (l <= 0.1 and s <= 0.1):\n",
    "        return \"black\"\n",
    "\n",
    "    return \"non-primary\"\n",
    "\n",
    "def analyze_image_colors(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert('RGB')\n",
    "    data = np.array(image)\n",
    "\n",
    "    unique_colors, counts = np.unique(data.reshape(-1, data.shape[2]), axis=0, return_counts=True)\n",
    "    total_pixels = int(counts.sum())\n",
    "\n",
    "    color_counts = {\n",
    "        \"Red\": 0,\n",
    "        \"Yellow\": 0,\n",
    "        \"Green\": 0,\n",
    "        \"Blue\": 0,\n",
    "        \"White\": 0,\n",
    "        \"Black\": 0,\n",
    "        \"Non-primary\": 0\n",
    "    }\n",
    "\n",
    "    for color, count in zip(unique_colors, counts):\n",
    "        category = get_color_category(tuple(color))\n",
    "        color_counts[category.capitalize()] += int(count)\n",
    "\n",
    "    color_percentages = {color: (count / total_pixels) * 100 for color, count in color_counts.items()}\n",
    "    primary_total = color_counts[\"Red\"] + color_counts[\"Yellow\"] + color_counts[\"Blue\"]\n",
    "    color_dominance = \"Primary colors\" if primary_total > color_counts[\"Non-primary\"] else \"Non-primary colors\"\n",
    "\n",
    "    return {\n",
    "        \"Color Analysis\": {\n",
    "            \"Colors Found\": {\n",
    "                color: {\n",
    "                    \"Pixel Count\": count,\n",
    "                    \"Percentage\": f\"{color_percentages[color]:.2f}%\"\n",
    "                } for color, count in color_counts.items()\n",
    "            },\n",
    "            \"Dominance\": color_dominance\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) OpenAI API Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def send_image_to_openai(image_path, base64_image, retries=3):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"\"\"\n",
    "                        Analyze the following image and provide a detailed description in the format of JSON only. Ensure the output is strictly in JSON format without any additional text or code block formatting. The JSON should include the following standardized labels:\n",
    "\n",
    "                        1. **Image Analysis**: The root dictionary containing all analysis data.\n",
    "                        \n",
    "                        2. **Suitability**:\n",
    "                            - \"Partial Nudity\": Boolean indicating the presence of nudity (e.g., bare torso or non explicits).\n",
    "                            - \"Full Nudity\": Boolean indicating the presence of explicit nudity of private body parts.\n",
    "                            - \"Obscene Gestures\": Boolean indicating the presence of obscene gestures.\n",
    "                            - \"Alcohol\": Boolean indicating the presence of alcohol.\n",
    "                            - \"Drugs\": Boolean indicating the presence of drugs.\n",
    "                            - \"Addictions\": Boolean indicating the presence of addictions.\n",
    "\n",
    "                        3. **Objects**:\n",
    "                            - \"Total Objects Identified\": Integer representing the total number of objects identified.\n",
    "                            - \"Average Features Per Object\": Float representing the average number of features per object.\n",
    "                            - \"Objects Details\": Dictionary containing details of each object, where each object is labeled as \"Object_1\", \"Object_2\", etc., with the following structure:\n",
    "                                - \"Name\": The name of the object - as simplest and descriptive possible.\n",
    "                                - \"Portion Boolean\": 0-1 output indicating if the object is a portion of a larger object (1) or a complete object (0). For example, a leg is a portion of a human. However, if the object is just cropped but clearly identifiable as a complete object, it should be considered a complete object.\n",
    "                                - \"Color\": The color of the object.\n",
    "                                - \"Features\": List of features of the object.\n",
    "                                - \"Total Features\": Integer representing the number of features for the object.\n",
    "\n",
    "                        4. **Place**:\n",
    "                            - \"Name\": The name of the place - as simplest and descriptive as possible.\n",
    "                            - \"Certainty Boolean\": 0-1 output indicating if the place is clearly identifiable (1) or not (0).\n",
    "                            - \"Fantasy/Adventurous Place\": Boolean (0-1) indicating whether the place is classified as a fantasy/adventurous place or not.\n",
    "                            - \"Explanation\": Detailed explanation of why the place is classified as fantasy/adventurous or not. Fantasy places are those that do not exist in reality, and adventurous places are defined as those involving clear statements of traveling to space or another country.\n",
    "\n",
    "                        5. **Characters**:\n",
    "                            - \"Total Characters Identified\": Integer representing the total number of characters identified.\n",
    "                            - \"Average Features Per Character\": Float representing the average number of features per character.\n",
    "                            - \"Character Details\": Dictionary containing details of each character, where each character is labeled as \"Character_1\", \"Character_2\", etc., with the following structure:\n",
    "                                - \"Name\": The name of the character - as simplest and descriptive as possible.\n",
    "                                - \"Portion Boolean\": 0-1 output indicating if the character is a portion of a larger character (1) or a complete character (0). For example, a leg is a portion of a human. However, if the character is just cropped but clearly identifiable as a complete character, it should be considered a complete character.\n",
    "                                - \"Human or Non-Human\": 0-1 output indicating if the character appears human (1) or non-human (0). Anthropomorphized characters or any other combination not fully human are considered non-human.\n",
    "                                - \"Physical Features\": List of physical features of the character.\n",
    "                                - \"Explanation\": Explanation for why the character is classified as human or non-human, and why these physical features are inferred.\n",
    "                                - \"Age\": Expected age range of the character (a single number).\n",
    "                            **Note**: If the \"character\" consists of only a part of a body (such as a hand, leg, or face without enough distinguishing features to identify it as a complete character), do not count it as a \"character.\"\n",
    "\n",
    "                        Ensure that the structure of the JSON output strictly adheres to these standardized labels.\n",
    "                        \"\"\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 750\n",
    "    }\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload) as response:\n",
    "                    status = response.status\n",
    "                    response_text = await response.text()\n",
    "\n",
    "                    if status == 429:\n",
    "                        print(\"Rate limit exceeded, retrying...\")\n",
    "                        await asyncio.sleep(2 ** attempt)\n",
    "                        continue\n",
    "                    elif status == 200:\n",
    "                        content = await response.json()\n",
    "\n",
    "                        # Track API usage\n",
    "                        api_usage['total_api_calls'] += 1\n",
    "                        api_usage['total_tokens_used'] += content.get('usage', {}).get('total_tokens', 0)\n",
    "                        api_usage['model_used'] = content.get('model', 'gpt-4o-mini')\n",
    "\n",
    "                        # Handle and return JSON content\n",
    "                        if 'choices' in content:\n",
    "                            message_content = content['choices'][0].get('message', {}).get('content', '').strip()\n",
    "                            try:\n",
    "                                return json.loads(message_content)\n",
    "                            except json.JSONDecodeError as e:\n",
    "                                print(f\"Error decoding JSON from OpenAI response for {image_path}: {e}\")\n",
    "                                return None\n",
    "                        else:\n",
    "                            print(f\"Unexpected response format from OpenAI API for {image_path}.\")\n",
    "                            return None\n",
    "                    else:\n",
    "                        print(f\"Request failed with status code {status} for {image_path}.\")\n",
    "                        return None\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Request failed due to a client error: {e}\")\n",
    "            await asyncio.sleep(2 ** attempt)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error occurred: {e}\")\n",
    "            await asyncio.sleep(2 ** attempt)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Scene Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_scenes_output(output_dir, json_output_dir):\n",
    "    os.makedirs(json_output_dir, exist_ok=True)\n",
    "    scenes = sorted([f for f in os.listdir(output_dir) if f.endswith('.jpg')], key=extract_scene_number)\n",
    "    total_scenes = len(scenes)\n",
    "    with tqdm(total=total_scenes, desc=\"Processing Scenes\", unit=\"scene\") as pbar:\n",
    "        tasks = [process_single_scene(i, scene, output_dir, json_output_dir, pbar) for i, scene in enumerate(scenes)]\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "async def process_single_scene(i, scene, output_dir, json_output_dir, pbar):\n",
    "    async with semaphore:  # Limit concurrent execution\n",
    "        scene_path = os.path.join(output_dir, scene)\n",
    "\n",
    "        # Encode image in base64\n",
    "        base64_image = await encode_image(scene_path)\n",
    "\n",
    "        # Perform color analysis\n",
    "        color_analysis_result = analyze_image_colors(scene_path)\n",
    "\n",
    "        # Send image to OpenAI for further analysis\n",
    "        openai_response = await send_image_to_openai(scene_path, base64_image)\n",
    "\n",
    "        # Check if openai_response is valid (not None or empty)\n",
    "        if not openai_response:\n",
    "            print(f\"Skipping {scene} due to invalid OpenAI response.\")\n",
    "            pbar.update(1)\n",
    "            return\n",
    "\n",
    "        # Combine both results, and include the reference to the image file\n",
    "        final_output = {\n",
    "            \"Image File\": scene,\n",
    "            \"Image Analysis\": {\n",
    "                **color_analysis_result[\"Color Analysis\"],\n",
    "                **openai_response.get(\"Image Analysis\", {})\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # The filename already includes the scene number and frame number\n",
    "        output_filename = os.path.splitext(scene)[0] + '_analysis.json'\n",
    "        output_path = os.path.join(json_output_dir, output_filename)\n",
    "\n",
    "        try:\n",
    "            async with aiofiles.open(output_path, 'w') as json_file:\n",
    "                await json_file.write(json.dumps(final_output, indent=4))\n",
    "                print(f\"Saved analysis for {scene} as {output_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save analysis for {scene}: {e}\")\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "def extract_scene_number(filename):\n",
    "    match = re.search(r'\\d+', filename)\n",
    "    return int(match.group()) if match else -1\n",
    "\n",
    "def extract_frame_number(filename):\n",
    "    match = re.search(r'_frame_(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Luminance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Helper function to calculate the average luminance of a frame\n",
    "def calculate_luminance(image):\n",
    "    grayscale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return np.mean(grayscale)\n",
    "\n",
    "# Function to save the first and last frames of a scene side by side\n",
    "def save_frame_pair(output_dir, scene_index, last_frame, first_frame, strong_luminance=False):\n",
    "    # Create luminance_output directory if it doesn't exist\n",
    "    luminance_output_dir = os.path.join(output_dir, \"luminance_output\")\n",
    "    os.makedirs(luminance_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Concatenate images horizontally (side by side)\n",
    "    concatenated_image = np.concatenate((last_frame, first_frame), axis=1)\n",
    "\n",
    "    # Add a label in the filename if strong luminance change is detected\n",
    "    filename_suffix = \"_STRONG_LUMINANCE\" if strong_luminance else \"\"\n",
    "    \n",
    "    # Save the concatenated image in the luminance_output folder\n",
    "    output_image_path = os.path.join(luminance_output_dir, f\"scene_{scene_index + 1}_to_{scene_index + 2}{filename_suffix}.jpg\")\n",
    "    cv2.imwrite(output_image_path, concatenated_image)\n",
    "    print(f\"Saved frame comparison image to {output_image_path}\")\n",
    "\n",
    "\n",
    "# Analyze scene transitions for luminance changes and detect short scenes, saving the images\n",
    "def analyze_scenes_for_flicker_and_short_scenes(video_path, scenes, video_length, output_dir, short_scene_threshold=1.0, luminance_threshold=25):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_scenes = len(scenes)\n",
    "    strong_luminance_changes = 0\n",
    "    short_scenes_count = 0\n",
    "\n",
    "    print(f\"Analyzing scenes for flicker and short scene detection...\", flush=True)\n",
    "\n",
    "    for scene_index in range(total_scenes - 1):\n",
    "        start_timecode, end_timecode = scenes[scene_index]\n",
    "        next_start_timecode, next_end_timecode = scenes[scene_index + 1]\n",
    "\n",
    "        # Convert timecodes to frame numbers\n",
    "        start_frame = int(start_timecode.get_frames())\n",
    "        end_frame = int(end_timecode.get_frames())\n",
    "        next_start_frame = int(next_start_timecode.get_frames())\n",
    "\n",
    "        # Get the last frame of the current scene\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, end_frame - 1)  # Extract the exact last frame\n",
    "        ret1, last_frame = cap.read()\n",
    "\n",
    "        # Get the first frame of the next scene\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, next_start_frame)  # Extract the exact first frame\n",
    "        ret2, first_frame = cap.read()\n",
    "\n",
    "        # Check if frames are read correctly\n",
    "        if not ret1:\n",
    "            print(f\"Failed to retrieve last frame of Scene {scene_index + 1}\", flush=True)\n",
    "        if not ret2:\n",
    "            print(f\"Failed to retrieve first frame of Scene {scene_index + 2}\", flush=True)\n",
    "\n",
    "        if ret1 and ret2:\n",
    "            # Calculate luminance for the last frame of the current scene and the first frame of the next scene\n",
    "            last_luminance = calculate_luminance(last_frame)\n",
    "            first_luminance = calculate_luminance(first_frame)\n",
    "\n",
    "            print(f\"Scene {scene_index + 1} to Scene {scene_index + 2}: Last Luminance: {last_luminance}, First Luminance: {first_luminance}\", flush=True)\n",
    "\n",
    "            # Check for strong luminance changes (flicker detection)\n",
    "            strong_luminance = False\n",
    "            if last_luminance is not None and first_luminance is not None:\n",
    "                luminance_change = abs(first_luminance - last_luminance)\n",
    "                print(f\"Luminance change for scene {scene_index + 1} to {scene_index + 2}: {luminance_change}\", flush=True)\n",
    "\n",
    "                if luminance_change > luminance_threshold:\n",
    "                    strong_luminance_changes += 1\n",
    "                    strong_luminance = True\n",
    "                    print(f\"Strong luminance change detected between Scene {scene_index + 1} and Scene {scene_index + 2}\", flush=True)\n",
    "\n",
    "            # Save the frame comparison side by side, with a label if strong luminance change is detected\n",
    "            save_frame_pair(output_dir, scene_index, last_frame, first_frame, strong_luminance)\n",
    "\n",
    "        # Calculate the duration of each scene and check for short scenes\n",
    "        scene_duration = (end_frame - start_frame) / fps if video_length > 0 else None\n",
    "        print(f\"Scene {scene_index + 1} duration: {scene_duration} seconds\", flush=True)\n",
    "\n",
    "        if scene_duration is not None and scene_duration < short_scene_threshold:\n",
    "            short_scenes_count += 1\n",
    "            print(f\"Short scene detected in Scene {scene_index + 1}\", flush=True)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Calculate percentages\n",
    "    percentage_strong_transitions = (strong_luminance_changes / (total_scenes - 1)) * 100 if total_scenes > 1 else 0\n",
    "    percentage_short_scenes = (short_scenes_count / total_scenes) * 100 if total_scenes > 0 else 0\n",
    "\n",
    "    print(f\"Percentage of strong luminance transitions_30%: {percentage_strong_transitions}%\")\n",
    "    print(f\"Percentage of short scenes: {percentage_short_scenes}%\")\n",
    "\n",
    "    return {\n",
    "        \"percentage_strong_luminance_transitions\": percentage_strong_transitions,\n",
    "        \"percentage_short_scenes_3secs\": percentage_short_scenes,\n",
    "        \"number_of_strong_luminance_transitions_30%\": strong_luminance_changes,\n",
    "        \"number_of_short_scenes_3secs\": short_scenes_count\n",
    "    }\n",
    "\n",
    "# Example to run the full process with your existing functions\n",
    "def process_video_with_flicker_analysis(video_path, output_dir, short_scene_threshold=3.0, luminance_threshold=25):\n",
    "    # Get the video length\n",
    "    video_length = get_video_length(video_path)\n",
    "    \n",
    "    # Detect scenes using your analyze_video function\n",
    "    scenes = analyze_video(video_path)\n",
    "    \n",
    "    # Ensure scenes are properly detected\n",
    "    if not scenes:\n",
    "        print(\"No scenes detected.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Total scenes detected: {len(scenes)}\")\n",
    "    \n",
    "    # Analyze luminance changes and short scenes\n",
    "    flicker_and_short_scene_stats = analyze_scenes_for_flicker_and_short_scenes(video_path, scenes, video_length, output_dir, short_scene_threshold, luminance_threshold)\n",
    "    \n",
    "    # Output results\n",
    "    print(f\"Flicker and short scene analysis results: {flicker_and_short_scene_stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Run whole analysis of each json output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Path Construction: get_image_path generates the correct path to the image file based on the JSON filename.\n",
    "\n",
    "Entity Extraction:extract_entities_from_json pulls characters, objects, and places from the JSON data.\n",
    "\n",
    "Image-to-Image Comparison:perform_image_to_image_comparison compares partial objects with full objects using the OpenAI API.\n",
    "\n",
    "Entity Comparison:compare_entities handles both name-based and image-based comparisons to decide whether two entities should be consolidated.\n",
    "\n",
    "Consolidation:Entities across frames are consolidated into a single summary file that tracks where each entity was found.\n",
    "\n",
    "Main Execution:The script runs through all JSON files, processes the entities, and saves the consolidated results to a summary JSON file.\n",
    "\n",
    "Key Features of This Implementation:\n",
    "Text-Based Comparison: The code first attempts to merge entities based on exact name matches. If no match is found, it uses the OpenAI API to determine if two entities with different names should be merged.\n",
    "\n",
    "Image-to-Image Comparison: If one of the entities is flagged as a portion, or if names don't match but the entities might still be the same, the code performs an image-to-image comparison using the OpenAI API.\n",
    "\n",
    "Efficient Processing: The code processes each frame sequentially and logs all merges into merged_entities_log, ensuring you have a record of what entities were merged, including their original names and frames.\n",
    "\n",
    "No Overwritten Functionality: The original image analysis functionality is preserved and integrated smoothly with the text-based comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Unique Characters, Objects, and Places: This can be done by counting the keys in the consolidated_data dictionary.\n",
    "Average Characters per Frame: This can be calculated by summing up all instances of characters found across frames and dividing by the total number of frames where characters appear.\n",
    "Average Features per Character/Object: Calculate this by summing the features of all characters/objects and dividing by the total number of characters/objects.\n",
    "Overall Color Analysis: Aggregate the color data from all JSON files.\n",
    "Filter Compliance: Check for any instances where the filters (e.g., nudity, drugs) are not compliant and log the frame numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import openai\n",
    "\n",
    "# Apply nest_asyncio to handle the running event loop\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize OpenAI API\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OpenAI API key is not set.\")\n",
    "openai.api_key = api_key\n",
    "\n",
    "#Initial consolidation of entities\n",
    "def initial_consolidation(json_files):\n",
    "    consolidated_data = {\"characters\": {}, \"objects\": {}, \"places\": {}}\n",
    "    merge_tracking = {\"characters\": {}, \"objects\": {}, \"places\": {}}\n",
    "\n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            consolidate_from_json(json_data, consolidated_data, merge_tracking, os.path.basename(json_file))\n",
    "\n",
    "    return consolidated_data\n",
    "\n",
    "\n",
    "# Function to save entities to JSON\n",
    "def save_entities_to_json(entities, path):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(entities, f, indent=4)\n",
    "\n",
    "# Function to consolidate entities from a JSON file\n",
    "# Function to consolidate entities from a JSON file\n",
    "def consolidate_from_json(json_data, consolidated_data, merge_tracking, json_file_name):\n",
    "    frame_number = json_file_name.split('_')[3]  # Extract frame number\n",
    "\n",
    "    if \"Image Analysis\" in json_data:\n",
    "        characters = json_data[\"Image Analysis\"].get(\"Characters\", {}).get(\"Character Details\", {})\n",
    "        for key, details in characters.items():\n",
    "            name = details.get(\"Name\")\n",
    "            if name:\n",
    "                if name not in consolidated_data[\"characters\"]:\n",
    "                    consolidated_data[\"characters\"][name] = details\n",
    "                    consolidated_data[\"characters\"][name][\"merged_from\"] = []\n",
    "                    merge_tracking[\"characters\"][name] = {\"merged_from\": []}\n",
    "                if frame_number not in consolidated_data[\"characters\"][name][\"merged_from\"]:\n",
    "                    consolidated_data[\"characters\"][name][\"merged_from\"].append(frame_number)\n",
    "                    merge_tracking[\"characters\"][name][\"merged_from\"].append(frame_number)\n",
    "\n",
    "        objects = json_data[\"Image Analysis\"].get(\"Objects\", {}).get(\"Objects Details\", {})\n",
    "        for key, details in objects.items():\n",
    "            name = details.get(\"Name\")\n",
    "            if name:\n",
    "                if name not in consolidated_data[\"objects\"]:\n",
    "                    consolidated_data[\"objects\"][name] = details\n",
    "                    consolidated_data[\"objects\"][name][\"merged_from\"] = []\n",
    "                    merge_tracking[\"objects\"][name] = {\"merged_from\": []}\n",
    "                if frame_number not in consolidated_data[\"objects\"][name][\"merged_from\"]:\n",
    "                    consolidated_data[\"objects\"][name][\"merged_from\"].append(frame_number)\n",
    "                    merge_tracking[\"objects\"][name][\"merged_from\"].append(frame_number)\n",
    "\n",
    "        place = json_data[\"Image Analysis\"].get(\"Place\", {})\n",
    "        place_name = place.get(\"Name\")\n",
    "        if place_name:\n",
    "            if place_name not in consolidated_data[\"places\"]:\n",
    "                consolidated_data[\"places\"][place_name] = place\n",
    "                consolidated_data[\"places\"][place_name][\"merged_from\"] = []\n",
    "                merge_tracking[\"places\"][place_name] = {\"merged_from\": []}\n",
    "            if frame_number not in consolidated_data[\"places\"][place_name][\"merged_from\"]:\n",
    "                consolidated_data[\"places\"][place_name][\"merged_from\"].append(frame_number)\n",
    "                merge_tracking[\"places\"][place_name][\"merged_from\"].append(frame_number)\n",
    "\n",
    "\n",
    "# Function to cluster entities using OpenAI API and name the clusters\n",
    "# Initialize API usage tracking\n",
    "api_usage = {\n",
    "    \"total_api_calls\": 0,\n",
    "    \"total_tokens_used\": 0,\n",
    "    \"model_used\": \"gpt-4\",  # Set the default model name here\n",
    "}\n",
    "\n",
    "# Function to cluster entities using OpenAI API and track token usage\n",
    "async def cluster_entities(api_key, entities):\n",
    "    # Generate lists for characters, objects, and places from entities\n",
    "    character_list = ', '.join(entities['characters'].keys())\n",
    "    object_list = ', '.join(entities['objects'].keys())\n",
    "    place_list = ', '.join(entities['places'].keys())\n",
    "\n",
    "    if not character_list and not object_list and not place_list:\n",
    "        return \"No entities available to cluster.\"\n",
    "\n",
    "    # Adjusted OpenAI prompt to return structured output (dictionaries)\n",
    "    prompt = f\"\"\"\n",
    "    You are tasked with clustering and naming entities from a TV show. Below are lists of characters, objects, and places extracted from different scenes. These lists sometimes contain multiple labels for the same entity.\n",
    "\n",
    "    **Instructions:**\n",
    "\n",
    "    1. Group the characters, objects, and places that refer to the same entity and suggest a single **final name** for each group (be smart what could be the same individual/object in the show one and what couldn't).\n",
    "    2. Return the result as a dictionary where each cluster (key) contains the entities (values) that belong to that cluster.\n",
    "    3. Use this format:\n",
    "\n",
    "    {{\n",
    "      \"Characters Clusters\": {{\n",
    "        \"Final Name 1\": [\"Character 1\", \"Character 2\", ...],\n",
    "        \"Final Name 2\": [\"Character 3\", \"Character 4\", ...]\n",
    "      }},\n",
    "      \"Objects Clusters\": {{\n",
    "        \"Final Name 1\": [\"Object 1\", \"Object 2\", ...],\n",
    "        \"Final Name 2\": [\"Object 3\", \"Object 4\", ...]\n",
    "      }},\n",
    "      \"Places Clusters\": {{\n",
    "        \"Final Name 1\": [\"Place 1\", \"Place 2\", ...],\n",
    "        \"Final Name 2\": [\"Place 3\", \"Place 4\", ...]\n",
    "      }}\n",
    "    }}\n",
    "\n",
    "    **Characters:**\n",
    "    {character_list}\n",
    "\n",
    "    **Objects:**\n",
    "    {object_list}\n",
    "\n",
    "    **Places:**\n",
    "    {place_list}\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4\",  # Ensure you set the correct model here\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": 2000\n",
    "    }\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload) as response:\n",
    "                response_json = await response.json()\n",
    "                clusters = response_json.get('choices', [{}])[0].get('message', {}).get('content', '')\n",
    "\n",
    "                # Track API usage\n",
    "                api_usage['total_api_calls'] += 1\n",
    "                api_usage['total_tokens_used'] += response_json.get('usage', {}).get('total_tokens', 0)\n",
    "                api_usage['model_used'] = response_json.get('model', 'gpt-4')\n",
    "\n",
    "                return json.loads(clusters)  # Convert the response to JSON\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing API response: {e}\")\n",
    "            return {}\n",
    "\n",
    "\n",
    "# Function to merge clusters with final_final_entities.json, keeping names and frame numbers\n",
    "def merge_clusters_with_entities(final_entities, clusters):\n",
    "    merged_entities = {\"characters\": {}, \"objects\": {}, \"places\": {}}\n",
    "\n",
    "    # Process characters\n",
    "    character_clusters = clusters.get(\"Characters Clusters\", {})\n",
    "    for final_name, cluster_items in character_clusters.items():\n",
    "        merged_entities[\"characters\"][final_name] = {\"merged_from\": [], \"merged_names\": []}\n",
    "        for item in cluster_items:\n",
    "            if item in final_entities[\"characters\"]:\n",
    "                entity_data = final_entities[\"characters\"][item]\n",
    "                merged_entities[\"characters\"][final_name] = {\n",
    "                    **entity_data,\n",
    "                    \"Name\": final_name,\n",
    "                    \"merged_from\": list(set(merged_entities[\"characters\"][final_name][\"merged_from\"] + entity_data[\"merged_from\"])),\n",
    "                    \"merged_names\": list(set(merged_entities[\"characters\"][final_name][\"merged_names\"] + [item]))\n",
    "                }\n",
    "\n",
    "    # Process objects\n",
    "    object_clusters = clusters.get(\"Objects Clusters\", {})\n",
    "    for final_name, cluster_items in object_clusters.items():\n",
    "        merged_entities[\"objects\"][final_name] = {\"merged_from\": [], \"merged_names\": []}\n",
    "        for item in cluster_items:\n",
    "            if item in final_entities[\"objects\"]:\n",
    "                entity_data = final_entities[\"objects\"][item]\n",
    "                merged_entities[\"objects\"][final_name] = {\n",
    "                    **entity_data,\n",
    "                    \"Name\": final_name,\n",
    "                    \"merged_from\": list(set(merged_entities[\"objects\"][final_name][\"merged_from\"] + entity_data[\"merged_from\"])),\n",
    "                    \"merged_names\": list(set(merged_entities[\"objects\"][final_name][\"merged_names\"] + [item]))\n",
    "                }\n",
    "\n",
    "    # Process places\n",
    "    place_clusters = clusters.get(\"Places Clusters\", {})\n",
    "    for final_name, cluster_items in place_clusters.items():\n",
    "        merged_entities[\"places\"][final_name] = {\"merged_from\": [], \"merged_names\": []}\n",
    "        for item in cluster_items:\n",
    "            if item in final_entities[\"places\"]:\n",
    "                entity_data = final_entities[\"places\"][item]\n",
    "                merged_entities[\"places\"][final_name] = {\n",
    "                    **entity_data,\n",
    "                    \"Name\": final_name,\n",
    "                    \"merged_from\": list(set(merged_entities[\"places\"][final_name][\"merged_from\"] + entity_data[\"merged_from\"])),\n",
    "                    \"merged_names\": list(set(merged_entities[\"places\"][final_name][\"merged_names\"] + [item]))\n",
    "                }\n",
    "\n",
    "    return merged_entities\n",
    "\n",
    "# Main async function for clustering and merging entities\n",
    "# Main async function for clustering and merging entities\n",
    "async def cluster_and_merge_entities(api_key, json_output_dir, video_folder_path):\n",
    "    json_files = [os.path.join(json_output_dir, f) for f in os.listdir(json_output_dir) if f.endswith('.json')]\n",
    "\n",
    "    if not json_files:\n",
    "        print(\"No JSON files found for consolidation.\")\n",
    "        return None\n",
    "\n",
    "    # Consolidate entities from all JSON files\n",
    "    final_entities = initial_consolidation(json_files)  # Ensure only json_files is passed here\n",
    "\n",
    "    # Cluster entities using OpenAI API\n",
    "    clusters = await cluster_entities(api_key, final_entities)\n",
    "\n",
    "    if not clusters:\n",
    "        print(\"No clusters returned from OpenAI API.\")\n",
    "        return None\n",
    "\n",
    "    # Merge clusters into final final JSON\n",
    "    merged_final_entities = merge_clusters_with_entities(final_entities, clusters)\n",
    "\n",
    "    # Save the merged entities to JSON\n",
    "    merged_final_entities_path = os.path.join(video_folder_path, 'final_summary.json')\n",
    "    save_entities_to_json(merged_final_entities, merged_final_entities_path)\n",
    "    print(f\"Merged entities saved to {merged_final_entities_path}\")\n",
    "    \n",
    "    return merged_final_entities  # Return merged entities for further use\n",
    "\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Define paths\n",
    "#     final_final_json_path = \"path/to/final_final_entities.json\"\n",
    "#     video_folder_path = \"path/to/video_folder\"\n",
    "    \n",
    "#     # Run the async function\n",
    "#     asyncio.run(cluster_and_merge_entities(api_key, final_final_json_path, video_folder_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summary_statistics(entities):\n",
    "    statistics = {\n",
    "        \"number_of_characters\": len(entities.get(\"characters\", {})),\n",
    "        \"number_of_objects\": len(entities.get(\"objects\", {})),\n",
    "        \"number_of_places\": len(entities.get(\"places\", {})),\n",
    "        \"avg_features_per_character\": 0,\n",
    "        \"avg_features_per_object\": 0,\n",
    "        \"avg_appearances_per_character\": 0,\n",
    "        \"avg_appearances_per_object\": 0,\n",
    "    }\n",
    "\n",
    "    # Calculate average features per character\n",
    "    total_character_features = 0\n",
    "    total_character_appearances = 0\n",
    "    for char, details in entities.get(\"characters\", {}).items():\n",
    "        total_character_features += len(details.get(\"Physical Features\", []))\n",
    "        total_character_appearances += len(details.get(\"merged_from\", []))\n",
    "    \n",
    "    if statistics[\"number_of_characters\"] > 0:\n",
    "        statistics[\"avg_features_per_character\"] = total_character_features / statistics[\"number_of_characters\"]\n",
    "        statistics[\"avg_appearances_per_character\"] = total_character_appearances / statistics[\"number_of_characters\"]\n",
    "\n",
    "    # Calculate average features per object\n",
    "    total_object_features = 0\n",
    "    total_object_appearances = 0\n",
    "    for obj, details in entities.get(\"objects\", {}).items():\n",
    "        total_object_features += details.get(\"Total Features\", 0)\n",
    "        total_object_appearances += len(details.get(\"merged_from\", []))\n",
    "\n",
    "    if statistics[\"number_of_objects\"] > 0:\n",
    "        statistics[\"avg_features_per_object\"] = total_object_features / statistics[\"number_of_objects\"]\n",
    "        statistics[\"avg_appearances_per_object\"] = total_object_appearances / statistics[\"number_of_objects\"]\n",
    "\n",
    "    return statistics\n",
    "\n",
    "def modify_summary_json_with_statistics(entities, path):\n",
    "    # Compute statistics\n",
    "    statistics = compute_summary_statistics(entities)\n",
    "    \n",
    "    # Load the original JSON data\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Create a new dictionary with statistics at the top\n",
    "    modified_data = {\n",
    "        \"summary_statistics\": statistics,\n",
    "        **data  # This merges the existing data under the statistics\n",
    "    }\n",
    "\n",
    "    # Save the modified data back to the JSON\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(modified_data, f, indent=4)\n",
    "\n",
    "    print(f\"Summary statistics added and saved to {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_additional_stats(video_path, scenes_output_dir, start_time, end_time, video_output_dir, json_output_dir, summary_statistics, scenes):\n",
    "    # Initialize additional information\n",
    "    additional_info = {\n",
    "        \"video_title\": None,\n",
    "        \"video_size_bytes\": None,\n",
    "        \"video_length_seconds\": None,\n",
    "        \"number_of_scenes\": None,\n",
    "        \"processing_time_seconds\": None\n",
    "    }\n",
    "\n",
    "    # Extract video title\n",
    "    try:\n",
    "        additional_info[\"video_title\"] = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        print(f\"Video title: {additional_info['video_title']}\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting video title: {e}\", flush=True)\n",
    "\n",
    "    # Extract video size\n",
    "    try:\n",
    "        additional_info[\"video_size_bytes\"] = os.path.getsize(video_path)\n",
    "        print(f\"Video size: {additional_info['video_size_bytes']} bytes\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting video size: {e}\", flush=True)\n",
    "\n",
    "    # Get video length\n",
    "    try:\n",
    "        additional_info[\"video_length_seconds\"] = get_video_length(video_path)\n",
    "        print(f\"Video length: {additional_info['video_length_seconds']} seconds\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating video length: {e}\", flush=True)\n",
    "\n",
    "    # Count the number of scenes\n",
    "    try:\n",
    "        additional_info[\"number_of_scenes\"] = len([f for f in os.listdir(scenes_output_dir) if f.endswith('.jpg')])\n",
    "        print(f\"Number of scenes: {additional_info['number_of_scenes']}\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error counting scenes: {e}\", flush=True)\n",
    "\n",
    "    # Calculate processing time\n",
    "    try:\n",
    "        additional_info[\"processing_time_seconds\"] = end_time - start_time\n",
    "        print(f\"Processing time: {additional_info['processing_time_seconds']} seconds\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating processing time: {e}\", flush=True)\n",
    "\n",
    "    # Analyze all JSON files in the json_output folder\n",
    "    try:\n",
    "        json_stats = analyze_json_files(json_output_dir)\n",
    "        additional_info.update(json_stats)\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing JSON files: {e}\", flush=True)\n",
    "\n",
    "    # Perform luminance analysis and short scene detection\n",
    "    try:\n",
    "        flicker_stats = analyze_scenes_for_flicker_and_short_scenes(video_path, scenes, additional_info[\"video_length_seconds\"], video_output_dir)\n",
    "        additional_info.update(flicker_stats)\n",
    "        print(f\"Luminance and short scene analysis results: {flicker_stats}\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error performing luminance analysis: {e}\", flush=True)\n",
    "\n",
    "    # Include API usage stats\n",
    "    additional_info[\"api_usage\"] = {\n",
    "        \"total_api_calls\": api_usage[\"total_api_calls\"],\n",
    "        \"total_tokens_used\": api_usage[\"total_tokens_used\"],\n",
    "        \"model_used\": api_usage[\"model_used\"]\n",
    "    }\n",
    "\n",
    "    # Include the summary statistics in the same JSON file\n",
    "    additional_info[\"summary_statistics\"] = summary_statistics\n",
    "\n",
    "    # Save the additional stats in a new JSON file inside the video folder\n",
    "    stats_output_path = os.path.join(video_output_dir, f'{additional_info[\"video_title\"]}_stats.json')\n",
    "\n",
    "    try:\n",
    "        with open(stats_output_path, 'w') as f:\n",
    "            json.dump(additional_info, f, indent=4)\n",
    "        print(f\"Additional stats saved to {stats_output_path}\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving stats to JSON: {e}\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_json_files(json_output_dir):\n",
    "    color_distribution = {\n",
    "        \"Red\": 0,\n",
    "        \"Yellow\": 0,\n",
    "        \"Green\": 0,\n",
    "        \"Blue\": 0,\n",
    "        \"White\": 0,\n",
    "        \"Black\": 0,\n",
    "        \"Non-primary\": 0\n",
    "    }\n",
    "    total_frames = 0\n",
    "    non_compliant_frames = []\n",
    "    total_fantasy_places = 0\n",
    "    total_places = 0\n",
    "\n",
    "    # Iterate over all JSON files in the json_output folder\n",
    "    for json_file in os.listdir(json_output_dir):\n",
    "        if not json_file.endswith('.json'):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(os.path.join(json_output_dir, json_file), 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # Get color data\n",
    "            image_analysis = data.get(\"Image Analysis\", {})\n",
    "            colors_found = image_analysis.get(\"Colors Found\", {})\n",
    "\n",
    "            # Ensure \"Colors Found\" contains the expected structure\n",
    "            for color in [\"Red\", \"Yellow\", \"Green\", \"Blue\", \"White\", \"Black\", \"Non-primary\"]:\n",
    "                color_info = colors_found.get(color, {})\n",
    "                percentage_str = color_info.get(\"Percentage\", \"0%\").replace('%', '')\n",
    "                try:\n",
    "                    color_percentage = float(percentage_str)\n",
    "                    color_distribution[color] += color_percentage\n",
    "                except (ValueError, TypeError):\n",
    "                    print(f\"Invalid color percentage in file {json_file} for color {color}\")\n",
    "                    continue\n",
    "\n",
    "            total_frames += 1\n",
    "\n",
    "            # Check for compliance issues (Suitability)\n",
    "            suitability = image_analysis.get(\"Suitability\", {})\n",
    "            if suitability:  # Make sure suitability exists and is not empty\n",
    "                non_compliant_features = [feature for feature, value in suitability.items() if value]\n",
    "                if non_compliant_features:\n",
    "                    non_compliant_frames.append({\n",
    "                        \"frame\": json_file,\n",
    "                        \"features\": non_compliant_features\n",
    "                    })\n",
    "\n",
    "            # Check for fantasy/adventurous places\n",
    "            place = image_analysis.get(\"Place\", {})\n",
    "            if place.get(\"Certainty Boolean\") == 1:\n",
    "                total_places += 1\n",
    "                if place.get(\"Fantasy/Adventurous Place\") == 1:\n",
    "                    total_fantasy_places += 1\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON in file {json_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {json_file}: {e}\")\n",
    "\n",
    "    # Calculate final stats\n",
    "    overall_color_distribution = {color: (value / total_frames) if total_frames > 0 else 0\n",
    "                                  for color, value in color_distribution.items()}\n",
    "    percentage_fantasy_places = (total_fantasy_places / total_places * 100) if total_places > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"Overall Color Distribution\": overall_color_distribution,\n",
    "        \"Non-Compliant Frames\": non_compliant_frames,\n",
    "        \"Percentage of Fantasy/Adventurous Places\": percentage_fantasy_places\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Main Function Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Videos:   0%|          | 0/1 [00:00<?, ?video/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: shinchantrim.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:pyscenedetect:VideoManager is deprecated and will be removed.\n",
      "INFO:pyscenedetect:Loaded 1 video, framerate: 30.000 FPS, resolution: 442 x 360\n",
      "INFO:pyscenedetect:Detecting scenes...\n",
      "INFO:root:Detected 33 scenes:\n",
      "INFO:root:Scene 1: Start 00:00:00.000 / Frame 0, End 00:00:00.500 / Frame 15\n",
      "INFO:root:Scene 2: Start 00:00:00.500 / Frame 15, End 00:00:03.733 / Frame 112\n",
      "INFO:root:Scene 3: Start 00:00:03.733 / Frame 112, End 00:00:06.400 / Frame 192\n",
      "INFO:root:Scene 4: Start 00:00:06.400 / Frame 192, End 00:00:07.600 / Frame 228\n",
      "INFO:root:Scene 5: Start 00:00:07.600 / Frame 228, End 00:00:08.600 / Frame 258\n",
      "INFO:root:Scene 6: Start 00:00:08.600 / Frame 258, End 00:00:09.700 / Frame 291\n",
      "INFO:root:Scene 7: Start 00:00:09.700 / Frame 291, End 00:00:12.200 / Frame 366\n",
      "INFO:root:Scene 8: Start 00:00:12.200 / Frame 366, End 00:00:14.333 / Frame 430\n",
      "INFO:root:Scene 9: Start 00:00:14.333 / Frame 430, End 00:00:15.667 / Frame 470\n",
      "INFO:root:Scene 10: Start 00:00:15.667 / Frame 470, End 00:00:17.200 / Frame 516\n",
      "INFO:root:Scene 11: Start 00:00:17.200 / Frame 516, End 00:00:19.367 / Frame 581\n",
      "INFO:root:Scene 12: Start 00:00:19.367 / Frame 581, End 00:00:21.967 / Frame 659\n",
      "INFO:root:Scene 13: Start 00:00:21.967 / Frame 659, End 00:00:24.300 / Frame 729\n",
      "INFO:root:Scene 14: Start 00:00:24.300 / Frame 729, End 00:00:26.300 / Frame 789\n",
      "INFO:root:Scene 15: Start 00:00:26.300 / Frame 789, End 00:00:28.667 / Frame 860\n",
      "INFO:root:Scene 16: Start 00:00:28.667 / Frame 860, End 00:00:31.167 / Frame 935\n",
      "INFO:root:Scene 17: Start 00:00:31.167 / Frame 935, End 00:00:31.967 / Frame 959\n",
      "INFO:root:Scene 18: Start 00:00:31.967 / Frame 959, End 00:00:33.267 / Frame 998\n",
      "INFO:root:Scene 19: Start 00:00:33.267 / Frame 998, End 00:00:34.667 / Frame 1040\n",
      "INFO:root:Scene 20: Start 00:00:34.667 / Frame 1040, End 00:00:35.800 / Frame 1074\n",
      "INFO:root:Scene 21: Start 00:00:35.800 / Frame 1074, End 00:00:37.567 / Frame 1127\n",
      "INFO:root:Scene 22: Start 00:00:37.567 / Frame 1127, End 00:00:38.167 / Frame 1145\n",
      "INFO:root:Scene 23: Start 00:00:38.167 / Frame 1145, End 00:00:39.167 / Frame 1175\n",
      "INFO:root:Scene 24: Start 00:00:39.167 / Frame 1175, End 00:00:43.167 / Frame 1295\n",
      "INFO:root:Scene 25: Start 00:00:43.167 / Frame 1295, End 00:00:44.100 / Frame 1323\n",
      "INFO:root:Scene 26: Start 00:00:44.100 / Frame 1323, End 00:00:45.100 / Frame 1353\n",
      "INFO:root:Scene 27: Start 00:00:45.100 / Frame 1353, End 00:00:47.000 / Frame 1410\n",
      "INFO:root:Scene 28: Start 00:00:47.000 / Frame 1410, End 00:00:48.467 / Frame 1454\n",
      "INFO:root:Scene 29: Start 00:00:48.467 / Frame 1454, End 00:00:49.600 / Frame 1488\n",
      "INFO:root:Scene 30: Start 00:00:49.600 / Frame 1488, End 00:00:51.767 / Frame 1553\n",
      "INFO:root:Scene 31: Start 00:00:51.767 / Frame 1553, End 00:00:53.967 / Frame 1619\n",
      "INFO:root:Scene 32: Start 00:00:53.967 / Frame 1619, End 00:00:59.600 / Frame 1788\n",
      "INFO:root:Scene 33: Start 00:00:59.600 / Frame 1788, End 00:00:59.667 / Frame 1790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted and saved middle frame of scene 1 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_1_frame_7.jpg\n",
      "Extracted and saved middle frame of scene 2 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_2_frame_63.jpg\n",
      "Extracted and saved middle frame of scene 3 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_3_frame_152.jpg\n",
      "Extracted and saved middle frame of scene 4 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_4_frame_210.jpg\n",
      "Extracted and saved middle frame of scene 5 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_5_frame_243.jpg\n",
      "Extracted and saved middle frame of scene 6 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_6_frame_274.jpg\n",
      "Extracted and saved middle frame of scene 7 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_7_frame_328.jpg\n",
      "Extracted and saved middle frame of scene 8 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_8_frame_398.jpg\n",
      "Extracted and saved middle frame of scene 9 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_9_frame_450.jpg\n",
      "Extracted and saved middle frame of scene 10 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_10_frame_493.jpg\n",
      "Extracted and saved middle frame of scene 11 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_11_frame_548.jpg\n",
      "Extracted and saved middle frame of scene 12 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_12_frame_620.jpg\n",
      "Extracted and saved middle frame of scene 13 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_13_frame_694.jpg\n",
      "Extracted and saved middle frame of scene 14 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_14_frame_759.jpg\n",
      "Extracted and saved middle frame of scene 15 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_15_frame_824.jpg\n",
      "Extracted and saved middle frame of scene 16 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_16_frame_897.jpg\n",
      "Extracted and saved middle frame of scene 17 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_17_frame_947.jpg\n",
      "Extracted and saved middle frame of scene 18 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_18_frame_978.jpg\n",
      "Extracted and saved middle frame of scene 19 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_19_frame_1019.jpg\n",
      "Extracted and saved middle frame of scene 20 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_20_frame_1057.jpg\n",
      "Extracted and saved middle frame of scene 21 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_21_frame_1100.jpg\n",
      "Extracted and saved middle frame of scene 22 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_22_frame_1136.jpg\n",
      "Extracted and saved middle frame of scene 23 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_23_frame_1160.jpg\n",
      "Extracted and saved middle frame of scene 24 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_24_frame_1235.jpg\n",
      "Extracted and saved middle frame of scene 25 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_25_frame_1309.jpg\n",
      "Extracted and saved middle frame of scene 26 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_26_frame_1338.jpg\n",
      "Extracted and saved middle frame of scene 27 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_27_frame_1381.jpg\n",
      "Extracted and saved middle frame of scene 28 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_28_frame_1432.jpg\n",
      "Extracted and saved middle frame of scene 29 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_29_frame_1471.jpg\n",
      "Extracted and saved middle frame of scene 30 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_30_frame_1520.jpg\n",
      "Extracted and saved middle frame of scene 31 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_31_frame_1586.jpg\n",
      "Extracted and saved middle frame of scene 32 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_32_frame_1703.jpg\n",
      "Extracted and saved middle frame of scene 33 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_33_frame_1789.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_1_frame_7.jpg as scene_1_frame_7_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error decoding JSON from OpenAI response for /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_2_frame_63.jpg: Expecting property name enclosed in double quotes: line 103 column 40 (char 2854)\n",
      "Skipping scene_2_frame_63.jpg due to invalid OpenAI response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_6_frame_274.jpg as scene_6_frame_274_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error decoding JSON from OpenAI response for /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_5_frame_243.jpg: Expecting ',' delimiter: line 86 column 10 (char 2956)\n",
      "Skipping scene_5_frame_243.jpg due to invalid OpenAI response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_3_frame_152.jpg as scene_3_frame_152_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_7_frame_328.jpg as scene_7_frame_328_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_4_frame_210.jpg as scene_4_frame_210_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_9_frame_450.jpg as scene_9_frame_450_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_12_frame_620.jpg as scene_12_frame_620_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_11_frame_548.jpg as scene_11_frame_548_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_13_frame_694.jpg as scene_13_frame_694_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_8_frame_398.jpg as scene_8_frame_398_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_14_frame_759.jpg as scene_14_frame_759_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_15_frame_824.jpg as scene_15_frame_824_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_10_frame_493.jpg as scene_10_frame_493_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_16_frame_897.jpg as scene_16_frame_897_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_21_frame_1100.jpg as scene_21_frame_1100_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_17_frame_947.jpg as scene_17_frame_947_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_19_frame_1019.jpg as scene_19_frame_1019_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_23_frame_1160.jpg as scene_23_frame_1160_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_18_frame_978.jpg as scene_18_frame_978_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_24_frame_1235.jpg as scene_24_frame_1235_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_26_frame_1338.jpg as scene_26_frame_1338_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_25_frame_1309.jpg as scene_25_frame_1309_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_20_frame_1057.jpg as scene_20_frame_1057_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_28_frame_1432.jpg as scene_28_frame_1432_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_22_frame_1136.jpg as scene_22_frame_1136_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_30_frame_1520.jpg as scene_30_frame_1520_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error decoding JSON from OpenAI response for /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_29_frame_1471.jpg: Unterminated string starting at: line 95 column 26 (char 2865)\n",
      "Skipping scene_29_frame_1471.jpg due to invalid OpenAI response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_33_frame_1789.jpg as scene_33_frame_1789_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_31_frame_1586.jpg as scene_31_frame_1586_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_32_frame_1703.jpg as scene_32_frame_1703_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Scenes: 100%|██████████| 33/33 [01:12<00:00,  2.19s/scene]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_27_frame_1381.jpg as scene_27_frame_1381_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR:pyscenedetect:VideoManager is deprecated and will be removed.\n",
      "INFO:pyscenedetect:Loaded 1 video, framerate: 30.000 FPS, resolution: 442 x 360\n",
      "INFO:pyscenedetect:Detecting scenes...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged entities saved to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/final_summary.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Detected 33 scenes:\n",
      "INFO:root:Scene 1: Start 00:00:00.000 / Frame 0, End 00:00:00.500 / Frame 15\n",
      "INFO:root:Scene 2: Start 00:00:00.500 / Frame 15, End 00:00:03.733 / Frame 112\n",
      "INFO:root:Scene 3: Start 00:00:03.733 / Frame 112, End 00:00:06.400 / Frame 192\n",
      "INFO:root:Scene 4: Start 00:00:06.400 / Frame 192, End 00:00:07.600 / Frame 228\n",
      "INFO:root:Scene 5: Start 00:00:07.600 / Frame 228, End 00:00:08.600 / Frame 258\n",
      "INFO:root:Scene 6: Start 00:00:08.600 / Frame 258, End 00:00:09.700 / Frame 291\n",
      "INFO:root:Scene 7: Start 00:00:09.700 / Frame 291, End 00:00:12.200 / Frame 366\n",
      "INFO:root:Scene 8: Start 00:00:12.200 / Frame 366, End 00:00:14.333 / Frame 430\n",
      "INFO:root:Scene 9: Start 00:00:14.333 / Frame 430, End 00:00:15.667 / Frame 470\n",
      "INFO:root:Scene 10: Start 00:00:15.667 / Frame 470, End 00:00:17.200 / Frame 516\n",
      "INFO:root:Scene 11: Start 00:00:17.200 / Frame 516, End 00:00:19.367 / Frame 581\n",
      "INFO:root:Scene 12: Start 00:00:19.367 / Frame 581, End 00:00:21.967 / Frame 659\n",
      "INFO:root:Scene 13: Start 00:00:21.967 / Frame 659, End 00:00:24.300 / Frame 729\n",
      "INFO:root:Scene 14: Start 00:00:24.300 / Frame 729, End 00:00:26.300 / Frame 789\n",
      "INFO:root:Scene 15: Start 00:00:26.300 / Frame 789, End 00:00:28.667 / Frame 860\n",
      "INFO:root:Scene 16: Start 00:00:28.667 / Frame 860, End 00:00:31.167 / Frame 935\n",
      "INFO:root:Scene 17: Start 00:00:31.167 / Frame 935, End 00:00:31.967 / Frame 959\n",
      "INFO:root:Scene 18: Start 00:00:31.967 / Frame 959, End 00:00:33.267 / Frame 998\n",
      "INFO:root:Scene 19: Start 00:00:33.267 / Frame 998, End 00:00:34.667 / Frame 1040\n",
      "INFO:root:Scene 20: Start 00:00:34.667 / Frame 1040, End 00:00:35.800 / Frame 1074\n",
      "INFO:root:Scene 21: Start 00:00:35.800 / Frame 1074, End 00:00:37.567 / Frame 1127\n",
      "INFO:root:Scene 22: Start 00:00:37.567 / Frame 1127, End 00:00:38.167 / Frame 1145\n",
      "INFO:root:Scene 23: Start 00:00:38.167 / Frame 1145, End 00:00:39.167 / Frame 1175\n",
      "INFO:root:Scene 24: Start 00:00:39.167 / Frame 1175, End 00:00:43.167 / Frame 1295\n",
      "INFO:root:Scene 25: Start 00:00:43.167 / Frame 1295, End 00:00:44.100 / Frame 1323\n",
      "INFO:root:Scene 26: Start 00:00:44.100 / Frame 1323, End 00:00:45.100 / Frame 1353\n",
      "INFO:root:Scene 27: Start 00:00:45.100 / Frame 1353, End 00:00:47.000 / Frame 1410\n",
      "INFO:root:Scene 28: Start 00:00:47.000 / Frame 1410, End 00:00:48.467 / Frame 1454\n",
      "INFO:root:Scene 29: Start 00:00:48.467 / Frame 1454, End 00:00:49.600 / Frame 1488\n",
      "INFO:root:Scene 30: Start 00:00:49.600 / Frame 1488, End 00:00:51.767 / Frame 1553\n",
      "INFO:root:Scene 31: Start 00:00:51.767 / Frame 1553, End 00:00:53.967 / Frame 1619\n",
      "INFO:root:Scene 32: Start 00:00:53.967 / Frame 1619, End 00:00:59.600 / Frame 1788\n",
      "INFO:root:Scene 33: Start 00:00:59.600 / Frame 1788, End 00:00:59.667 / Frame 1790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total scenes detected: 33\n",
      "Analyzing scenes for flicker and short scene detection...\n",
      "Scene 1 to Scene 2: Last Luminance: 4.295493966817496, First Luminance: 5.206498240321769\n",
      "Luminance change for scene 1 to 2: 0.9110042735042736\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_1_to_2.jpg\n",
      "Scene 1 duration: 0.5 seconds\n",
      "Short scene detected in Scene 1\n",
      "Scene 2 to Scene 3: Last Luminance: 141.56720713926597, First Luminance: 109.7097787833082\n",
      "Luminance change for scene 2 to 3: 31.85742835595778\n",
      "Strong luminance change detected between Scene 2 and Scene 3\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_2_to_3_STRONG_LUMINANCE.jpg\n",
      "Scene 2 duration: 3.2333333333333334 seconds\n",
      "Scene 3 to Scene 4: Last Luminance: 186.15883609854197, First Luminance: 102.41040723981901\n",
      "Luminance change for scene 3 to 4: 83.74842885872296\n",
      "Strong luminance change detected between Scene 3 and Scene 4\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_3_to_4_STRONG_LUMINANCE.jpg\n",
      "Scene 3 duration: 2.6666666666666665 seconds\n",
      "Short scene detected in Scene 3\n",
      "Scene 4 to Scene 5: Last Luminance: 100.89718451483158, First Luminance: 100.67565359477125\n",
      "Luminance change for scene 4 to 5: 0.22153092006033148\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_4_to_5.jpg\n",
      "Scene 4 duration: 1.2 seconds\n",
      "Short scene detected in Scene 4\n",
      "Scene 5 to Scene 6: Last Luminance: 101.92118526897939, First Luminance: 129.11129964806435\n",
      "Luminance change for scene 5 to 6: 27.19011437908496\n",
      "Strong luminance change detected between Scene 5 and Scene 6\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_5_to_6_STRONG_LUMINANCE.jpg\n",
      "Scene 5 duration: 1.0 seconds\n",
      "Short scene detected in Scene 5\n",
      "Scene 6 to Scene 7: Last Luminance: 129.0060583207642, First Luminance: 127.15872926093515\n",
      "Luminance change for scene 6 to 7: 1.8473290598290504\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_6_to_7.jpg\n",
      "Scene 6 duration: 1.1 seconds\n",
      "Short scene detected in Scene 6\n",
      "Scene 7 to Scene 8: Last Luminance: 130.32708647561589, First Luminance: 142.1031171442936\n",
      "Luminance change for scene 7 to 8: 11.77603066867772\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_7_to_8.jpg\n",
      "Scene 7 duration: 2.5 seconds\n",
      "Short scene detected in Scene 7\n",
      "Scene 8 to Scene 9: Last Luminance: 153.09660633484162, First Luminance: 130.67861990950226\n",
      "Luminance change for scene 8 to 9: 22.417986425339365\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_8_to_9.jpg\n",
      "Scene 8 duration: 2.1333333333333333 seconds\n",
      "Short scene detected in Scene 8\n",
      "Scene 9 to Scene 10: Last Luminance: 142.3982277526395, First Luminance: 124.2300527903469\n",
      "Luminance change for scene 9 to 10: 18.16817496229261\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_9_to_10.jpg\n",
      "Scene 9 duration: 1.3333333333333333 seconds\n",
      "Short scene detected in Scene 9\n",
      "Scene 10 to Scene 11: Last Luminance: 141.40339994972348, First Luminance: 244.59868652589242\n",
      "Luminance change for scene 10 to 11: 103.19528657616894\n",
      "Strong luminance change detected between Scene 10 and Scene 11\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_10_to_11_STRONG_LUMINANCE.jpg\n",
      "Scene 10 duration: 1.5333333333333334 seconds\n",
      "Short scene detected in Scene 10\n",
      "Scene 11 to Scene 12: Last Luminance: 123.84864881850176, First Luminance: 126.65301030668678\n",
      "Luminance change for scene 11 to 12: 2.804361488185023\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_11_to_12.jpg\n",
      "Scene 11 duration: 2.1666666666666665 seconds\n",
      "Short scene detected in Scene 11\n",
      "Scene 12 to Scene 13: Last Luminance: 153.80728381096029, First Luminance: 178.7736676721971\n",
      "Luminance change for scene 12 to 13: 24.966383861236807\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_12_to_13.jpg\n",
      "Scene 12 duration: 2.6 seconds\n",
      "Short scene detected in Scene 12\n",
      "Scene 13 to Scene 14: Last Luminance: 162.53964303670185, First Luminance: 124.94590874811463\n",
      "Luminance change for scene 13 to 14: 37.593734288587214\n",
      "Strong luminance change detected between Scene 13 and Scene 14\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_13_to_14_STRONG_LUMINANCE.jpg\n",
      "Scene 13 duration: 2.3333333333333335 seconds\n",
      "Short scene detected in Scene 13\n",
      "Scene 14 to Scene 15: Last Luminance: 120.40805052790347, First Luminance: 119.92532679738562\n",
      "Luminance change for scene 14 to 15: 0.4827237305178471\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_14_to_15.jpg\n",
      "Scene 14 duration: 2.0 seconds\n",
      "Short scene detected in Scene 14\n",
      "Scene 15 to Scene 16: Last Luminance: 118.65208647561589, First Luminance: 134.8749308697838\n",
      "Luminance change for scene 15 to 16: 16.22284439416792\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_15_to_16.jpg\n",
      "Scene 15 duration: 2.3666666666666667 seconds\n",
      "Short scene detected in Scene 15\n",
      "Scene 16 to Scene 17: Last Luminance: 158.51898567119156, First Luminance: 95.45601432880845\n",
      "Luminance change for scene 16 to 17: 63.06297134238311\n",
      "Strong luminance change detected between Scene 16 and Scene 17\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_16_to_17_STRONG_LUMINANCE.jpg\n",
      "Scene 16 duration: 2.5 seconds\n",
      "Short scene detected in Scene 16\n",
      "Scene 17 to Scene 18: Last Luminance: 94.77059451985923, First Luminance: 114.66525263951735\n",
      "Luminance change for scene 17 to 18: 19.894658119658118\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_17_to_18.jpg\n",
      "Scene 17 duration: 0.8 seconds\n",
      "Short scene detected in Scene 17\n",
      "Scene 18 to Scene 19: Last Luminance: 121.72281297134238, First Luminance: 151.73871292106585\n",
      "Luminance change for scene 18 to 19: 30.015899949723476\n",
      "Strong luminance change detected between Scene 18 and Scene 19\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_18_to_19_STRONG_LUMINANCE.jpg\n",
      "Scene 18 duration: 1.3 seconds\n",
      "Short scene detected in Scene 18\n",
      "Scene 19 to Scene 20: Last Luminance: 153.40393413775766, First Luminance: 111.12910382101559\n",
      "Luminance change for scene 19 to 20: 42.27483031674207\n",
      "Strong luminance change detected between Scene 19 and Scene 20\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_19_to_20_STRONG_LUMINANCE.jpg\n",
      "Scene 19 duration: 1.4 seconds\n",
      "Short scene detected in Scene 19\n",
      "Scene 20 to Scene 21: Last Luminance: 112.05571266968326, First Luminance: 113.18805932629462\n",
      "Luminance change for scene 20 to 21: 1.132346656611361\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_20_to_21.jpg\n",
      "Scene 20 duration: 1.1333333333333333 seconds\n",
      "Short scene detected in Scene 20\n",
      "Scene 21 to Scene 22: Last Luminance: 113.43071895424836, First Luminance: 144.84001382604325\n",
      "Luminance change for scene 21 to 22: 31.409294871794884\n",
      "Strong luminance change detected between Scene 21 and Scene 22\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_21_to_22_STRONG_LUMINANCE.jpg\n",
      "Scene 21 duration: 1.7666666666666666 seconds\n",
      "Short scene detected in Scene 21\n",
      "Scene 22 to Scene 23: Last Luminance: 105.8453431372549, First Luminance: 77.56530920060332\n",
      "Luminance change for scene 22 to 23: 28.280033936651577\n",
      "Strong luminance change detected between Scene 22 and Scene 23\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_22_to_23_STRONG_LUMINANCE.jpg\n",
      "Scene 22 duration: 0.6 seconds\n",
      "Short scene detected in Scene 22\n",
      "Scene 23 to Scene 24: Last Luminance: 36.14315610859728, First Luminance: 84.05032051282052\n",
      "Luminance change for scene 23 to 24: 47.90716440422324\n",
      "Strong luminance change detected between Scene 23 and Scene 24\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_23_to_24_STRONG_LUMINANCE.jpg\n",
      "Scene 23 duration: 1.0 seconds\n",
      "Short scene detected in Scene 23\n",
      "Scene 24 to Scene 25: Last Luminance: 96.28558949220714, First Luminance: 44.83327048768225\n",
      "Luminance change for scene 24 to 25: 51.45231900452489\n",
      "Strong luminance change detected between Scene 24 and Scene 25\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_24_to_25_STRONG_LUMINANCE.jpg\n",
      "Scene 24 duration: 4.0 seconds\n",
      "Scene 25 to Scene 26: Last Luminance: 43.522203368526895, First Luminance: 147.48290598290598\n",
      "Luminance change for scene 25 to 26: 103.96070261437907\n",
      "Strong luminance change detected between Scene 25 and Scene 26\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_25_to_26_STRONG_LUMINANCE.jpg\n",
      "Scene 25 duration: 0.9333333333333333 seconds\n",
      "Short scene detected in Scene 25\n",
      "Scene 26 to Scene 27: Last Luminance: 148.29737933634993, First Luminance: 95.68776395173454\n",
      "Luminance change for scene 26 to 27: 52.609615384615395\n",
      "Strong luminance change detected between Scene 26 and Scene 27\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_26_to_27_STRONG_LUMINANCE.jpg\n",
      "Scene 26 duration: 1.0 seconds\n",
      "Short scene detected in Scene 26\n",
      "Scene 27 to Scene 28: Last Luminance: 97.19228883861237, First Luminance: 117.99717823026647\n",
      "Luminance change for scene 27 to 28: 20.8048893916541\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_27_to_28.jpg\n",
      "Scene 27 duration: 1.9 seconds\n",
      "Short scene detected in Scene 27\n",
      "Scene 28 to Scene 29: Last Luminance: 141.50967194570137, First Luminance: 127.47830568124685\n",
      "Luminance change for scene 28 to 29: 14.031366264454519\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_28_to_29.jpg\n",
      "Scene 28 duration: 1.4666666666666666 seconds\n",
      "Short scene detected in Scene 28\n",
      "Scene 29 to Scene 30: Last Luminance: 127.55165912518854, First Luminance: 140.6698152337858\n",
      "Luminance change for scene 29 to 30: 13.118156108597276\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_29_to_30.jpg\n",
      "Scene 29 duration: 1.1333333333333333 seconds\n",
      "Short scene detected in Scene 29\n",
      "Scene 30 to Scene 31: Last Luminance: 140.21667295123177, First Luminance: 144.3519356460533\n",
      "Luminance change for scene 30 to 31: 4.1352626948215345\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_30_to_31.jpg\n",
      "Scene 30 duration: 2.1666666666666665 seconds\n",
      "Short scene detected in Scene 30\n",
      "Scene 31 to Scene 32: Last Luminance: 150.63413147310206, First Luminance: 33.3557503770739\n",
      "Luminance change for scene 31 to 32: 117.27838109602816\n",
      "Strong luminance change detected between Scene 31 and Scene 32\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_31_to_32_STRONG_LUMINANCE.jpg\n",
      "Scene 31 duration: 2.2 seconds\n",
      "Short scene detected in Scene 31\n",
      "Scene 32 to Scene 33: Last Luminance: 4.625125691302162, First Luminance: 5.410036450477627\n",
      "Luminance change for scene 32 to 33: 0.7849107591754647\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/luminance_output/scene_32_to_33.jpg\n",
      "Scene 32 duration: 5.633333333333334 seconds\n",
      "Percentage of strong luminance transitions_30%: 46.875%\n",
      "Percentage of short scenes: 87.87878787878788%\n",
      "Flicker and short scene analysis results: {'percentage_strong_luminance_transitions': 46.875, 'percentage_short_scenes_3secs': 87.87878787878788, 'number_of_strong_luminance_transitions_30%': 15, 'number_of_short_scenes_3secs': 29}\n",
      "Video title: shinchantrim\n",
      "Video size: 3885860 bytes\n",
      "Video length: 59.7 seconds\n",
      "Number of scenes: 33\n",
      "Processing time: 124.43352389335632 seconds\n",
      "Analyzing scenes for flicker and short scene detection...\n",
      "Scene 1 to Scene 2: Last Luminance: 4.295493966817496, First Luminance: 5.206498240321769\n",
      "Luminance change for scene 1 to 2: 0.9110042735042736\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_1_to_2.jpg\n",
      "Scene 1 duration: 0.5 seconds\n",
      "Short scene detected in Scene 1\n",
      "Scene 2 to Scene 3: Last Luminance: 141.56720713926597, First Luminance: 109.7097787833082\n",
      "Luminance change for scene 2 to 3: 31.85742835595778\n",
      "Strong luminance change detected between Scene 2 and Scene 3\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_2_to_3_STRONG_LUMINANCE.jpg\n",
      "Scene 2 duration: 3.2333333333333334 seconds\n",
      "Scene 3 to Scene 4: Last Luminance: 186.15883609854197, First Luminance: 102.41040723981901\n",
      "Luminance change for scene 3 to 4: 83.74842885872296\n",
      "Strong luminance change detected between Scene 3 and Scene 4\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_3_to_4_STRONG_LUMINANCE.jpg\n",
      "Scene 3 duration: 2.6666666666666665 seconds\n",
      "Scene 4 to Scene 5: Last Luminance: 100.89718451483158, First Luminance: 100.67565359477125\n",
      "Luminance change for scene 4 to 5: 0.22153092006033148\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_4_to_5.jpg\n",
      "Scene 4 duration: 1.2 seconds\n",
      "Scene 5 to Scene 6: Last Luminance: 101.92118526897939, First Luminance: 129.11129964806435\n",
      "Luminance change for scene 5 to 6: 27.19011437908496\n",
      "Strong luminance change detected between Scene 5 and Scene 6\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_5_to_6_STRONG_LUMINANCE.jpg\n",
      "Scene 5 duration: 1.0 seconds\n",
      "Scene 6 to Scene 7: Last Luminance: 129.0060583207642, First Luminance: 127.15872926093515\n",
      "Luminance change for scene 6 to 7: 1.8473290598290504\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_6_to_7.jpg\n",
      "Scene 6 duration: 1.1 seconds\n",
      "Scene 7 to Scene 8: Last Luminance: 130.32708647561589, First Luminance: 142.1031171442936\n",
      "Luminance change for scene 7 to 8: 11.77603066867772\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_7_to_8.jpg\n",
      "Scene 7 duration: 2.5 seconds\n",
      "Scene 8 to Scene 9: Last Luminance: 153.09660633484162, First Luminance: 130.67861990950226\n",
      "Luminance change for scene 8 to 9: 22.417986425339365\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_8_to_9.jpg\n",
      "Scene 8 duration: 2.1333333333333333 seconds\n",
      "Scene 9 to Scene 10: Last Luminance: 142.3982277526395, First Luminance: 124.2300527903469\n",
      "Luminance change for scene 9 to 10: 18.16817496229261\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_9_to_10.jpg\n",
      "Scene 9 duration: 1.3333333333333333 seconds\n",
      "Scene 10 to Scene 11: Last Luminance: 141.40339994972348, First Luminance: 244.59868652589242\n",
      "Luminance change for scene 10 to 11: 103.19528657616894\n",
      "Strong luminance change detected between Scene 10 and Scene 11\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_10_to_11_STRONG_LUMINANCE.jpg\n",
      "Scene 10 duration: 1.5333333333333334 seconds\n",
      "Scene 11 to Scene 12: Last Luminance: 123.84864881850176, First Luminance: 126.65301030668678\n",
      "Luminance change for scene 11 to 12: 2.804361488185023\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_11_to_12.jpg\n",
      "Scene 11 duration: 2.1666666666666665 seconds\n",
      "Scene 12 to Scene 13: Last Luminance: 153.80728381096029, First Luminance: 178.7736676721971\n",
      "Luminance change for scene 12 to 13: 24.966383861236807\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_12_to_13.jpg\n",
      "Scene 12 duration: 2.6 seconds\n",
      "Scene 13 to Scene 14: Last Luminance: 162.53964303670185, First Luminance: 124.94590874811463\n",
      "Luminance change for scene 13 to 14: 37.593734288587214\n",
      "Strong luminance change detected between Scene 13 and Scene 14\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_13_to_14_STRONG_LUMINANCE.jpg\n",
      "Scene 13 duration: 2.3333333333333335 seconds\n",
      "Scene 14 to Scene 15: Last Luminance: 120.40805052790347, First Luminance: 119.92532679738562\n",
      "Luminance change for scene 14 to 15: 0.4827237305178471\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_14_to_15.jpg\n",
      "Scene 14 duration: 2.0 seconds\n",
      "Scene 15 to Scene 16: Last Luminance: 118.65208647561589, First Luminance: 134.8749308697838\n",
      "Luminance change for scene 15 to 16: 16.22284439416792\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_15_to_16.jpg\n",
      "Scene 15 duration: 2.3666666666666667 seconds\n",
      "Scene 16 to Scene 17: Last Luminance: 158.51898567119156, First Luminance: 95.45601432880845\n",
      "Luminance change for scene 16 to 17: 63.06297134238311\n",
      "Strong luminance change detected between Scene 16 and Scene 17\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_16_to_17_STRONG_LUMINANCE.jpg\n",
      "Scene 16 duration: 2.5 seconds\n",
      "Scene 17 to Scene 18: Last Luminance: 94.77059451985923, First Luminance: 114.66525263951735\n",
      "Luminance change for scene 17 to 18: 19.894658119658118\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_17_to_18.jpg\n",
      "Scene 17 duration: 0.8 seconds\n",
      "Short scene detected in Scene 17\n",
      "Scene 18 to Scene 19: Last Luminance: 121.72281297134238, First Luminance: 151.73871292106585\n",
      "Luminance change for scene 18 to 19: 30.015899949723476\n",
      "Strong luminance change detected between Scene 18 and Scene 19\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_18_to_19_STRONG_LUMINANCE.jpg\n",
      "Scene 18 duration: 1.3 seconds\n",
      "Scene 19 to Scene 20: Last Luminance: 153.40393413775766, First Luminance: 111.12910382101559\n",
      "Luminance change for scene 19 to 20: 42.27483031674207\n",
      "Strong luminance change detected between Scene 19 and Scene 20\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_19_to_20_STRONG_LUMINANCE.jpg\n",
      "Scene 19 duration: 1.4 seconds\n",
      "Scene 20 to Scene 21: Last Luminance: 112.05571266968326, First Luminance: 113.18805932629462\n",
      "Luminance change for scene 20 to 21: 1.132346656611361\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_20_to_21.jpg\n",
      "Scene 20 duration: 1.1333333333333333 seconds\n",
      "Scene 21 to Scene 22: Last Luminance: 113.43071895424836, First Luminance: 144.84001382604325\n",
      "Luminance change for scene 21 to 22: 31.409294871794884\n",
      "Strong luminance change detected between Scene 21 and Scene 22\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_21_to_22_STRONG_LUMINANCE.jpg\n",
      "Scene 21 duration: 1.7666666666666666 seconds\n",
      "Scene 22 to Scene 23: Last Luminance: 105.8453431372549, First Luminance: 77.56530920060332\n",
      "Luminance change for scene 22 to 23: 28.280033936651577\n",
      "Strong luminance change detected between Scene 22 and Scene 23\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_22_to_23_STRONG_LUMINANCE.jpg\n",
      "Scene 22 duration: 0.6 seconds\n",
      "Short scene detected in Scene 22\n",
      "Scene 23 to Scene 24: Last Luminance: 36.14315610859728, First Luminance: 84.05032051282052\n",
      "Luminance change for scene 23 to 24: 47.90716440422324\n",
      "Strong luminance change detected between Scene 23 and Scene 24\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_23_to_24_STRONG_LUMINANCE.jpg\n",
      "Scene 23 duration: 1.0 seconds\n",
      "Scene 24 to Scene 25: Last Luminance: 96.28558949220714, First Luminance: 44.83327048768225\n",
      "Luminance change for scene 24 to 25: 51.45231900452489\n",
      "Strong luminance change detected between Scene 24 and Scene 25\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_24_to_25_STRONG_LUMINANCE.jpg\n",
      "Scene 24 duration: 4.0 seconds\n",
      "Scene 25 to Scene 26: Last Luminance: 43.522203368526895, First Luminance: 147.48290598290598\n",
      "Luminance change for scene 25 to 26: 103.96070261437907\n",
      "Strong luminance change detected between Scene 25 and Scene 26\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_25_to_26_STRONG_LUMINANCE.jpg\n",
      "Scene 25 duration: 0.9333333333333333 seconds\n",
      "Short scene detected in Scene 25\n",
      "Scene 26 to Scene 27: Last Luminance: 148.29737933634993, First Luminance: 95.68776395173454\n",
      "Luminance change for scene 26 to 27: 52.609615384615395\n",
      "Strong luminance change detected between Scene 26 and Scene 27\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_26_to_27_STRONG_LUMINANCE.jpg\n",
      "Scene 26 duration: 1.0 seconds\n",
      "Scene 27 to Scene 28: Last Luminance: 97.19228883861237, First Luminance: 117.99717823026647\n",
      "Luminance change for scene 27 to 28: 20.8048893916541\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_27_to_28.jpg\n",
      "Scene 27 duration: 1.9 seconds\n",
      "Scene 28 to Scene 29: Last Luminance: 141.50967194570137, First Luminance: 127.47830568124685\n",
      "Luminance change for scene 28 to 29: 14.031366264454519\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_28_to_29.jpg\n",
      "Scene 28 duration: 1.4666666666666666 seconds\n",
      "Scene 29 to Scene 30: Last Luminance: 127.55165912518854, First Luminance: 140.6698152337858\n",
      "Luminance change for scene 29 to 30: 13.118156108597276\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_29_to_30.jpg\n",
      "Scene 29 duration: 1.1333333333333333 seconds\n",
      "Scene 30 to Scene 31: Last Luminance: 140.21667295123177, First Luminance: 144.3519356460533\n",
      "Luminance change for scene 30 to 31: 4.1352626948215345\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_30_to_31.jpg\n",
      "Scene 30 duration: 2.1666666666666665 seconds\n",
      "Scene 31 to Scene 32: Last Luminance: 150.63413147310206, First Luminance: 33.3557503770739\n",
      "Luminance change for scene 31 to 32: 117.27838109602816\n",
      "Strong luminance change detected between Scene 31 and Scene 32\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_31_to_32_STRONG_LUMINANCE.jpg\n",
      "Scene 31 duration: 2.2 seconds\n",
      "Scene 32 to Scene 33: Last Luminance: 4.625125691302162, First Luminance: 5.410036450477627\n",
      "Luminance change for scene 32 to 33: 0.7849107591754647\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_32_to_33.jpg\n",
      "Scene 32 duration: 5.633333333333334 seconds\n",
      "Percentage of strong luminance transitions_30%: 46.875%\n",
      "Percentage of short scenes: 12.121212121212121%\n",
      "Luminance and short scene analysis results: {'percentage_strong_luminance_transitions': 46.875, 'percentage_short_scenes_3secs': 12.121212121212121, 'number_of_strong_luminance_transitions_30%': 15, 'number_of_short_scenes_3secs': 4}\n",
      "Additional stats saved to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/shinchantrim_stats.json\n",
      "Finished processing video: shinchantrim.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Videos: 100%|██████████| 1/1 [02:06<00:00, 126.12s/video]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED PROCESSING ALL VIDEOS.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "\n",
    "async def process_video(video_file, directory_path, output_base_dir, api_key):\n",
    "    try:\n",
    "        # Track the start time\n",
    "        start_time = time.time()\n",
    "\n",
    "        video_path = os.path.join(directory_path, video_file)\n",
    "        video_name = os.path.splitext(video_file)[0]\n",
    "        video_size = os.path.getsize(video_path)\n",
    "\n",
    "        video_output_dir = os.path.join(output_base_dir, video_name)\n",
    "        scenes_output_dir = os.path.join(video_output_dir, 'scenes_output')\n",
    "        json_output_dir = os.path.join(video_output_dir, 'json_output')\n",
    "        luminance_output_dir = os.path.join(video_output_dir, 'luminance_output')  # Create a folder for luminance output\n",
    "\n",
    "        os.makedirs(scenes_output_dir, exist_ok=True)\n",
    "        os.makedirs(json_output_dir, exist_ok=True)\n",
    "        os.makedirs(luminance_output_dir, exist_ok=True)  # Ensure the folder for luminance is created\n",
    "\n",
    "        print(f\"Processing video: {video_file}\", flush=True)\n",
    "\n",
    "        # Analyze video for scenes\n",
    "        scenes = analyze_video(video_path)\n",
    "        extract_frames_imageio(video_path, scenes, scenes_output_dir)\n",
    "        await process_scenes_output(scenes_output_dir, json_output_dir)  # Run async scene processing\n",
    "\n",
    "        final_json_path = os.path.join(json_output_dir, 'final_entities.json')\n",
    "        \n",
    "        # Await cluster_and_merge_entities and assign returned value to merged_final_entities\n",
    "        merged_final_entities = await cluster_and_merge_entities(api_key, json_output_dir, video_output_dir)\n",
    "\n",
    "        if merged_final_entities is None:\n",
    "            print(f\"Error: Failed to merge entities for video {video_file}\")\n",
    "            return\n",
    "\n",
    "        # Perform luminance analysis and save the images\n",
    "        process_video_with_flicker_analysis(video_path, luminance_output_dir)\n",
    "\n",
    "        # Compute summary statistics\n",
    "        summary_statistics = compute_summary_statistics(merged_final_entities)\n",
    "\n",
    "        # Track the end time\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Update this line in process_video:\n",
    "        save_additional_stats(video_path, scenes_output_dir, start_time, end_time, video_output_dir, json_output_dir, summary_statistics, scenes)\n",
    "\n",
    "\n",
    "        print(f\"Finished processing video: {video_file}\", flush=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video {video_file}: {e}\", flush=True)\n",
    "        \n",
    "async def process_videos_in_directory(directory_path, output_base_dir, api_key):\n",
    "    video_files = [f for f in os.listdir(directory_path) if f.endswith(('.mp4', '.avi', '.mkv'))]\n",
    "\n",
    "    if not video_files:\n",
    "        print(\"No video files found in the directory.\", flush=True)\n",
    "        return\n",
    "\n",
    "    with tqdm(total=len(video_files), desc=\"Processing Videos\", unit=\"video\") as pbar:\n",
    "        for video_file in video_files:\n",
    "            await process_video(video_file, directory_path, output_base_dir, api_key)\n",
    "            pbar.update(1)\n",
    "\n",
    "# Ensure the main script has appropriate paths\n",
    "video_directory = '/Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/02_Video_DB'\n",
    "output_base_directory = '/Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB'\n",
    "\n",
    "# Run the async function (sequential processing)\n",
    "asyncio.run(process_videos_in_directory(video_directory, output_base_directory, api_key))\n",
    "print(\"FINISHED PROCESSING ALL VIDEOS.\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
