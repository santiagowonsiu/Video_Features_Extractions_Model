{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to analyse the whole database of videos and process it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import json\n",
    "import imageio\n",
    "import re\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import colorsys\n",
    "import aiofiles\n",
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from scenedetect import VideoManager, SceneManager\n",
    "from scenedetect.detectors import ContentDetector\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import csv\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load OpenAI API key from .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Apply nest_asyncio to handle the running event loop\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Concurrency limit\n",
    "semaphore = asyncio.Semaphore(5)\n",
    "\n",
    "# A dictionary to store characters across frames\n",
    "character_frames = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRACK API USAGE CALLS\n",
    "\n",
    "# Initialize API usage tracking\n",
    "api_usage = {\n",
    "    \"total_api_calls\": 0,\n",
    "    \"total_tokens_used\": 0,\n",
    "    \"model_used\": \"gpt-4\"  # Assuming you're using GPT-4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Video Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_video(video_path, threshold=27.0):\n",
    "    if not os.path.exists(video_path):\n",
    "        raise FileNotFoundError(f\"The video file {video_path} does not exist.\")\n",
    "    \n",
    "    video_manager = VideoManager([video_path])\n",
    "    scene_manager = SceneManager()\n",
    "    scene_manager.add_detector(ContentDetector(threshold=threshold))\n",
    "\n",
    "    video_manager.set_downscale_factor()\n",
    "    video_manager.start()\n",
    "\n",
    "    scene_manager.detect_scenes(frame_source=video_manager)\n",
    "    scene_list = scene_manager.get_scene_list()\n",
    "\n",
    "    video_manager.release()\n",
    "\n",
    "    logging.info(f'Detected {len(scene_list)} scenes:')\n",
    "    for i, scene in enumerate(scene_list):\n",
    "        logging.info(f'Scene {i + 1}: Start {scene[0].get_timecode()} / Frame {scene[0].get_frames()}, '\n",
    "              f'End {scene[1].get_timecode()} / Frame {scene[1].get_frames()}')\n",
    "\n",
    "    return scene_list\n",
    "\n",
    "def get_video_length(video_path):\n",
    "    # You can use a tool like OpenCV, ffmpeg, or similar to calculate video length\n",
    "    import cv2\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_length = frame_count / fps\n",
    "    cap.release()\n",
    "    return video_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Frame Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames_imageio(video_path, scenes, output_dir):\n",
    "    reader = imageio.get_reader(video_path)\n",
    "    for i, scene in enumerate(scenes):\n",
    "        start_frame, end_frame = scene\n",
    "        \n",
    "        # Convert FrameTimecode to integer frame numbers\n",
    "        start_frame_num = int(start_frame)\n",
    "        end_frame_num = int(end_frame)\n",
    "        \n",
    "        # Calculate the middle frame of the scene\n",
    "        middle_frame = (start_frame_num + end_frame_num) // 2\n",
    "        \n",
    "        # Set the reader to the middle frame and extract it\n",
    "        reader.set_image_index(middle_frame)\n",
    "        frame = reader.get_next_data()\n",
    "        \n",
    "        # Save the frame as an image with frame number in the filename\n",
    "        output_path = os.path.join(output_dir, f'scene_{i + 1}_frame_{middle_frame}.jpg')\n",
    "        imageio.imwrite(output_path, frame)\n",
    "        print(f\"Extracted and saved middle frame of scene {i + 1} as {output_path}\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Image Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def encode_image(image_path):\n",
    "    async with aiofiles.open(image_path, \"rb\") as image_file:\n",
    "        content = await image_file.read()\n",
    "        return base64.b64encode(content).decode('utf-8')\n",
    "\n",
    "def get_color_category(color):\n",
    "    r, g, b = [x / 255.0 for x in color]\n",
    "    h, l, s = colorsys.rgb_to_hls(r, g, b)\n",
    "\n",
    "    primary_hues = {\n",
    "        \"red\": (0.0, 0.1),  \n",
    "        \"yellow\": (0.1, 0.18),\n",
    "        \"green\": (0.25, 0.4),\n",
    "        \"blue\": (0.55, 0.75),\n",
    "    }\n",
    "\n",
    "    for color_name, hue_range in primary_hues.items():\n",
    "        if hue_range[0] <= h <= hue_range[1]:\n",
    "            return color_name\n",
    "\n",
    "    if (l >= 0.9 and s <= 0.1):\n",
    "        return \"white\"\n",
    "    if (l <= 0.1 and s <= 0.1):\n",
    "        return \"black\"\n",
    "\n",
    "    return \"non-primary\"\n",
    "\n",
    "def analyze_image_colors(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert('RGB')\n",
    "    data = np.array(image)\n",
    "\n",
    "    unique_colors, counts = np.unique(data.reshape(-1, data.shape[2]), axis=0, return_counts=True)\n",
    "    total_pixels = int(counts.sum())\n",
    "\n",
    "    color_counts = {\n",
    "        \"Red\": 0,\n",
    "        \"Yellow\": 0,\n",
    "        \"Green\": 0,\n",
    "        \"Blue\": 0,\n",
    "        \"White\": 0,\n",
    "        \"Black\": 0,\n",
    "        \"Non-primary\": 0\n",
    "    }\n",
    "\n",
    "    for color, count in zip(unique_colors, counts):\n",
    "        category = get_color_category(tuple(color))\n",
    "        color_counts[category.capitalize()] += int(count)\n",
    "\n",
    "    color_percentages = {color: (count / total_pixels) * 100 for color, count in color_counts.items()}\n",
    "    primary_total = color_counts[\"Red\"] + color_counts[\"Yellow\"] + color_counts[\"Blue\"]\n",
    "    color_dominance = \"Primary colors\" if primary_total > color_counts[\"Non-primary\"] else \"Non-primary colors\"\n",
    "\n",
    "    return {\n",
    "        \"Color Analysis\": {\n",
    "            \"Colors Found\": {\n",
    "                color: {\n",
    "                    \"Pixel Count\": count,\n",
    "                    \"Percentage\": f\"{color_percentages[color]:.2f}%\"\n",
    "                } for color, count in color_counts.items()\n",
    "            },\n",
    "            \"Dominance\": color_dominance\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) OpenAI API Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def send_image_to_openai(image_path, base64_image, retries=3):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"\"\"\n",
    "                        Analyze the following image and provide a detailed description in the format of JSON only. Ensure the output is strictly in JSON format without any additional text or code block formatting. The JSON should include the following standardized labels:\n",
    "\n",
    "                        1. **Image Analysis**: The root dictionary containing all analysis data.\n",
    "                        \n",
    "                        2. **Suitability**:\n",
    "                            - \"Nudity\": Boolean indicating the presence of nudity.\n",
    "                            - \"Obscene Gestures\": Boolean indicating the presence of obscene gestures.\n",
    "                            - \"Alcohol\": Boolean indicating the presence of alcohol.\n",
    "                            - \"Drugs\": Boolean indicating the presence of drugs.\n",
    "                            - \"Addictions\": Boolean indicating the presence of addictions.\n",
    "\n",
    "                        3. **Objects**:\n",
    "                            - \"Total Objects Identified\": Integer representing the total number of objects identified.\n",
    "                            - \"Average Features Per Object\": Float representing the average number of features per object.\n",
    "                            - \"Objects Details\": Dictionary containing details of each object, where each object is labeled as \"Object_1\", \"Object_2\", etc., with the following structure:\n",
    "                                - \"Name\": The name of the object - as simplest and descriptive mossible.\n",
    "                                - \"Portion Boolean\": 0-1 output indicating if the object is a portion of a larger object (1) or a complete object (0). For example, a leg is a portion of a human. However, if the object is just cropped but clearly identifiable as a complete object, it should be considered a complete object.\n",
    "                                - \"Color\": The color of the object.\n",
    "                                - \"Features\": List of features of the object.\n",
    "                                - \"Total Features\": Integer representing the number of features for the object.\n",
    "\n",
    "                        4. **Place**:\n",
    "                            - \"Name\": The name of the place - as simplest and descriptive mossible.\n",
    "                            - \"Certainty Boolean\": 0-1 output indicating if the place is clearly identifiable (1) or not (0).\n",
    "                            - \"Fantasy/Adventurous Place\": Boolean (0-1) indicating whether the place is classified as a fantasy/adventurous place or not.\n",
    "                            - \"Explanation\": Detailed explanation of why the place is classified as fantasy/adventurous or not. Fantasy places are those that do not exist in reality, and adventurous places are defined as those involving clear statements of traveling to space or another country.\n",
    "\n",
    "                        5. **Characters**:\n",
    "                            - \"Total Characters Identified\": Integer representing the total number of characters identified.\n",
    "                            - \"Average Features Per Character\": Float representing the average number of features per character.\n",
    "                            - \"Character Details\": Dictionary containing details of each character, where each character is labeled as \"Character_1\", \"Character_2\", etc., with the following structure:\n",
    "                                - \"Name\": The name of the character - as simplest and descriptive mossible.\n",
    "                                - \"Portion Boolean\": 0-1 output indicating if the character is a portion of a larger character (1) or a complete character (0). For example, a leg is a portion of a human. However, if the character is just cropped but clearly identifiable as a complete character, it should be considered a complete character.\n",
    "                                - \"Human or Non-Human\": 0-1 output indicating if the character appears human (1) or non-human (0). Anthropomorphized characters or any other combination not fully human are considered non-human.\n",
    "                                - \"Physical Features\": List of physical features of the character.\n",
    "                                - \"Explanation\": Explanation for why the character is classified as human or non-human, and why these physical features are inferred.\n",
    "                                - \"Age\": Expected age range of the character (a single number).\n",
    "                            **Note**: If the \"character\" consists of only a part of a body (such as a hand, leg, or face without enough distinguishing features to identify it as a complete character), do not count it as a \"character.\"\n",
    "\n",
    "                        Ensure that the structure of the JSON output strictly adheres to these standardized labels.\n",
    "                        \"\"\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 750\n",
    "    }\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload) as response:\n",
    "                    # Log the status code and full response for debugging\n",
    "                    status = response.status\n",
    "                    response_text = await response.text()\n",
    "                    \n",
    "                    # print(f\"Response Status Code: {status}\")\n",
    "                    # print(f\"Response Content: {response_text}\")\n",
    "\n",
    "                    if status == 429:\n",
    "                        print(\"Rate limit exceeded, retrying...\")\n",
    "                        await asyncio.sleep(2 ** attempt)\n",
    "                        continue\n",
    "                    elif status == 200:\n",
    "                        content = await response.json()\n",
    "                        \n",
    "                        # Log the full JSON content\n",
    "                        # print(f\"Full JSON Response for {image_path}: {content}\")\n",
    "                        \n",
    "                        if 'choices' in content:\n",
    "                            message_content = content['choices'][0].get('message', {}).get('content', '').strip()\n",
    "                            try:\n",
    "                                return json.loads(message_content)\n",
    "                            except json.JSONDecodeError as e:\n",
    "                                print(f\"Error decoding JSON from OpenAI response for {image_path}: {e}\")\n",
    "                                # print(f\"OpenAI Response Content: {message_content}\")\n",
    "                                return None\n",
    "                        else:\n",
    "                            print(f\"Unexpected response format from OpenAI API for {image_path}.\")\n",
    "                            return None\n",
    "                    else:\n",
    "                        print(f\"Request failed with status code {status} for {image_path}.\")\n",
    "                        # print(f\"Response Content: {response_text}\")\n",
    "                        return None\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Request failed due to a client error: {e}\")\n",
    "            await asyncio.sleep(2 ** attempt)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error occurred: {e}\")\n",
    "            await asyncio.sleep(2 ** attempt)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Scene Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_scenes_output(output_dir, json_output_dir):\n",
    "    os.makedirs(json_output_dir, exist_ok=True)\n",
    "    scenes = sorted([f for f in os.listdir(output_dir) if f.endswith('.jpg')], key=extract_scene_number)\n",
    "    total_scenes = len(scenes)\n",
    "    with tqdm(total=total_scenes, desc=\"Processing Scenes\", unit=\"scene\") as pbar:\n",
    "        tasks = [process_single_scene(i, scene, output_dir, json_output_dir, pbar) for i, scene in enumerate(scenes)]\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "async def process_single_scene(i, scene, output_dir, json_output_dir, pbar):\n",
    "    async with semaphore:  # Limit concurrent execution\n",
    "        scene_path = os.path.join(output_dir, scene)\n",
    "\n",
    "        # Encode image in base64\n",
    "        base64_image = await encode_image(scene_path)\n",
    "\n",
    "        # Perform color analysis\n",
    "        color_analysis_result = analyze_image_colors(scene_path)\n",
    "\n",
    "        # Send image to OpenAI for further analysis\n",
    "        openai_response = await send_image_to_openai(scene_path, base64_image)\n",
    "\n",
    "        # Check if openai_response is valid (not None or empty)\n",
    "        if not openai_response:\n",
    "            print(f\"Skipping {scene} due to invalid OpenAI response.\")\n",
    "            pbar.update(1)\n",
    "            return\n",
    "\n",
    "        # Combine both results, and include the reference to the image file\n",
    "        final_output = {\n",
    "            \"Image File\": scene,\n",
    "            \"Image Analysis\": {\n",
    "                **color_analysis_result[\"Color Analysis\"],\n",
    "                **openai_response.get(\"Image Analysis\", {})\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # The filename already includes the scene number and frame number\n",
    "        output_filename = os.path.splitext(scene)[0] + '_analysis.json'\n",
    "        output_path = os.path.join(json_output_dir, output_filename)\n",
    "\n",
    "        try:\n",
    "            async with aiofiles.open(output_path, 'w') as json_file:\n",
    "                await json_file.write(json.dumps(final_output, indent=4))\n",
    "                print(f\"Saved analysis for {scene} as {output_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save analysis for {scene}: {e}\")\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "def extract_scene_number(filename):\n",
    "    match = re.search(r'\\d+', filename)\n",
    "    return int(match.group()) if match else -1\n",
    "\n",
    "def extract_frame_number(filename):\n",
    "    match = re.search(r'_frame_(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Run whole analysis of each json output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Path Construction: get_image_path generates the correct path to the image file based on the JSON filename.\n",
    "\n",
    "Entity Extraction:extract_entities_from_json pulls characters, objects, and places from the JSON data.\n",
    "\n",
    "Image-to-Image Comparison:perform_image_to_image_comparison compares partial objects with full objects using the OpenAI API.\n",
    "\n",
    "Entity Comparison:compare_entities handles both name-based and image-based comparisons to decide whether two entities should be consolidated.\n",
    "\n",
    "Consolidation:Entities across frames are consolidated into a single summary file that tracks where each entity was found.\n",
    "\n",
    "Main Execution:The script runs through all JSON files, processes the entities, and saves the consolidated results to a summary JSON file.\n",
    "\n",
    "Key Features of This Implementation:\n",
    "Text-Based Comparison: The code first attempts to merge entities based on exact name matches. If no match is found, it uses the OpenAI API to determine if two entities with different names should be merged.\n",
    "\n",
    "Image-to-Image Comparison: If one of the entities is flagged as a portion, or if names don't match but the entities might still be the same, the code performs an image-to-image comparison using the OpenAI API.\n",
    "\n",
    "Efficient Processing: The code processes each frame sequentially and logs all merges into merged_entities_log, ensuring you have a record of what entities were merged, including their original names and frames.\n",
    "\n",
    "No Overwritten Functionality: The original image analysis functionality is preserved and integrated smoothly with the text-based comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Unique Characters, Objects, and Places: This can be done by counting the keys in the consolidated_data dictionary.\n",
    "Average Characters per Frame: This can be calculated by summing up all instances of characters found across frames and dividing by the total number of frames where characters appear.\n",
    "Average Features per Character/Object: Calculate this by summing the features of all characters/objects and dividing by the total number of characters/objects.\n",
    "Overall Color Analysis: Aggregate the color data from all JSON files.\n",
    "Filter Compliance: Check for any instances where the filters (e.g., nudity, drugs) are not compliant and log the frame numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import openai\n",
    "\n",
    "# Apply nest_asyncio to handle the running event loop\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize OpenAI API\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OpenAI API key is not set.\")\n",
    "openai.api_key = api_key\n",
    "\n",
    "#Initial consolidation of entities\n",
    "def initial_consolidation(json_files):\n",
    "    consolidated_data = {\"characters\": {}, \"objects\": {}, \"places\": {}}\n",
    "    merge_tracking = {\"characters\": {}, \"objects\": {}, \"places\": {}}\n",
    "\n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            consolidate_from_json(json_data, consolidated_data, merge_tracking, os.path.basename(json_file))\n",
    "\n",
    "    return consolidated_data\n",
    "\n",
    "\n",
    "# Function to save entities to JSON\n",
    "def save_entities_to_json(entities, path):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(entities, f, indent=4)\n",
    "\n",
    "# Function to consolidate entities from a JSON file\n",
    "# Function to consolidate entities from a JSON file\n",
    "def consolidate_from_json(json_data, consolidated_data, merge_tracking, json_file_name):\n",
    "    frame_number = json_file_name.split('_')[3]  # Extract frame number\n",
    "\n",
    "    if \"Image Analysis\" in json_data:\n",
    "        characters = json_data[\"Image Analysis\"].get(\"Characters\", {}).get(\"Character Details\", {})\n",
    "        for key, details in characters.items():\n",
    "            name = details.get(\"Name\")\n",
    "            if name:\n",
    "                if name not in consolidated_data[\"characters\"]:\n",
    "                    consolidated_data[\"characters\"][name] = details\n",
    "                    consolidated_data[\"characters\"][name][\"merged_from\"] = []\n",
    "                    merge_tracking[\"characters\"][name] = {\"merged_from\": []}\n",
    "                if frame_number not in consolidated_data[\"characters\"][name][\"merged_from\"]:\n",
    "                    consolidated_data[\"characters\"][name][\"merged_from\"].append(frame_number)\n",
    "                    merge_tracking[\"characters\"][name][\"merged_from\"].append(frame_number)\n",
    "\n",
    "        objects = json_data[\"Image Analysis\"].get(\"Objects\", {}).get(\"Objects Details\", {})\n",
    "        for key, details in objects.items():\n",
    "            name = details.get(\"Name\")\n",
    "            if name:\n",
    "                if name not in consolidated_data[\"objects\"]:\n",
    "                    consolidated_data[\"objects\"][name] = details\n",
    "                    consolidated_data[\"objects\"][name][\"merged_from\"] = []\n",
    "                    merge_tracking[\"objects\"][name] = {\"merged_from\": []}\n",
    "                if frame_number not in consolidated_data[\"objects\"][name][\"merged_from\"]:\n",
    "                    consolidated_data[\"objects\"][name][\"merged_from\"].append(frame_number)\n",
    "                    merge_tracking[\"objects\"][name][\"merged_from\"].append(frame_number)\n",
    "\n",
    "        place = json_data[\"Image Analysis\"].get(\"Place\", {})\n",
    "        place_name = place.get(\"Name\")\n",
    "        if place_name:\n",
    "            if place_name not in consolidated_data[\"places\"]:\n",
    "                consolidated_data[\"places\"][place_name] = place\n",
    "                consolidated_data[\"places\"][place_name][\"merged_from\"] = []\n",
    "                merge_tracking[\"places\"][place_name] = {\"merged_from\": []}\n",
    "            if frame_number not in consolidated_data[\"places\"][place_name][\"merged_from\"]:\n",
    "                consolidated_data[\"places\"][place_name][\"merged_from\"].append(frame_number)\n",
    "                merge_tracking[\"places\"][place_name][\"merged_from\"].append(frame_number)\n",
    "\n",
    "\n",
    "# Function to cluster entities using OpenAI API and name the clusters\n",
    "async def cluster_entities(api_key, entities):\n",
    "    # Generate lists for characters, objects, and places from entities\n",
    "    character_list = ', '.join(entities['characters'].keys())\n",
    "    object_list = ', '.join(entities['objects'].keys())\n",
    "    place_list = ', '.join(entities['places'].keys())\n",
    "\n",
    "    if not character_list and not object_list and not place_list:\n",
    "        return \"No entities available to cluster.\"\n",
    "\n",
    "    # Adjusted OpenAI prompt to return structured output (dictionaries)\n",
    "    prompt = f\"\"\"\n",
    "    You are tasked with clustering and naming entities from a TV show. Below are lists of characters, objects, and places extracted from different scenes. These lists sometimes contain multiple labels for the same entity.\n",
    "\n",
    "    **Instructions:**\n",
    "\n",
    "    1. Group the characters, objects, and places that refer to the same entity and suggest a single **final name** for each group (be smart what could be the same individual/object in the show one and what couldn't).\n",
    "    2. Return the result as a dictionary where each cluster (key) contains the entities (values) that belong to that cluster.\n",
    "    3. Use this format:\n",
    "\n",
    "    {{\n",
    "      \"Characters Clusters\": {{\n",
    "        \"Final Name 1\": [\"Character 1\", \"Character 2\", ...],\n",
    "        \"Final Name 2\": [\"Character 3\", \"Character 4\", ...]\n",
    "      }},\n",
    "      \"Objects Clusters\": {{\n",
    "        \"Final Name 1\": [\"Object 1\", \"Object 2\", ...],\n",
    "        \"Final Name 2\": [\"Object 3\", \"Object 4\", ...]\n",
    "      }},\n",
    "      \"Places Clusters\": {{\n",
    "        \"Final Name 1\": [\"Place 1\", \"Place 2\", ...],\n",
    "        \"Final Name 2\": [\"Place 3\", \"Place 4\", ...]\n",
    "      }}\n",
    "    }}\n",
    "\n",
    "    **Characters:**\n",
    "    {character_list}\n",
    "\n",
    "    **Objects:**\n",
    "    {object_list}\n",
    "\n",
    "    **Places:**\n",
    "    {place_list}\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": 2000\n",
    "    }\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload) as response:\n",
    "                response_json = await response.json()\n",
    "                clusters = response_json.get('choices', [{}])[0].get('message', {}).get('content', '')\n",
    "                return json.loads(clusters)  # Convert the response to JSON\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing API response: {e}\")\n",
    "            return {}\n",
    "\n",
    "# Function to merge clusters with final_final_entities.json, keeping names and frame numbers\n",
    "def merge_clusters_with_entities(final_entities, clusters):\n",
    "    merged_entities = {\"characters\": {}, \"objects\": {}, \"places\": {}}\n",
    "\n",
    "    # Process characters\n",
    "    character_clusters = clusters.get(\"Characters Clusters\", {})\n",
    "    for final_name, cluster_items in character_clusters.items():\n",
    "        merged_entities[\"characters\"][final_name] = {\"merged_from\": [], \"merged_names\": []}\n",
    "        for item in cluster_items:\n",
    "            if item in final_entities[\"characters\"]:\n",
    "                entity_data = final_entities[\"characters\"][item]\n",
    "                merged_entities[\"characters\"][final_name] = {\n",
    "                    **entity_data,\n",
    "                    \"Name\": final_name,\n",
    "                    \"merged_from\": list(set(merged_entities[\"characters\"][final_name][\"merged_from\"] + entity_data[\"merged_from\"])),\n",
    "                    \"merged_names\": list(set(merged_entities[\"characters\"][final_name][\"merged_names\"] + [item]))\n",
    "                }\n",
    "\n",
    "    # Process objects\n",
    "    object_clusters = clusters.get(\"Objects Clusters\", {})\n",
    "    for final_name, cluster_items in object_clusters.items():\n",
    "        merged_entities[\"objects\"][final_name] = {\"merged_from\": [], \"merged_names\": []}\n",
    "        for item in cluster_items:\n",
    "            if item in final_entities[\"objects\"]:\n",
    "                entity_data = final_entities[\"objects\"][item]\n",
    "                merged_entities[\"objects\"][final_name] = {\n",
    "                    **entity_data,\n",
    "                    \"Name\": final_name,\n",
    "                    \"merged_from\": list(set(merged_entities[\"objects\"][final_name][\"merged_from\"] + entity_data[\"merged_from\"])),\n",
    "                    \"merged_names\": list(set(merged_entities[\"objects\"][final_name][\"merged_names\"] + [item]))\n",
    "                }\n",
    "\n",
    "    # Process places\n",
    "    place_clusters = clusters.get(\"Places Clusters\", {})\n",
    "    for final_name, cluster_items in place_clusters.items():\n",
    "        merged_entities[\"places\"][final_name] = {\"merged_from\": [], \"merged_names\": []}\n",
    "        for item in cluster_items:\n",
    "            if item in final_entities[\"places\"]:\n",
    "                entity_data = final_entities[\"places\"][item]\n",
    "                merged_entities[\"places\"][final_name] = {\n",
    "                    **entity_data,\n",
    "                    \"Name\": final_name,\n",
    "                    \"merged_from\": list(set(merged_entities[\"places\"][final_name][\"merged_from\"] + entity_data[\"merged_from\"])),\n",
    "                    \"merged_names\": list(set(merged_entities[\"places\"][final_name][\"merged_names\"] + [item]))\n",
    "                }\n",
    "\n",
    "    return merged_entities\n",
    "# Main async function for clustering and merging entities\n",
    "async def cluster_and_merge_entities(api_key, json_output_dir, video_folder_path):\n",
    "    json_files = [os.path.join(json_output_dir, f) for f in os.listdir(json_output_dir) if f.endswith('.json')]\n",
    "\n",
    "    if not json_files:\n",
    "        print(\"No JSON files found for consolidation.\")\n",
    "        return\n",
    "\n",
    "    # Consolidate entities from all JSON files\n",
    "    final_entities = initial_consolidation(json_files)  # Ensure only json_files is passed here\n",
    "    print(final_entities)\n",
    "\n",
    "    # Cluster entities using OpenAI API\n",
    "    clusters = await cluster_entities(api_key, final_entities)\n",
    "\n",
    "    if not clusters:\n",
    "        print(\"No clusters returned from OpenAI API.\")\n",
    "        return\n",
    "\n",
    "    # Merge clusters into final final JSON\n",
    "    merged_final_entities = merge_clusters_with_entities(final_entities, clusters)\n",
    "\n",
    "    # Save the merged entities to JSON\n",
    "    merged_final_entities_path = os.path.join(video_folder_path, 'final_summary.json')\n",
    "    save_entities_to_json(merged_final_entities, merged_final_entities_path)\n",
    "    print(f\"Merged entities saved to {merged_final_entities_path}\")\n",
    "\n",
    "    # Update the final JSON with summary statistics\n",
    "    update_final_json_with_summary(merged_final_entities_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## other stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate summary statistics\n",
    "def generate_summary_statistics(final_entities):\n",
    "    stats = {\n",
    "        \"unique_characters_count\": len(final_entities[\"characters\"]),\n",
    "        \"unique_objects_count\": len(final_entities[\"objects\"]),\n",
    "        \"unique_places_count\": len(final_entities[\"places\"]),\n",
    "        \"average_physical_features_per_character\": 0,\n",
    "        \"average_features_per_object\": 0,\n",
    "        \"average_frames_per_character\": 0\n",
    "    }\n",
    "\n",
    "    # Calculate average physical features for characters\n",
    "    characters = final_entities.get(\"characters\", {})\n",
    "    if len(characters) > 0:\n",
    "        total_features_characters = sum(len(character.get(\"Physical Features\", [])) for character in characters.values())\n",
    "        stats[\"average_physical_features_per_character\"] = total_features_characters / len(characters)\n",
    "\n",
    "    # Calculate average features for objects\n",
    "    objects = final_entities.get(\"objects\", {})\n",
    "    if len(objects) > 0:\n",
    "        total_features_objects = sum(len(obj.get(\"Features\", [])) for obj in objects.values())\n",
    "        stats[\"average_features_per_object\"] = total_features_objects / len(objects)\n",
    "\n",
    "    # Calculate average frames per character\n",
    "    if len(characters) > 0:\n",
    "        total_frames_characters = sum(len(character.get(\"merged_from\", [])) for character in characters.values())\n",
    "        stats[\"average_frames_per_character\"] = total_frames_characters / len(characters)\n",
    "\n",
    "    return stats\n",
    "\n",
    "# # Example usage\n",
    "# final_json_path = '/path/to/your/final_summary.json'\n",
    "# update_final_json_with_summary(final_json_path)\n",
    "# print(f\"Updated final JSON with summary statistics saved to {final_json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Main Function Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Videos:   0%|          | 0/1 [00:00<?, ?video/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video 1/1: Bananas_in_pyjamas copy.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:pyscenedetect:VideoManager is deprecated and will be removed.\n",
      "INFO:pyscenedetect:Loaded 1 video, framerate: 24.969 FPS, resolution: 640 x 360\n",
      "INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 320 x 180\n",
      "INFO:pyscenedetect:Detecting scenes...\n",
      "INFO:root:Detected 3 scenes:\n",
      "INFO:root:Scene 1: Start 00:00:00.000 / Frame 0, End 00:00:08.370 / Frame 209\n",
      "INFO:root:Scene 2: Start 00:00:08.370 / Frame 209, End 00:00:19.344 / Frame 483\n",
      "INFO:root:Scene 3: Start 00:00:19.344 / Frame 483, End 00:00:22.147 / Frame 553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted and saved middle frame of scene 1 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/scenes_output/scene_1_frame_104.jpg\n",
      "Extracted and saved middle frame of scene 2 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/scenes_output/scene_2_frame_346.jpg\n",
      "Extracted and saved middle frame of scene 3 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/scenes_output/scene_3_frame_518.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_1_frame_104.jpg as scene_1_frame_104_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_3_frame_518.jpg as scene_3_frame_518_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Scenes: 100%|██████████| 3/3 [00:17<00:00,  5.84s/scene]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_2_frame_346.jpg as scene_2_frame_346_analysis.json\n",
      "{'characters': {'Banana Character 1': {'Name': 'Banana Character 1', 'Portion Boolean': 0, 'Human or Non-Human': 0, 'Physical Features': ['Yellow skin', 'Big eyes', 'Smiling mouth'], 'Explanation': 'The character is anthropomorphized, resembling a banana with human-like features, thus classified as non-human.', 'Age': 5, 'merged_from': ['346', '518']}, 'Banana Character 2': {'Name': 'Banana Character 2', 'Portion Boolean': 0, 'Human or Non-Human': 0, 'Physical Features': ['Yellow skin', 'Big eyes', 'Smiling mouth'], 'Explanation': 'The character is anthropomorphized, resembling a banana with human-like features, thus classified as non-human.', 'Age': 5, 'merged_from': ['346', '518']}}, 'objects': {'House': {'Name': 'House', 'Portion Boolean': 0, 'Color': 'Blue and White', 'Features': ['Roof', 'Walls', 'Door', 'Windows'], 'Total Features': 4, 'merged_from': ['104']}, 'Trees': {'Name': 'Trees', 'Portion Boolean': 0, 'Color': 'Green', 'Features': ['Branches', 'Leaves', 'Trunk'], 'Total Features': 3, 'merged_from': ['104']}, 'Tree': {'Name': 'Tree', 'Portion Boolean': 0, 'Color': 'Brown', 'Features': ['Bark', 'Branches'], 'Total Features': 2, 'merged_from': ['346', '518']}, 'Banana Characters': {'Name': 'Banana Characters', 'Portion Boolean': 0, 'Color': 'Yellow', 'Features': ['Eyes', 'Mouth', 'Clothes', 'Arms', 'Hair'], 'Total Features': 5, 'merged_from': ['346']}, 'Banana Character 1': {'Name': 'Banana Character 1', 'Portion Boolean': 0, 'Color': 'Yellow', 'Features': ['Eyes', 'Mouth', 'Stripes'], 'Total Features': 3, 'merged_from': ['518']}, 'Banana Character 2': {'Name': 'Banana Character 2', 'Portion Boolean': 0, 'Color': 'Yellow', 'Features': ['Eyes', 'Mouth', 'Stripes'], 'Total Features': 3, 'merged_from': ['518']}}, 'places': {'Fantasy House Scene': {'Name': 'Fantasy House Scene', 'Certainty Boolean': 1, 'Fantasy/Adventurous Place': 1, 'Explanation': 'The scene features a whimsical house depicted in a cartoon style, which does not represent a realistic place, indicating it is a fantasy setting.', 'merged_from': ['104']}, 'Cartoon Landscape': {'Name': 'Cartoon Landscape', 'Certainty Boolean': 1, 'Fantasy/Adventurous Place': 1, 'Explanation': 'The background features a cartoonish style with exaggerated colors and shapes that do not represent any real-life environment, classifying it as fantasy.', 'merged_from': ['346']}, 'Cartoon Garden': {'Name': 'Cartoon Garden', 'Certainty Boolean': 1, 'Fantasy/Adventurous Place': 1, 'Explanation': 'The cartoon garden is colorful and features animated characters and objects that do not exist in reality, qualifying it as a fantasy place.', 'merged_from': ['518']}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged entities saved to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/final_summary.json\n",
      "Finished processing video: Bananas_in_pyjamas copy.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Videos: 100%|██████████| 1/1 [00:25<00:00, 25.52s/video]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED PROCESSING ALL VIDEOS.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure the function is called correctly and used in processing\n",
    "def process_videos_in_directory(directory_path, output_base_dir):\n",
    "    video_files = [f for f in os.listdir(directory_path) if f.endswith(('.mp4', '.avi', '.mkv'))]\n",
    "\n",
    "    if not video_files:\n",
    "        print(\"No video files found in the directory.\", flush=True)\n",
    "        return\n",
    "\n",
    "    with tqdm(total=len(video_files), desc=\"Processing Videos\", unit=\"video\") as pbar:\n",
    "        for i, video_file in enumerate(video_files):\n",
    "            start_time = time.time()\n",
    "\n",
    "            try:\n",
    "                video_path = os.path.join(directory_path, video_file)\n",
    "                video_name = os.path.splitext(video_file)[0]\n",
    "                video_size = os.path.getsize(video_path)\n",
    "\n",
    "                video_output_dir = os.path.join(output_base_dir, video_name)\n",
    "                scenes_output_dir = os.path.join(video_output_dir, 'scenes_output')\n",
    "                json_output_dir = os.path.join(video_output_dir, 'json_output')\n",
    "\n",
    "                os.makedirs(scenes_output_dir, exist_ok=True)\n",
    "                os.makedirs(json_output_dir, exist_ok=True)\n",
    "\n",
    "                print(f\"Processing video {i + 1}/{len(video_files)}: {video_file}\", flush=True)\n",
    "\n",
    "                scenes = analyze_video(video_path)\n",
    "                extract_frames_imageio(video_path, scenes, scenes_output_dir)\n",
    "                asyncio.run(process_scenes_output(scenes_output_dir, json_output_dir))  # Run async scene processing\n",
    "\n",
    "                # Ensure final_entities.json is created in the correct location\n",
    "                final_json_path = os.path.join(json_output_dir, 'final_entities.json')\n",
    "                asyncio.run(cluster_and_merge_entities(api_key, json_output_dir, video_output_dir))\n",
    "\n",
    "                end_time = time.time()\n",
    "                processing_time = end_time - start_time\n",
    "\n",
    "                # Calculate video length (in seconds)\n",
    "                video_length = get_video_length(video_path)\n",
    "\n",
    "                # Calculate scenes per minute\n",
    "                scenes_per_minute = len(scenes) / (video_length / 60)\n",
    "\n",
    "                print(f\"Finished processing video: {video_file}\", flush=True)\n",
    "                pbar.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing video {video_file}: {e}\", flush=True)\n",
    "\n",
    "\n",
    "# Ensure the main script has appropriate paths\n",
    "video_directory = '/Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/02_Video_DB'\n",
    "output_base_directory = '/Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB'\n",
    "\n",
    "process_videos_in_directory(video_directory, output_base_directory)\n",
    "print(\"FINISHED PROCESSING ALL VIDEOS.\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
