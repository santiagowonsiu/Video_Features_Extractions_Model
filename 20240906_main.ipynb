{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to analyse the whole database of videos and process it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import json\n",
    "import imageio\n",
    "import re\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import colorsys\n",
    "import aiofiles\n",
    "import nest_asyncio\n",
    "from tqdm.asyncio import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from scenedetect import VideoManager, SceneManager\n",
    "from scenedetect.detectors import ContentDetector\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import csv\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load OpenAI API key from .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Apply nest_asyncio to handle the running event loop\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Concurrency limit\n",
    "semaphore = asyncio.Semaphore(5)\n",
    "\n",
    "# A dictionary to store characters across frames\n",
    "character_frames = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRACK API USAGE CALLS\n",
    "\n",
    "# Initialize API usage tracking\n",
    "api_usage = {\n",
    "    \"total_api_calls\": 0,\n",
    "    \"total_tokens_used\": 0,\n",
    "    \"model_used\": \"gpt-4\"  # Assuming you're using GPT-4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Video Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_video(video_path, threshold=27.0):\n",
    "    if not os.path.exists(video_path):\n",
    "        raise FileNotFoundError(f\"The video file {video_path} does not exist.\")\n",
    "    \n",
    "    video_manager = VideoManager([video_path])\n",
    "    scene_manager = SceneManager()\n",
    "    scene_manager.add_detector(ContentDetector(threshold=threshold))\n",
    "\n",
    "    video_manager.set_downscale_factor()\n",
    "    video_manager.start()\n",
    "\n",
    "    scene_manager.detect_scenes(frame_source=video_manager)\n",
    "    scene_list = scene_manager.get_scene_list()\n",
    "\n",
    "    video_manager.release()\n",
    "\n",
    "    logging.info(f'Detected {len(scene_list)} scenes:')\n",
    "    for i, scene in enumerate(scene_list):\n",
    "        logging.info(f'Scene {i + 1}: Start {scene[0].get_timecode()} / Frame {scene[0].get_frames()}, '\n",
    "              f'End {scene[1].get_timecode()} / Frame {scene[1].get_frames()}')\n",
    "\n",
    "    return scene_list\n",
    "\n",
    "def get_video_length(video_path):\n",
    "    # You can use a tool like OpenCV, ffmpeg, or similar to calculate video length\n",
    "    import cv2\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_length = frame_count / fps\n",
    "    cap.release()\n",
    "    return video_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Frame Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames_imageio(video_path, scenes, output_dir):\n",
    "    reader = imageio.get_reader(video_path)\n",
    "    for i, scene in enumerate(scenes):\n",
    "        start_frame, end_frame = scene\n",
    "        \n",
    "        # Convert FrameTimecode to integer frame numbers\n",
    "        start_frame_num = int(start_frame)\n",
    "        end_frame_num = int(end_frame)\n",
    "        \n",
    "        # Calculate the middle frame of the scene\n",
    "        middle_frame = (start_frame_num + end_frame_num) // 2\n",
    "        \n",
    "        # Set the reader to the middle frame and extract it\n",
    "        reader.set_image_index(middle_frame)\n",
    "        frame = reader.get_next_data()\n",
    "        \n",
    "        # Save the frame as an image with frame number in the filename\n",
    "        output_path = os.path.join(output_dir, f'scene_{i + 1}_frame_{middle_frame}.jpg')\n",
    "        imageio.imwrite(output_path, frame)\n",
    "        print(f\"Extracted and saved middle frame of scene {i + 1} as {output_path}\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Image Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def encode_image(image_path):\n",
    "    async with aiofiles.open(image_path, \"rb\") as image_file:\n",
    "        content = await image_file.read()\n",
    "        return base64.b64encode(content).decode('utf-8')\n",
    "\n",
    "def get_color_category(color):\n",
    "    r, g, b = [x / 255.0 for x in color]\n",
    "    h, l, s = colorsys.rgb_to_hls(r, g, b)\n",
    "\n",
    "    primary_hues = {\n",
    "        \"red\": (0.0, 0.1),  \n",
    "        \"yellow\": (0.1, 0.18),\n",
    "        \"green\": (0.25, 0.4),\n",
    "        \"blue\": (0.55, 0.75),\n",
    "    }\n",
    "\n",
    "    for color_name, hue_range in primary_hues.items():\n",
    "        if hue_range[0] <= h <= hue_range[1]:\n",
    "            return color_name\n",
    "\n",
    "    if (l >= 0.9 and s <= 0.1):\n",
    "        return \"white\"\n",
    "    if (l <= 0.1 and s <= 0.1):\n",
    "        return \"black\"\n",
    "\n",
    "    return \"non-primary\"\n",
    "\n",
    "def analyze_image_colors(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert('RGB')\n",
    "    data = np.array(image)\n",
    "\n",
    "    unique_colors, counts = np.unique(data.reshape(-1, data.shape[2]), axis=0, return_counts=True)\n",
    "    total_pixels = int(counts.sum())\n",
    "\n",
    "    color_counts = {\n",
    "        \"Red\": 0,\n",
    "        \"Yellow\": 0,\n",
    "        \"Green\": 0,\n",
    "        \"Blue\": 0,\n",
    "        \"White\": 0,\n",
    "        \"Black\": 0,\n",
    "        \"Non-primary\": 0\n",
    "    }\n",
    "\n",
    "    for color, count in zip(unique_colors, counts):\n",
    "        category = get_color_category(tuple(color))\n",
    "        color_counts[category.capitalize()] += int(count)\n",
    "\n",
    "    color_percentages = {color: (count / total_pixels) * 100 for color, count in color_counts.items()}\n",
    "    primary_total = color_counts[\"Red\"] + color_counts[\"Yellow\"] + color_counts[\"Blue\"]\n",
    "    color_dominance = \"Primary colors\" if primary_total > color_counts[\"Non-primary\"] else \"Non-primary colors\"\n",
    "\n",
    "    return {\n",
    "        \"Color Analysis\": {\n",
    "            \"Colors Found\": {\n",
    "                color: {\n",
    "                    \"Pixel Count\": count,\n",
    "                    \"Percentage\": f\"{color_percentages[color]:.2f}%\"\n",
    "                } for color, count in color_counts.items()\n",
    "            },\n",
    "            \"Dominance\": color_dominance\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) OpenAI API Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def send_image_to_openai(image_path, base64_image, retries=3):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": \"\"\"\n",
    "                        Analyze the following image and provide a detailed description in the format of JSON only. Ensure the output is strictly in JSON format without any additional text or code block formatting. The JSON should include the following standardized labels:\n",
    "\n",
    "                        1. **Image Analysis**: The root dictionary containing all analysis data.\n",
    "                        \n",
    "                        2. **Suitability**:\n",
    "                            - \"Partial Nudity\": Boolean indicating the presence of nudity (e.g., bare torso or non explicits).\n",
    "                            - \"Full Nudity\": Boolean indicating the presence of explicit nudity of private body parts.\n",
    "                            - \"Obscene Gestures\": Boolean indicating the presence of obscene gestures.\n",
    "                            - \"Alcohol\": Boolean indicating the presence of alcohol.\n",
    "                            - \"Drugs\": Boolean indicating the presence of drugs.\n",
    "                            - \"Addictions\": Boolean indicating the presence of addictions.\n",
    "\n",
    "                        3. **Objects**:\n",
    "                            - \"Total Objects Identified\": Integer representing the total number of objects identified.\n",
    "                            - \"Average Features Per Object\": Float representing the average number of features per object.\n",
    "                            - \"Objects Details\": Dictionary containing details of each object, where each object is labeled as \"Object_1\", \"Object_2\", etc., with the following structure:\n",
    "                                - \"Name\": The name of the object - as simplest and descriptive possible.\n",
    "                                - \"Portion Boolean\": 0-1 output indicating if the object is a portion of a larger object (1) or a complete object (0). For example, a leg is a portion of a human. However, if the object is just cropped but clearly identifiable as a complete object, it should be considered a complete object.\n",
    "                                - \"Color\": The color of the object.\n",
    "                                - \"Features\": List of features of the object.\n",
    "                                - \"Total Features\": Integer representing the number of features for the object.\n",
    "\n",
    "                        4. **Place**:\n",
    "                            - \"Name\": The name of the place - as simplest and descriptive (dont use generic such as cartoon, unknown, setting).\n",
    "                            - \"Certainty Boolean\": 0-1 output indicating if the place is clearly identifiable (1) or not (0).\n",
    "                            - \"Fantasy/Adventurous Place\": Boolean (0-1) indicating whether the place is classified as a fantasy/adventurous place or not.\n",
    "                            - \"Explanation\": Detailed explanation of why the place is classified as fantasy/adventurous or not. Fantasy places are those that do not exist in reality, and adventurous places are defined as those involving clear statements of traveling to space or another country.\n",
    "\n",
    "                        5. **Characters**:\n",
    "                            - \"Total Characters Identified\": Integer representing the total number of characters identified.\n",
    "                            - \"Average Features Per Character\": Float representing the average number of features per character.\n",
    "                            - \"Character Details\": Dictionary containing details of each character, where each character is labeled as \"Character_1\", \"Character_2\", etc., with the following structure:\n",
    "                                - \"Name\": The name of the character - as simplest and descriptive as possible.\n",
    "                                - \"Portion Boolean\": 0-1 output indicating if the character is a portion of a larger character (1) or a complete character (0). For example, a leg is a portion of a human. However, if the character is just cropped but clearly identifiable as a complete character, it should be considered a complete character.\n",
    "                                - \"Human or Non-Human\": 0-1 output indicating if the character appears human (1) or non-human (0). Anthropomorphized characters or any other combination not fully human are considered non-human.\n",
    "                                - \"Physical Features\": List of physical features of the character.\n",
    "                                - \"Explanation\": Explanation for why the character is classified as human or non-human, and why these physical features are inferred.\n",
    "                                - \"Age\": Expected age range of the character (a single number).\n",
    "                            **Note**: If the \"character\" consists of only a part of a body (such as a hand, leg, or face without enough distinguishing features to identify it as a complete character), do not count it as a \"character.\"\n",
    "\n",
    "                        Ensure that the structure of the JSON output strictly adheres to these standardized labels.\n",
    "                        \"\"\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 750\n",
    "    }\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload) as response:\n",
    "                    status = response.status\n",
    "                    response_text = await response.text()\n",
    "\n",
    "                    if status == 429:\n",
    "                        print(\"Rate limit exceeded, retrying...\")\n",
    "                        await asyncio.sleep(2 ** attempt)\n",
    "                        continue\n",
    "                    elif status == 200:\n",
    "                        content = await response.json()\n",
    "\n",
    "                        # Track API usage\n",
    "                        api_usage['total_api_calls'] += 1\n",
    "                        api_usage['total_tokens_used'] += content.get('usage', {}).get('total_tokens', 0)\n",
    "                        api_usage['model_used'] = content.get('model', 'gpt-4o-mini')\n",
    "\n",
    "                        # Handle and return JSON content\n",
    "                        if 'choices' in content:\n",
    "                            message_content = content['choices'][0].get('message', {}).get('content', '').strip()\n",
    "                            try:\n",
    "                                return json.loads(message_content)\n",
    "                            except json.JSONDecodeError as e:\n",
    "                                print(f\"Error decoding JSON from OpenAI response for {image_path}: {e}\")\n",
    "                                return None\n",
    "                        else:\n",
    "                            print(f\"Unexpected response format from OpenAI API for {image_path}.\")\n",
    "                            return None\n",
    "                    else:\n",
    "                        print(f\"Request failed with status code {status} for {image_path}.\")\n",
    "                        return None\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Request failed due to a client error: {e}\")\n",
    "            await asyncio.sleep(2 ** attempt)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error occurred: {e}\")\n",
    "            await asyncio.sleep(2 ** attempt)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Scene Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_scenes_output(output_dir, json_output_dir):\n",
    "    os.makedirs(json_output_dir, exist_ok=True)\n",
    "    scenes = sorted([f for f in os.listdir(output_dir) if f.endswith('.jpg')], key=extract_scene_number)\n",
    "    total_scenes = len(scenes)\n",
    "    with tqdm(total=total_scenes, desc=\"Processing Scenes\", unit=\"scene\") as pbar:\n",
    "        tasks = [process_single_scene(i, scene, output_dir, json_output_dir, pbar) for i, scene in enumerate(scenes)]\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "async def process_single_scene(i, scene, output_dir, json_output_dir, pbar):\n",
    "    async with semaphore:  # Limit concurrent execution\n",
    "        scene_path = os.path.join(output_dir, scene)\n",
    "\n",
    "        # Encode image in base64\n",
    "        base64_image = await encode_image(scene_path)\n",
    "\n",
    "        # Perform color analysis\n",
    "        color_analysis_result = analyze_image_colors(scene_path)\n",
    "\n",
    "        # Send image to OpenAI for further analysis\n",
    "        openai_response = await send_image_to_openai(scene_path, base64_image)\n",
    "\n",
    "        # Check if openai_response is valid (not None or empty)\n",
    "        if not openai_response:\n",
    "            print(f\"Skipping {scene} due to invalid OpenAI response.\")\n",
    "            pbar.update(1)\n",
    "            return\n",
    "\n",
    "        # Combine both results, and include the reference to the image file\n",
    "        final_output = {\n",
    "            \"Image File\": scene,\n",
    "            \"Image Analysis\": {\n",
    "                **color_analysis_result[\"Color Analysis\"],\n",
    "                **openai_response.get(\"Image Analysis\", {})\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # The filename already includes the scene number and frame number\n",
    "        output_filename = os.path.splitext(scene)[0] + '_analysis.json'\n",
    "        output_path = os.path.join(json_output_dir, output_filename)\n",
    "\n",
    "        try:\n",
    "            async with aiofiles.open(output_path, 'w') as json_file:\n",
    "                await json_file.write(json.dumps(final_output, indent=4))\n",
    "                print(f\"Saved analysis for {scene} as {output_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save analysis for {scene}: {e}\")\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "def extract_scene_number(filename):\n",
    "    match = re.search(r'\\d+', filename)\n",
    "    return int(match.group()) if match else -1\n",
    "\n",
    "def extract_frame_number(filename):\n",
    "    match = re.search(r'_frame_(\\d+)', filename)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Luminance and Edge Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Helper function to calculate the average luminance of a frame\n",
    "def calculate_luminance(image):\n",
    "    grayscale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return np.mean(grayscale)\n",
    "\n",
    "# Function to save the first and last frames of a scene side by side, with a label if strong luminance is detected\n",
    "def save_frame_pair(luminance_output_dir, scene_index, last_frame, first_frame, strong_luminance=False):\n",
    "    # Concatenate images horizontally (side by side)\n",
    "    concatenated_image = np.concatenate((last_frame, first_frame), axis=1)\n",
    "\n",
    "    # Add a label in the filename if strong luminance change is detected\n",
    "    filename_suffix = \"_STRONG_LUMINANCE\" if strong_luminance else \"\"\n",
    "\n",
    "    # Save the frame comparison image in the correct directory\n",
    "    output_image_path = os.path.join(luminance_output_dir, f\"scene_{scene_index + 1}_to_{scene_index + 2}{filename_suffix}.jpg\")\n",
    "    cv2.imwrite(output_image_path, concatenated_image)\n",
    "    print(f\"Saved frame comparison image to {output_image_path}\")\n",
    "\n",
    "    \n",
    "\n",
    "# Analyze scene transitions for luminance changes and detect short scenes, saving the images\n",
    "def analyze_scenes_for_flicker_and_short_scenes(video_path, scenes, video_length, luminance_output_dir, short_scene_threshold=1.0, luminance_threshold=25):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_scenes = len(scenes)\n",
    "    strong_luminance_changes = 0\n",
    "    short_scenes_count = 0\n",
    "\n",
    "    print(f\"Analyzing scenes for flicker and short scene detection...\", flush=True)\n",
    "\n",
    "    for scene_index in range(total_scenes - 1):\n",
    "        start_timecode, end_timecode = scenes[scene_index]\n",
    "        next_start_timecode, next_end_timecode = scenes[scene_index + 1]\n",
    "\n",
    "        # Convert timecodes to frame numbers\n",
    "        start_frame = int(start_timecode.get_frames())\n",
    "        end_frame = int(end_timecode.get_frames())\n",
    "        next_start_frame = int(next_start_timecode.get_frames())\n",
    "\n",
    "        # Get the last frame of the current scene\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, end_frame - 1)\n",
    "        ret1, last_frame = cap.read()\n",
    "\n",
    "        # Get the first frame of the next scene\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, next_start_frame)\n",
    "        ret2, first_frame = cap.read()\n",
    "\n",
    "        if not ret1 or not ret2:\n",
    "            print(f\"Warning: Failed to read frames for scene {scene_index + 1} to {scene_index + 2}. Skipping this scene.\")\n",
    "            continue\n",
    "\n",
    "        # Calculate luminance for the last frame of the current scene and the first frame of the next scene\n",
    "        last_luminance = calculate_luminance(last_frame)\n",
    "        first_luminance = calculate_luminance(first_frame)\n",
    "\n",
    "        # Check for strong luminance changes\n",
    "        strong_luminance = False\n",
    "        if last_luminance is not None and first_luminance is not None:\n",
    "            luminance_change = abs(first_luminance - last_luminance)\n",
    "            if luminance_change > luminance_threshold:\n",
    "                strong_luminance_changes += 1\n",
    "                strong_luminance = True\n",
    "\n",
    "        # Save the frame comparison image\n",
    "        save_frame_pair(luminance_output_dir, scene_index, last_frame, first_frame, strong_luminance)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Calculate percentages\n",
    "    percentage_strong_transitions = (strong_luminance_changes / (total_scenes - 1)) * 100 if total_scenes > 1 else 0\n",
    "    percentage_short_scenes = (short_scenes_count / total_scenes) * 100 if total_scenes > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"percentage_strong_luminance_transitions\": percentage_strong_transitions,\n",
    "        \"percentage_short_scenes\": percentage_short_scenes,\n",
    "        \"number_of_strong_luminance_transitions\": strong_luminance_changes,\n",
    "        \"number_of_short_scenes\": short_scenes_count\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate edge density in an image and save it in the correct folder\n",
    "def calculate_edge_density(image_path, edges_output_dir):\n",
    "    try:\n",
    "        # Read the image in grayscale\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Failed to load image: {image_path}\")\n",
    "\n",
    "        # Apply Canny edge detection\n",
    "        edges = cv2.Canny(image, 100, 200)\n",
    "\n",
    "        # Calculate the percentage of edge pixels\n",
    "        edge_pixels = np.sum(edges > 0)\n",
    "        total_pixels = image.size\n",
    "        edge_density = (edge_pixels / total_pixels) * 100\n",
    "\n",
    "        # Save the edge-detected image in the edges_output_dir\n",
    "        image_filename = os.path.basename(image_path)  # Get the image filename\n",
    "        edge_image_path = os.path.join(edges_output_dir, f\"{image_filename.replace('.jpg', '_edges.jpg')}\")\n",
    "        cv2.imwrite(edge_image_path, edges)\n",
    "        print(f\"Saved edge detection image to {edge_image_path}\")\n",
    "\n",
    "        return edge_density\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def analyze_edge_density_for_scenes(scenes_output_dir, edges_output_dir):\n",
    "    edge_densities = []\n",
    "\n",
    "    # Process each frame in the scenes_output_dir\n",
    "    for image_file in os.listdir(scenes_output_dir):\n",
    "        if image_file.endswith(\".jpg\") and \"_edges\" not in image_file:\n",
    "            image_path = os.path.join(scenes_output_dir, image_file)\n",
    "            edge_density = calculate_edge_density(image_path, edges_output_dir)\n",
    "            edge_image_path = os.path.join(edges_output_dir, f\"{image_file.replace('.jpg', '_edges.jpg')}\")\n",
    "\n",
    "            # Save edge-detected image in the edges_output_dir\n",
    "            cv2.imwrite(edge_image_path, edge_density)  # Save the detected edge image to the edges_output_dir\n",
    "\n",
    "            # Only add to the list if edge_density is valid\n",
    "            if edge_density is not None:\n",
    "                edge_densities.append(edge_density)\n",
    "\n",
    "    # Calculate the average edge density\n",
    "    avg_edge_density = sum(edge_densities) / len(edge_densities) if edge_densities else 0\n",
    "\n",
    "    print(f\"Average edge density for all scenes: {avg_edge_density}%\")\n",
    "    \n",
    "    # Return the average edge density\n",
    "    return {\"average_edge_density\": avg_edge_density}\n",
    "\n",
    "\n",
    "\n",
    "# Edge detection function\n",
    "def save_edge_detection_images(scenes_output_dir, edges_output_dir):\n",
    "    # Ensure the edges_output_dir is created\n",
    "    os.makedirs(edges_output_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate through the scenes output directory\n",
    "    for image_file in os.listdir(scenes_output_dir):\n",
    "        if image_file.endswith(\".jpg\") and \"_edges\" not in image_file:\n",
    "            image_path = os.path.join(scenes_output_dir, image_file)\n",
    "            \n",
    "            # Perform edge detection and save the image in edges_output_dir\n",
    "            edge_density = calculate_edge_density(image_path, edges_output_dir)\n",
    "            print(f\"Edge density for {image_file}: {edge_density}%\")\n",
    "\n",
    "\n",
    "\n",
    "# Example to run the full process with your existing functions\n",
    "def process_video_with_flicker_analysis(video_path, output_dir, short_scene_threshold=3.0, luminance_threshold=25):\n",
    "    # Get the video length\n",
    "    video_length = get_video_length(video_path)\n",
    "    \n",
    "    # Detect scenes using your analyze_video function\n",
    "    scenes = analyze_video(video_path)\n",
    "    \n",
    "    # Ensure scenes are properly detected\n",
    "    if not scenes:\n",
    "        print(\"No scenes detected.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Total scenes detected: {len(scenes)}\")\n",
    "    \n",
    "    # Analyze luminance changes and short scenes\n",
    "    flicker_and_short_scene_stats = analyze_scenes_for_flicker_and_short_scenes(video_path, scenes, video_length, output_dir, short_scene_threshold, luminance_threshold)\n",
    "    \n",
    "    # Output results\n",
    "    return flicker_and_short_scene_stats\n",
    "\n",
    "# # Call the function with your video path and output directory\n",
    "# video_path = \"/Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/02_Video_DB/Bananas_in_pyjamas copy.mp4\"\n",
    "# output_dir = \"/Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy\"\n",
    "# process_video_with_flicker_analysis(video_path, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Run whole analysis of each json output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Path Construction: get_image_path generates the correct path to the image file based on the JSON filename.\n",
    "\n",
    "Entity Extraction:extract_entities_from_json pulls characters, objects, and places from the JSON data.\n",
    "\n",
    "Image-to-Image Comparison:perform_image_to_image_comparison compares partial objects with full objects using the OpenAI API.\n",
    "\n",
    "Entity Comparison:compare_entities handles both name-based and image-based comparisons to decide whether two entities should be consolidated.\n",
    "\n",
    "Consolidation:Entities across frames are consolidated into a single summary file that tracks where each entity was found.\n",
    "\n",
    "Main Execution:The script runs through all JSON files, processes the entities, and saves the consolidated results to a summary JSON file.\n",
    "\n",
    "Key Features of This Implementation:\n",
    "Text-Based Comparison: The code first attempts to merge entities based on exact name matches. If no match is found, it uses the OpenAI API to determine if two entities with different names should be merged.\n",
    "\n",
    "Image-to-Image Comparison: If one of the entities is flagged as a portion, or if names don't match but the entities might still be the same, the code performs an image-to-image comparison using the OpenAI API.\n",
    "\n",
    "Efficient Processing: The code processes each frame sequentially and logs all merges into merged_entities_log, ensuring you have a record of what entities were merged, including their original names and frames.\n",
    "\n",
    "No Overwritten Functionality: The original image analysis functionality is preserved and integrated smoothly with the text-based comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of Unique Characters, Objects, and Places: This can be done by counting the keys in the consolidated_data dictionary.\n",
    "Average Characters per Frame: This can be calculated by summing up all instances of characters found across frames and dividing by the total number of frames where characters appear.\n",
    "Average Features per Character/Object: Calculate this by summing the features of all characters/objects and dividing by the total number of characters/objects.\n",
    "Overall Color Analysis: Aggregate the color data from all JSON files.\n",
    "Filter Compliance: Check for any instances where the filters (e.g., nudity, drugs) are not compliant and log the frame numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import openai\n",
    "\n",
    "# Apply nest_asyncio to handle the running event loop\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize OpenAI API\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OpenAI API key is not set.\")\n",
    "openai.api_key = api_key\n",
    "\n",
    "#Initial consolidation of entities\n",
    "def initial_consolidation(json_files):\n",
    "    consolidated_data = {\"characters\": {}, \"objects\": {}, \"places\": {}}\n",
    "    merge_tracking = {\"characters\": {}, \"objects\": {}, \"places\": {}}\n",
    "\n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            consolidate_from_json(json_data, consolidated_data, merge_tracking, os.path.basename(json_file))\n",
    "\n",
    "    return consolidated_data\n",
    "\n",
    "\n",
    "# Function to save entities to JSON\n",
    "def save_entities_to_json(entities, path):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(entities, f, indent=4)\n",
    "\n",
    "# Function to consolidate entities from a JSON file\n",
    "# Function to consolidate entities from a JSON file\n",
    "def consolidate_from_json(json_data, consolidated_data, merge_tracking, json_file_name):\n",
    "    frame_number = json_file_name.split('_')[3]  # Extract frame number\n",
    "\n",
    "    if \"Image Analysis\" in json_data:\n",
    "        characters = json_data[\"Image Analysis\"].get(\"Characters\", {}).get(\"Character Details\", {})\n",
    "        for key, details in characters.items():\n",
    "            name = details.get(\"Name\")\n",
    "            if name:\n",
    "                if name not in consolidated_data[\"characters\"]:\n",
    "                    consolidated_data[\"characters\"][name] = details\n",
    "                    consolidated_data[\"characters\"][name][\"merged_from\"] = []\n",
    "                    merge_tracking[\"characters\"][name] = {\"merged_from\": []}\n",
    "                if frame_number not in consolidated_data[\"characters\"][name][\"merged_from\"]:\n",
    "                    consolidated_data[\"characters\"][name][\"merged_from\"].append(frame_number)\n",
    "                    merge_tracking[\"characters\"][name][\"merged_from\"].append(frame_number)\n",
    "\n",
    "        objects = json_data[\"Image Analysis\"].get(\"Objects\", {}).get(\"Objects Details\", {})\n",
    "        for key, details in objects.items():\n",
    "            name = details.get(\"Name\")\n",
    "            if name:\n",
    "                if name not in consolidated_data[\"objects\"]:\n",
    "                    consolidated_data[\"objects\"][name] = details\n",
    "                    consolidated_data[\"objects\"][name][\"merged_from\"] = []\n",
    "                    merge_tracking[\"objects\"][name] = {\"merged_from\": []}\n",
    "                if frame_number not in consolidated_data[\"objects\"][name][\"merged_from\"]:\n",
    "                    consolidated_data[\"objects\"][name][\"merged_from\"].append(frame_number)\n",
    "                    merge_tracking[\"objects\"][name][\"merged_from\"].append(frame_number)\n",
    "\n",
    "        place = json_data[\"Image Analysis\"].get(\"Place\", {})\n",
    "        place_name = place.get(\"Name\")\n",
    "        if place_name:\n",
    "            if place_name not in consolidated_data[\"places\"]:\n",
    "                consolidated_data[\"places\"][place_name] = place\n",
    "                consolidated_data[\"places\"][place_name][\"merged_from\"] = []\n",
    "                merge_tracking[\"places\"][place_name] = {\"merged_from\": []}\n",
    "            if frame_number not in consolidated_data[\"places\"][place_name][\"merged_from\"]:\n",
    "                consolidated_data[\"places\"][place_name][\"merged_from\"].append(frame_number)\n",
    "                merge_tracking[\"places\"][place_name][\"merged_from\"].append(frame_number)\n",
    "\n",
    "\n",
    "# Function to cluster entities using OpenAI API and name the clusters\n",
    "# Initialize API usage tracking\n",
    "api_usage = {\n",
    "    \"total_api_calls\": 0,\n",
    "    \"total_tokens_used\": 0,\n",
    "    \"model_used\": \"gpt-4\",  # Set the default model name here\n",
    "}\n",
    "\n",
    "# Function to cluster entities using OpenAI API and track token usage\n",
    "async def cluster_entities(api_key, entities):\n",
    "    # Generate lists for characters, objects, and places from entities\n",
    "    character_list = ', '.join(entities['characters'].keys())\n",
    "    object_list = ', '.join(entities['objects'].keys())\n",
    "    place_list = ', '.join(entities['places'].keys())\n",
    "\n",
    "    if not character_list and not object_list and not place_list:\n",
    "        return \"No entities available to cluster.\"\n",
    "\n",
    "    # Adjusted OpenAI prompt to return structured output (dictionaries)\n",
    "    prompt = f\"\"\"\n",
    "    You are tasked with clustering and naming entities from a TV show. Below are lists of characters, objects, and places extracted from different scenes. These lists sometimes contain multiple labels for the same entity.\n",
    "\n",
    "    **Instructions:**\n",
    "\n",
    "    1. Group the characters, objects, and places that refer to the same entity and suggest a single **final name** for each group ( 1. be smart what could be the same individual/object in the show one and what couldn't. 2.same for places - if it is another house environment or not the same in general outdoor place).\n",
    "    2. Return the result as a dictionary where each cluster (key) contains the entities (values) that belong to that cluster.\n",
    "    3. Use this format:\n",
    "\n",
    "    {{\n",
    "      \"Characters Clusters\": {{\n",
    "        \"Final Name 1\": [\"Character 1\", \"Character 2\", ...],\n",
    "        \"Final Name 2\": [\"Character 3\", \"Character 4\", ...]\n",
    "      }},\n",
    "      \"Objects Clusters\": {{\n",
    "        \"Final Name 1\": [\"Object 1\", \"Object 2\", ...],\n",
    "        \"Final Name 2\": [\"Object 3\", \"Object 4\", ...]\n",
    "      }},\n",
    "      \"Places Clusters\": {{\n",
    "        \"Final Name 1\": [\"Place 1\", \"Place 2\", ...],\n",
    "        \"Final Name 2\": [\"Place 3\", \"Place 4\", ...]\n",
    "      }}\n",
    "    }}\n",
    "\n",
    "    **Characters:**\n",
    "    {character_list}\n",
    "\n",
    "    **Objects:**\n",
    "    {object_list}\n",
    "\n",
    "    **Places:**\n",
    "    {place_list}\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4\",  # Ensure you set the correct model here\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": 2000\n",
    "    }\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload) as response:\n",
    "                response_json = await response.json()\n",
    "                clusters = response_json.get('choices', [{}])[0].get('message', {}).get('content', '')\n",
    "\n",
    "                # Track API usage\n",
    "                api_usage['total_api_calls'] += 1\n",
    "                api_usage['total_tokens_used'] += response_json.get('usage', {}).get('total_tokens', 0)\n",
    "                api_usage['model_used'] = response_json.get('model', 'gpt-4')\n",
    "\n",
    "                return json.loads(clusters)  # Convert the response to JSON\n",
    "        except Exception as e:\n",
    "            print(f\"Error accessing API response: {e}\")\n",
    "            return {}\n",
    "\n",
    "\n",
    "# Function to merge clusters with final_final_entities.json, keeping names and frame numbers\n",
    "def merge_clusters_with_entities(final_entities, clusters):\n",
    "    merged_entities = {\"characters\": {}, \"objects\": {}, \"places\": {}}\n",
    "\n",
    "    # Process characters\n",
    "    character_clusters = clusters.get(\"Characters Clusters\", {})\n",
    "    for final_name, cluster_items in character_clusters.items():\n",
    "        merged_entities[\"characters\"][final_name] = {\"merged_from\": [], \"merged_names\": []}\n",
    "        for item in cluster_items:\n",
    "            if item in final_entities[\"characters\"]:\n",
    "                entity_data = final_entities[\"characters\"][item]\n",
    "                merged_entities[\"characters\"][final_name] = {\n",
    "                    **entity_data,\n",
    "                    \"Name\": final_name,\n",
    "                    \"merged_from\": list(set(merged_entities[\"characters\"][final_name][\"merged_from\"] + entity_data[\"merged_from\"])),\n",
    "                    \"merged_names\": list(set(merged_entities[\"characters\"][final_name][\"merged_names\"] + [item]))\n",
    "                }\n",
    "\n",
    "    # Process objects\n",
    "    object_clusters = clusters.get(\"Objects Clusters\", {})\n",
    "    for final_name, cluster_items in object_clusters.items():\n",
    "        merged_entities[\"objects\"][final_name] = {\"merged_from\": [], \"merged_names\": []}\n",
    "        for item in cluster_items:\n",
    "            if item in final_entities[\"objects\"]:\n",
    "                entity_data = final_entities[\"objects\"][item]\n",
    "                merged_entities[\"objects\"][final_name] = {\n",
    "                    **entity_data,\n",
    "                    \"Name\": final_name,\n",
    "                    \"merged_from\": list(set(merged_entities[\"objects\"][final_name][\"merged_from\"] + entity_data[\"merged_from\"])),\n",
    "                    \"merged_names\": list(set(merged_entities[\"objects\"][final_name][\"merged_names\"] + [item]))\n",
    "                }\n",
    "\n",
    "    # Process places\n",
    "    place_clusters = clusters.get(\"Places Clusters\", {})\n",
    "    for final_name, cluster_items in place_clusters.items():\n",
    "        merged_entities[\"places\"][final_name] = {\"merged_from\": [], \"merged_names\": []}\n",
    "        for item in cluster_items:\n",
    "            if item in final_entities[\"places\"]:\n",
    "                entity_data = final_entities[\"places\"][item]\n",
    "                merged_entities[\"places\"][final_name] = {\n",
    "                    **entity_data,\n",
    "                    \"Name\": final_name,\n",
    "                    \"merged_from\": list(set(merged_entities[\"places\"][final_name][\"merged_from\"] + entity_data[\"merged_from\"])),\n",
    "                    \"merged_names\": list(set(merged_entities[\"places\"][final_name][\"merged_names\"] + [item]))\n",
    "                }\n",
    "\n",
    "    return merged_entities\n",
    "\n",
    "# Main async function for clustering and merging entities\n",
    "# Main async function for clustering and merging entities\n",
    "async def cluster_and_merge_entities(api_key, json_output_dir, video_folder_path):\n",
    "    json_files = [os.path.join(json_output_dir, f) for f in os.listdir(json_output_dir) if f.endswith('.json')]\n",
    "\n",
    "    if not json_files:\n",
    "        print(\"No JSON files found for consolidation.\")\n",
    "        return None\n",
    "\n",
    "    # Consolidate entities from all JSON files\n",
    "    final_entities = initial_consolidation(json_files)  # Ensure only json_files is passed here\n",
    "\n",
    "    # Cluster entities using OpenAI API\n",
    "    clusters = await cluster_entities(api_key, final_entities)\n",
    "\n",
    "    if not clusters:\n",
    "        print(\"No clusters returned from OpenAI API.\")\n",
    "        return None\n",
    "\n",
    "    # Merge clusters into final final JSON\n",
    "    merged_final_entities = merge_clusters_with_entities(final_entities, clusters)\n",
    "\n",
    "    # Save the merged entities to JSON\n",
    "    merged_final_entities_path = os.path.join(video_folder_path, 'final_summary.json')\n",
    "    save_entities_to_json(merged_final_entities, merged_final_entities_path)\n",
    "    print(f\"Merged entities saved to {merged_final_entities_path}\")\n",
    "    \n",
    "    return merged_final_entities  # Return merged entities for further use\n",
    "\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Define paths\n",
    "#     final_final_json_path = \"path/to/final_final_entities.json\"\n",
    "#     video_folder_path = \"path/to/video_folder\"\n",
    "    \n",
    "#     # Run the async function\n",
    "#     asyncio.run(cluster_and_merge_entities(api_key, final_final_json_path, video_folder_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summary_statistics(entities):\n",
    "    statistics = {\n",
    "        \"number_of_characters\": len(entities.get(\"characters\", {})),\n",
    "        \"number_of_objects\": len(entities.get(\"objects\", {})),\n",
    "        \"number_of_places\": len(entities.get(\"places\", {})),\n",
    "        \"avg_features_per_character\": 0,\n",
    "        \"avg_features_per_object\": 0,\n",
    "        \"avg_appearances_per_character\": 0,\n",
    "        \"avg_appearances_per_object\": 0,\n",
    "    }\n",
    "\n",
    "    # Calculate average features per character\n",
    "    total_character_features = 0\n",
    "    total_character_appearances = 0\n",
    "    for char, details in entities.get(\"characters\", {}).items():\n",
    "        total_character_features += len(details.get(\"Physical Features\", []))\n",
    "        total_character_appearances += len(details.get(\"merged_from\", []))\n",
    "    \n",
    "    if statistics[\"number_of_characters\"] > 0:\n",
    "        statistics[\"avg_features_per_character\"] = total_character_features / statistics[\"number_of_characters\"]\n",
    "        statistics[\"avg_appearances_per_character\"] = total_character_appearances / statistics[\"number_of_characters\"]\n",
    "\n",
    "    # Calculate average features per object\n",
    "    total_object_features = 0\n",
    "    total_object_appearances = 0\n",
    "    for obj, details in entities.get(\"objects\", {}).items():\n",
    "        total_object_features += details.get(\"Total Features\", 0)\n",
    "        total_object_appearances += len(details.get(\"merged_from\", []))\n",
    "\n",
    "    if statistics[\"number_of_objects\"] > 0:\n",
    "        statistics[\"avg_features_per_object\"] = total_object_features / statistics[\"number_of_objects\"]\n",
    "        statistics[\"avg_appearances_per_object\"] = total_object_appearances / statistics[\"number_of_objects\"]\n",
    "\n",
    "    return statistics\n",
    "\n",
    "def modify_summary_json_with_statistics(entities, path):\n",
    "    # Compute statistics\n",
    "    statistics = compute_summary_statistics(entities)\n",
    "    \n",
    "    # Load the original JSON data\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Create a new dictionary with statistics at the top\n",
    "    modified_data = {\n",
    "        \"summary_statistics\": statistics,\n",
    "        **data  # This merges the existing data under the statistics\n",
    "    }\n",
    "\n",
    "    # Save the modified data back to the JSON\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(modified_data, f, indent=4)\n",
    "\n",
    "    print(f\"Summary statistics added and saved to {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save additional Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_additional_stats(video_path, scenes_output_dir, start_time, end_time, video_output_dir, json_output_dir, summary_statistics, scenes, edges_output_dir, flicker_and_short_scene_stats, edge_density_stats, percentage_place_changes=None):\n",
    "    # Initialize additional information\n",
    "    additional_info = {\n",
    "        \"video_title\": None,\n",
    "        \"video_size_bytes\": None,\n",
    "        \"video_length_seconds\": None,\n",
    "        \"number_of_scenes\": None,\n",
    "        \"processing_time_seconds\": None\n",
    "    }\n",
    "\n",
    "    # Extract video title\n",
    "    try:\n",
    "        additional_info[\"video_title\"] = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        print(f\"Video title: {additional_info['video_title']}\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting video title: {e}\", flush=True)\n",
    "\n",
    "    # Extract video size\n",
    "    try:\n",
    "        additional_info[\"video_size_bytes\"] = os.path.getsize(video_path)\n",
    "        print(f\"Video size: {additional_info['video_size_bytes']} bytes\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting video size: {e}\", flush=True)\n",
    "\n",
    "    # Get video length\n",
    "    try:\n",
    "        additional_info[\"video_length_seconds\"] = get_video_length(video_path)\n",
    "        print(f\"Video length: {additional_info['video_length_seconds']} seconds\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating video length: {e}\", flush=True)\n",
    "\n",
    "    # Count the number of scenes\n",
    "    try:\n",
    "        additional_info[\"number_of_scenes\"] = len(scenes)\n",
    "        print(f\"Number of scenes: {additional_info['number_of_scenes']}\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error counting scenes: {e}\", flush=True)\n",
    "\n",
    "    # Calculate processing time\n",
    "    try:\n",
    "        additional_info[\"processing_time_seconds\"] = end_time - start_time\n",
    "        print(f\"Processing time: {additional_info['processing_time_seconds']} seconds\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating processing time: {e}\", flush=True)\n",
    "\n",
    "    # Include luminance analysis results\n",
    "    additional_info[\"luminance_analysis\"] = flicker_and_short_scene_stats\n",
    "\n",
    "    # Analyze all JSON files in the json_output folder\n",
    "    try:\n",
    "        json_stats = analyze_json_files(json_output_dir)\n",
    "        additional_info.update(json_stats)\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing JSON files: {e}\", flush=True)\n",
    "\n",
    "    # Include API usage stats\n",
    "    additional_info[\"api_usage\"] = {\n",
    "        \"total_api_calls\": api_usage[\"total_api_calls\"],\n",
    "        \"total_tokens_used\": api_usage[\"total_tokens_used\"],\n",
    "        \"model_used\": api_usage[\"model_used\"]\n",
    "    }\n",
    "\n",
    "    # Include the summary statistics in the same JSON file\n",
    "    additional_info[\"summary_statistics\"] = summary_statistics\n",
    "\n",
    "    # Include the already calculated edge density stats\n",
    "    additional_info.update(edge_density_stats)\n",
    "\n",
    "    # Add the place continuity percentage if available\n",
    "    if percentage_place_changes is not None:\n",
    "        additional_info[\"place_discontinuity_percentage\"] = percentage_place_changes\n",
    "        print(f\"Place continuity percentage: {percentage_place_changes}%\")\n",
    "\n",
    "    # Save the additional stats in a new JSON file inside the video folder\n",
    "    stats_output_path = os.path.join(video_output_dir, f'{additional_info[\"video_title\"]}_stats.json')\n",
    "\n",
    "    try:\n",
    "        with open(stats_output_path, 'w') as f:\n",
    "            json.dump(additional_info, f, indent=4)\n",
    "        print(f\"Additional stats saved to {stats_output_path}\", flush=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving stats to JSON: {e}\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_json_files(json_output_dir):\n",
    "    color_distribution = {\n",
    "        \"Red\": 0,\n",
    "        \"Yellow\": 0,\n",
    "        \"Green\": 0,\n",
    "        \"Blue\": 0,\n",
    "        \"White\": 0,\n",
    "        \"Black\": 0,\n",
    "        \"Non-primary\": 0\n",
    "    }\n",
    "    total_frames = 0\n",
    "    non_compliant_frames = []\n",
    "    total_fantasy_places = 0\n",
    "    total_places = 0\n",
    "\n",
    "    # Iterate over all JSON files in the json_output folder\n",
    "    for json_file in os.listdir(json_output_dir):\n",
    "        if not json_file.endswith('.json'):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(os.path.join(json_output_dir, json_file), 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # Get color data\n",
    "            image_analysis = data.get(\"Image Analysis\", {})\n",
    "            colors_found = image_analysis.get(\"Colors Found\", {})\n",
    "\n",
    "            # Ensure \"Colors Found\" contains the expected structure\n",
    "            for color in [\"Red\", \"Yellow\", \"Green\", \"Blue\", \"White\", \"Black\", \"Non-primary\"]:\n",
    "                color_info = colors_found.get(color, {})\n",
    "                percentage_str = color_info.get(\"Percentage\", \"0%\").replace('%', '')\n",
    "                try:\n",
    "                    color_percentage = float(percentage_str)\n",
    "                    color_distribution[color] += color_percentage\n",
    "                except (ValueError, TypeError):\n",
    "                    print(f\"Invalid color percentage in file {json_file} for color {color}\")\n",
    "                    continue\n",
    "\n",
    "            total_frames += 1\n",
    "\n",
    "            # Check for compliance issues (Suitability)\n",
    "            suitability = image_analysis.get(\"Suitability\", {})\n",
    "            if suitability:  # Make sure suitability exists and is not empty\n",
    "                non_compliant_features = [feature for feature, value in suitability.items() if value]\n",
    "                if non_compliant_features:\n",
    "                    non_compliant_frames.append({\n",
    "                        \"frame\": json_file,\n",
    "                        \"features\": non_compliant_features\n",
    "                    })\n",
    "\n",
    "            # Check for fantasy/adventurous places\n",
    "            place = image_analysis.get(\"Place\", {})\n",
    "            if place.get(\"Certainty Boolean\") == 1:\n",
    "                total_places += 1\n",
    "                if place.get(\"Fantasy/Adventurous Place\") == 1:\n",
    "                    total_fantasy_places += 1\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON in file {json_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {json_file}: {e}\")\n",
    "\n",
    "    # Calculate final stats\n",
    "    overall_color_distribution = {color: (value / total_frames) if total_frames > 0 else 0\n",
    "                                  for color, value in color_distribution.items()}\n",
    "    percentage_fantasy_places = (total_fantasy_places / total_places * 100) if total_places > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"Overall Color Distribution\": overall_color_distribution,\n",
    "        \"Non-Compliant Frames\": non_compliant_frames,\n",
    "        \"Percentage of Fantasy/Adventurous Places\": percentage_fantasy_places\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Situational Analysis (if changes in places between scenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def load_json_data(json_file_path):\n",
    "    with open(json_file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def compare_place_clusters(scene_1_place, scene_2_place, clusters):\n",
    "    \"\"\"\n",
    "    Compare two place names and check if they belong to the same cluster.\n",
    "    Return True if they are in the same cluster, False otherwise.\n",
    "    \"\"\"\n",
    "    place_clusters = clusters.get(\"Places Clusters\", {})\n",
    "    for cluster_name, places in place_clusters.items():\n",
    "        if scene_1_place in places and scene_2_place in places:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def process_scene_comparisons(json_output_dir, clusters, output_registry_path):\n",
    "    \"\"\"\n",
    "    Compare places between consecutive scenes and log the results.\n",
    "    \"\"\"\n",
    "    scene_files = sorted([f for f in os.listdir(json_output_dir) if f.endswith('_analysis.json')])\n",
    "    registry = []\n",
    "    total_comparisons = 0\n",
    "    place_changes = 0\n",
    "\n",
    "    # Loop through consecutive scene files\n",
    "    for i in range(len(scene_files) - 1):\n",
    "        scene_1_file = scene_files[i]\n",
    "        scene_2_file = scene_files[i + 1]\n",
    "\n",
    "        # Load the JSON data\n",
    "        scene_1_data = load_json_data(os.path.join(json_output_dir, scene_1_file))\n",
    "        scene_2_data = load_json_data(os.path.join(json_output_dir, scene_2_file))\n",
    "\n",
    "        # Extract place names\n",
    "        scene_1_place = scene_1_data.get(\"Image Analysis\", {}).get(\"Place\", {}).get(\"Name\", None)\n",
    "        scene_2_place = scene_2_data.get(\"Image Analysis\", {}).get(\"Place\", {}).get(\"Name\", None)\n",
    "\n",
    "        # Skip scenes without a place name\n",
    "        if not scene_1_place or not scene_2_place:\n",
    "            continue\n",
    "\n",
    "        total_comparisons += 1\n",
    "\n",
    "        # Compare place clusters\n",
    "        same_cluster = compare_place_clusters(scene_1_place, scene_2_place, clusters)\n",
    "\n",
    "        # Log the comparison result\n",
    "        registry.append({\n",
    "            \"scene_start\": scene_1_file,\n",
    "            \"scene_end\": scene_2_file,\n",
    "            \"scene_start_place\": scene_1_place,\n",
    "            \"scene_end_place\": scene_2_place,\n",
    "            \"same_cluster\": same_cluster\n",
    "        })\n",
    "\n",
    "        # Count place changes\n",
    "        if not same_cluster:\n",
    "            place_changes += 1\n",
    "\n",
    "    # Calculate the percentage of scene-to-scene place changes\n",
    "    percentage_place_changes = (place_changes / total_comparisons) * 100 if total_comparisons > 0 else 0\n",
    "\n",
    "    # Save the comparison registry to a JSON file\n",
    "    output_data = {\n",
    "        \"comparisons\": registry,\n",
    "        \"total_comparisons\": total_comparisons,\n",
    "        \"place_changes\": place_changes,\n",
    "        \"percentage_place_changes\": percentage_place_changes\n",
    "    }\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    output_registry_dir = os.path.dirname(output_registry_path)\n",
    "    os.makedirs(output_registry_dir, exist_ok=True)\n",
    "\n",
    "    # Save the results\n",
    "    with open(output_registry_path, 'w') as f:\n",
    "        json.dump(output_data, f, indent=4)\n",
    "\n",
    "    print(f\"Registry saved to {output_registry_path}\")\n",
    "    print(f\"Percentage of scene-to-scene place changes: {percentage_place_changes}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#places and chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_character_object_continuity(json_output_dir, clusters, entity_type=\"characters\"):\n",
    "    \"\"\"\n",
    "    Analyze continuity for characters or objects between consecutive JSON files and print the results.\n",
    "    \n",
    "    Args:\n",
    "        json_output_dir (str): The directory where the JSON scene analysis files are stored.\n",
    "        clusters (dict): The clusters from the final summary JSON.\n",
    "        entity_type (str): Either 'characters' or 'objects' to indicate which entities to analyze.\n",
    "\n",
    "    Returns:\n",
    "        None: Only prints the continuity analysis.\n",
    "    \"\"\"\n",
    "    json_files = sorted([f for f in os.listdir(json_output_dir) if f.endswith('.json')])\n",
    "    \n",
    "    total_comparisons = 0\n",
    "    changes = 0\n",
    "\n",
    "    # Loop through consecutive scenes\n",
    "    for i in range(len(json_files) - 1):\n",
    "        scene_1_file = json_files[i]\n",
    "        scene_2_file = json_files[i + 1]\n",
    "\n",
    "        scene_1_data = load_json_data(os.path.join(json_output_dir, scene_1_file))\n",
    "        scene_2_data = load_json_data(os.path.join(json_output_dir, scene_2_file))\n",
    "\n",
    "        if entity_type == \"characters\":\n",
    "            scene_1_entities = scene_1_data.get(\"Image Analysis\", {}).get(\"Characters\", {}).get(\"Character Details\", {})\n",
    "            scene_2_entities = scene_2_data.get(\"Image Analysis\", {}).get(\"Characters\", {}).get(\"Character Details\", {})\n",
    "        else:  # for objects\n",
    "            scene_1_entities = scene_1_data.get(\"Image Analysis\", {}).get(\"Objects\", {}).get(\"Objects Details\", {})\n",
    "            scene_2_entities = scene_2_data.get(\"Image Analysis\", {}).get(\"Objects\", {}).get(\"Objects Details\", {})\n",
    "\n",
    "        total_comparisons += 1\n",
    "        match_found = False\n",
    "\n",
    "        # Compare entities between two scenes\n",
    "        for entity_1_name, entity_1 in scene_1_entities.items():\n",
    "            for entity_2_name, entity_2 in scene_2_entities.items():\n",
    "                cluster_name = compare_clusters(entity_1[\"Name\"], entity_2[\"Name\"], clusters, entity_type)\n",
    "                if cluster_name:\n",
    "                    match_found = True\n",
    "                    print(f\"Matched {entity_type[:-1]} '{entity_1['Name']}' in scene '{scene_1_file}' with '{entity_2['Name']}' in scene '{scene_2_file}' (Cluster: {cluster_name})\")\n",
    "\n",
    "        if not match_found:\n",
    "            changes += 1\n",
    "            print(f\"No {entity_type} match found between scene '{scene_1_file}' and scene '{scene_2_file}'\")\n",
    "\n",
    "    percentage_changes = (changes / total_comparisons) * 100 if total_comparisons > 0 else 0\n",
    "\n",
    "    # Print summary of the analysis\n",
    "    print(f\"\\nSummary for {entity_type}:\")\n",
    "    print(f\"Total comparisons: {total_comparisons}\")\n",
    "    print(f\"{entity_type[:-1].capitalize()} changes: {changes}\")\n",
    "    print(f\"Percentage of {entity_type[:-1]} changes: {percentage_changes:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Main Function Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Videos:   0%|          | 0/2 [00:00<?, ?video/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: Bananas_in_pyjamas copy.mp4\n",
      "Starting scene analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:pyscenedetect:VideoManager is deprecated and will be removed.\n",
      "INFO:pyscenedetect:Loaded 1 video, framerate: 24.969 FPS, resolution: 640 x 360\n",
      "INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 320 x 180\n",
      "INFO:pyscenedetect:Detecting scenes...\n",
      "INFO:root:Detected 3 scenes:\n",
      "INFO:root:Scene 1: Start 00:00:00.000 / Frame 0, End 00:00:08.370 / Frame 209\n",
      "INFO:root:Scene 2: Start 00:00:08.370 / Frame 209, End 00:00:19.344 / Frame 483\n",
      "INFO:root:Scene 3: Start 00:00:19.344 / Frame 483, End 00:00:22.147 / Frame 553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted and saved middle frame of scene 1 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/scenes_output/scene_1_frame_104.jpg\n",
      "Extracted and saved middle frame of scene 2 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/scenes_output/scene_2_frame_346.jpg\n",
      "Extracted and saved middle frame of scene 3 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/scenes_output/scene_3_frame_518.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_1_frame_104.jpg as scene_1_frame_104_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_3_frame_518.jpg as scene_3_frame_518_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Scenes: 100%|██████████| 3/3 [00:13<00:00,  4.53s/scene]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_2_frame_346.jpg as scene_2_frame_346_analysis.json\n",
      "Scene analysis complete.\n",
      "Starting luminance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR:pyscenedetect:VideoManager is deprecated and will be removed.\n",
      "INFO:pyscenedetect:Loaded 1 video, framerate: 24.969 FPS, resolution: 640 x 360\n",
      "INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 320 x 180\n",
      "INFO:pyscenedetect:Detecting scenes...\n",
      "INFO:root:Detected 3 scenes:\n",
      "INFO:root:Scene 1: Start 00:00:00.000 / Frame 0, End 00:00:08.370 / Frame 209\n",
      "INFO:root:Scene 2: Start 00:00:08.370 / Frame 209, End 00:00:19.344 / Frame 483\n",
      "INFO:root:Scene 3: Start 00:00:19.344 / Frame 483, End 00:00:22.147 / Frame 553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total scenes detected: 3\n",
      "Analyzing scenes for flicker and short scene detection...\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/luminance_output/scene_1_to_2.jpg\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/luminance_output/scene_2_to_3_STRONG_LUMINANCE.jpg\n",
      "Luminance analysis complete.\n",
      "Merging entities...\n",
      "Merged entities saved to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/final_summary.json\n",
      "Entity merge complete.\n",
      "Starting edge detection...\n",
      "Saved edge detection image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/edges_output/scene_2_frame_346_edges.jpg\n",
      "Edge density for scene_2_frame_346.jpg: 2.2061631944444446%\n",
      "Saved edge detection image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/edges_output/scene_3_frame_518_edges.jpg\n",
      "Edge density for scene_3_frame_518.jpg: 2.0911458333333335%\n",
      "Saved edge detection image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/edges_output/scene_1_frame_104_edges.jpg\n",
      "Edge density for scene_1_frame_104.jpg: 5.284722222222222%\n",
      "Edge detection complete.\n",
      "Analyzing edge density...\n",
      "Saved edge detection image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/edges_output/scene_2_frame_346_edges.jpg\n",
      "Saved edge detection image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/edges_output/scene_3_frame_518_edges.jpg\n",
      "Saved edge detection image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/edges_output/scene_1_frame_104_edges.jpg\n",
      "Average edge density for all scenes: 3.1940104166666665%\n",
      "Edge density analysis complete.\n",
      "Computing summary statistics...\n",
      "Summary statistics computation complete.\n",
      "Starting place continuity analysis...\n",
      "Registry saved to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/places_continuity/continuity_registry.json\n",
      "Percentage of scene-to-scene place changes: 100.0%\n",
      "Place continuity analysis complete.\n",
      "Starting character continuity analysis...\n",
      "No characters match found between scene 'scene_1_frame_104_analysis.json' and scene 'scene_2_frame_346_analysis.json'\n",
      "No characters match found between scene 'scene_2_frame_346_analysis.json' and scene 'scene_3_frame_518_analysis.json'\n",
      "\n",
      "Summary for characters:\n",
      "Total comparisons: 2\n",
      "Character changes: 2\n",
      "Percentage of character changes: 100.00%\n",
      "Starting object continuity analysis...\n",
      "No objects match found between scene 'scene_1_frame_104_analysis.json' and scene 'scene_2_frame_346_analysis.json'\n",
      "Matched object 'Tree' in scene 'scene_2_frame_346_analysis.json' with 'Tree' in scene 'scene_3_frame_518_analysis.json' (Cluster: Tree)\n",
      "\n",
      "Summary for objects:\n",
      "Total comparisons: 2\n",
      "Object changes: 1\n",
      "Percentage of object changes: 50.00%\n",
      "Saving additional stats...\n",
      "Video title: Bananas_in_pyjamas copy\n",
      "Video size: 784472 bytes\n",
      "Video length: 22.147466666666666 seconds\n",
      "Number of scenes: 3\n",
      "Processing time: 23.301130294799805 seconds\n",
      "Place continuity percentage: 100.0%\n",
      "Additional stats saved to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/Bananas_in_pyjamas copy/Bananas_in_pyjamas copy_stats.json\n",
      "Finished processing video: Bananas_in_pyjamas copy.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Videos:  50%|█████     | 1/2 [00:23<00:23, 23.34s/video]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: shinchantrim.mp4\n",
      "Starting scene analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:pyscenedetect:VideoManager is deprecated and will be removed.\n",
      "INFO:pyscenedetect:Loaded 1 video, framerate: 29.979 FPS, resolution: 442 x 360\n",
      "INFO:pyscenedetect:Detecting scenes...\n",
      "INFO:root:Detected 5 scenes:\n",
      "INFO:root:Scene 1: Start 00:00:00.000 / Frame 0, End 00:00:01.101 / Frame 33\n",
      "INFO:root:Scene 2: Start 00:00:01.101 / Frame 33, End 00:00:02.635 / Frame 79\n",
      "INFO:root:Scene 3: Start 00:00:02.635 / Frame 79, End 00:00:04.803 / Frame 144\n",
      "INFO:root:Scene 4: Start 00:00:04.803 / Frame 144, End 00:00:07.405 / Frame 222\n",
      "INFO:root:Scene 5: Start 00:00:07.405 / Frame 222, End 00:00:07.605 / Frame 228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted and saved middle frame of scene 1 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_1_frame_16.jpg\n",
      "Extracted and saved middle frame of scene 2 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_2_frame_56.jpg\n",
      "Extracted and saved middle frame of scene 3 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_3_frame_111.jpg\n",
      "Extracted and saved middle frame of scene 4 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_4_frame_183.jpg\n",
      "Extracted and saved middle frame of scene 5 as /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/scenes_output/scene_5_frame_225.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_4_frame_183.jpg as scene_4_frame_183_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_3_frame_111.jpg as scene_3_frame_111_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_1_frame_16.jpg as scene_1_frame_16_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_2_frame_56.jpg as scene_2_frame_56_analysis.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Scenes: 100%|██████████| 5/5 [00:09<00:00,  1.97s/scene]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved analysis for scene_5_frame_225.jpg as scene_5_frame_225_analysis.json\n",
      "Scene analysis complete.\n",
      "Starting luminance analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR:pyscenedetect:VideoManager is deprecated and will be removed.\n",
      "INFO:pyscenedetect:Loaded 1 video, framerate: 29.979 FPS, resolution: 442 x 360\n",
      "INFO:pyscenedetect:Detecting scenes...\n",
      "INFO:root:Detected 5 scenes:\n",
      "INFO:root:Scene 1: Start 00:00:00.000 / Frame 0, End 00:00:01.101 / Frame 33\n",
      "INFO:root:Scene 2: Start 00:00:01.101 / Frame 33, End 00:00:02.635 / Frame 79\n",
      "INFO:root:Scene 3: Start 00:00:02.635 / Frame 79, End 00:00:04.803 / Frame 144\n",
      "INFO:root:Scene 4: Start 00:00:04.803 / Frame 144, End 00:00:07.405 / Frame 222\n",
      "INFO:root:Scene 5: Start 00:00:07.405 / Frame 222, End 00:00:07.605 / Frame 228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total scenes detected: 5\n",
      "Analyzing scenes for flicker and short scene detection...\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_1_to_2.jpg\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_2_to_3_STRONG_LUMINANCE.jpg\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_3_to_4.jpg\n",
      "Saved frame comparison image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/luminance_output/scene_4_to_5.jpg\n",
      "Luminance analysis complete.\n",
      "Merging entities...\n",
      "Merged entities saved to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/final_summary.json\n",
      "Entity merge complete.\n",
      "Starting edge detection...\n",
      "Saved edge detection image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/edges_output/scene_3_frame_111_edges.jpg\n",
      "Edge density for scene_3_frame_111.jpg: 6.813725490196078%\n",
      "Saved edge detection image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/edges_output/scene_5_frame_225_edges.jpg\n",
      "Edge density for scene_5_frame_225.jpg: 10.111236802413273%\n",
      "Saved edge detection image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/edges_output/scene_1_frame_16_edges.jpg\n",
      "Edge density for scene_1_frame_16.jpg: 4.985545500251383%\n",
      "Saved edge detection image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/edges_output/scene_4_frame_183_edges.jpg\n",
      "Edge density for scene_4_frame_183.jpg: 7.342885872297638%\n",
      "Saved edge detection image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/edges_output/scene_2_frame_56_edges.jpg\n",
      "Edge density for scene_2_frame_56.jpg: 4.655605832076421%\n",
      "Edge detection complete.\n",
      "Analyzing edge density...\n",
      "Saved edge detection image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/edges_output/scene_3_frame_111_edges.jpg\n",
      "Saved edge detection image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/edges_output/scene_5_frame_225_edges.jpg\n",
      "Saved edge detection image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/edges_output/scene_1_frame_16_edges.jpg\n",
      "Saved edge detection image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/edges_output/scene_4_frame_183_edges.jpg\n",
      "Saved edge detection image to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/edges_output/scene_2_frame_56_edges.jpg\n",
      "Average edge density for all scenes: 6.781799899446957%\n",
      "Edge density analysis complete.\n",
      "Computing summary statistics...\n",
      "Summary statistics computation complete.\n",
      "Starting place continuity analysis...\n",
      "Registry saved to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/places_continuity/continuity_registry.json\n",
      "Percentage of scene-to-scene place changes: 100.0%\n",
      "Place continuity analysis complete.\n",
      "Starting character continuity analysis...\n",
      "No characters match found between scene 'scene_1_frame_16_analysis.json' and scene 'scene_2_frame_56_analysis.json'\n",
      "No characters match found between scene 'scene_2_frame_56_analysis.json' and scene 'scene_3_frame_111_analysis.json'\n",
      "No characters match found between scene 'scene_3_frame_111_analysis.json' and scene 'scene_4_frame_183_analysis.json'\n",
      "No characters match found between scene 'scene_4_frame_183_analysis.json' and scene 'scene_5_frame_225_analysis.json'\n",
      "\n",
      "Summary for characters:\n",
      "Total comparisons: 4\n",
      "Character changes: 4\n",
      "Percentage of character changes: 100.00%\n",
      "Starting object continuity analysis...\n",
      "Matched object 'Character' in scene 'scene_1_frame_16_analysis.json' with 'Character' in scene 'scene_2_frame_56_analysis.json' (Cluster: Character)\n",
      "Matched object 'Character' in scene 'scene_2_frame_56_analysis.json' with 'Character' in scene 'scene_3_frame_111_analysis.json' (Cluster: Character)\n",
      "No objects match found between scene 'scene_3_frame_111_analysis.json' and scene 'scene_4_frame_183_analysis.json'\n",
      "Matched object 'Background' in scene 'scene_4_frame_183_analysis.json' with 'Background' in scene 'scene_5_frame_225_analysis.json' (Cluster: Background)\n",
      "\n",
      "Summary for objects:\n",
      "Total comparisons: 4\n",
      "Object changes: 1\n",
      "Percentage of object changes: 25.00%\n",
      "Saving additional stats...\n",
      "Video title: shinchantrim\n",
      "Video size: 584767 bytes\n",
      "Video length: 7.838736979166667 seconds\n",
      "Number of scenes: 5\n",
      "Processing time: 19.728759050369263 seconds\n",
      "Place continuity percentage: 100.0%\n",
      "Additional stats saved to /Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB/shinchantrim/shinchantrim_stats.json\n",
      "Finished processing video: shinchantrim.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Videos: 100%|██████████| 2/2 [00:43<00:00, 21.56s/video]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED PROCESSING ALL VIDEOS.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import asyncio\n",
    "from tqdm import tqdm\n",
    "\n",
    "async def process_video(video_file, directory_path, output_base_dir, api_key):\n",
    "    try:\n",
    "        # Track the start time\n",
    "        start_time = time.time()\n",
    "\n",
    "        video_path = os.path.join(directory_path, video_file)\n",
    "        video_name = os.path.splitext(video_file)[0]\n",
    "        video_size = os.path.getsize(video_path)\n",
    "\n",
    "        video_output_dir = os.path.join(output_base_dir, video_name)\n",
    "        scenes_output_dir = os.path.join(video_output_dir, 'scenes_output')\n",
    "        json_output_dir = os.path.join(video_output_dir, 'json_output')\n",
    "        luminance_output_dir = os.path.join(video_output_dir, 'luminance_output')\n",
    "\n",
    "        os.makedirs(scenes_output_dir, exist_ok=True)\n",
    "        os.makedirs(json_output_dir, exist_ok=True)\n",
    "        os.makedirs(luminance_output_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"Processing video: {video_file}\", flush=True)\n",
    "\n",
    "        # Analyze video for scenes\n",
    "        print(\"Starting scene analysis...\", flush=True)\n",
    "        scenes = analyze_video(video_path)\n",
    "        extract_frames_imageio(video_path, scenes, scenes_output_dir)\n",
    "        await process_scenes_output(scenes_output_dir, json_output_dir)  # Ensure this finishes\n",
    "        print(\"Scene analysis complete.\", flush=True)\n",
    "\n",
    "        # Perform luminance analysis and get the results\n",
    "        print(\"Starting luminance analysis...\", flush=True)\n",
    "        flicker_and_short_scene_stats = process_video_with_flicker_analysis(video_path, luminance_output_dir)\n",
    "        print(\"Luminance analysis complete.\", flush=True)\n",
    "\n",
    "        # Merge entities\n",
    "        final_json_path = os.path.join(json_output_dir, 'final_entities.json')\n",
    "        print(\"Merging entities...\", flush=True)\n",
    "        merged_final_entities = await cluster_and_merge_entities(api_key, json_output_dir, video_output_dir)\n",
    "\n",
    "        if merged_final_entities is None:\n",
    "            print(f\"Error: Failed to merge entities for video {video_file}\")\n",
    "            return\n",
    "        print(\"Entity merge complete.\", flush=True)\n",
    "\n",
    "        # Create the edges_output directory and process each scene for edge detection\n",
    "        edges_output_dir = os.path.join(video_output_dir, 'edges_output')\n",
    "        os.makedirs(edges_output_dir, exist_ok=True)\n",
    "        print(\"Starting edge detection...\", flush=True)\n",
    "        save_edge_detection_images(scenes_output_dir, edges_output_dir)\n",
    "        print(\"Edge detection complete.\", flush=True)\n",
    "\n",
    "        # Compute edge density statistics\n",
    "        print(\"Analyzing edge density...\", flush=True)\n",
    "        edge_density_stats = analyze_edge_density_for_scenes(scenes_output_dir, edges_output_dir)\n",
    "        print(\"Edge density analysis complete.\", flush=True)\n",
    "\n",
    "        # Compute summary statistics\n",
    "        print(\"Computing summary statistics...\", flush=True)\n",
    "        summary_statistics = compute_summary_statistics(merged_final_entities)\n",
    "        print(\"Summary statistics computation complete.\", flush=True)\n",
    "\n",
    "        # Track the end time\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Scene comparison step (new addition)\n",
    "        clusters_file = os.path.join(video_output_dir, 'final_summary.json')\n",
    "\n",
    "        # Ensure final_summary.json exists\n",
    "        if os.path.exists(clusters_file):\n",
    "            clusters = load_json_data(clusters_file)\n",
    "            \n",
    "            # Analyze place continuity\n",
    "            print(\"Starting place continuity analysis...\", flush=True)\n",
    "            process_scene_comparisons(json_output_dir, clusters, os.path.join(video_output_dir, 'places_continuity', 'continuity_registry.json'))\n",
    "            continuity_data = load_json_data(os.path.join(video_output_dir, 'places_continuity', 'continuity_registry.json'))\n",
    "            percentage_place_changes = continuity_data.get('percentage_place_changes', 0)\n",
    "            print(\"Place continuity analysis complete.\", flush=True)\n",
    "            \n",
    "            # Analyze character continuity (prints results)\n",
    "            print(\"Starting character continuity analysis...\", flush=True)\n",
    "            analyze_character_object_continuity(json_output_dir, clusters, entity_type=\"characters\")\n",
    "            \n",
    "            # Analyze object continuity (prints results)\n",
    "            print(\"Starting object continuity analysis...\", flush=True)\n",
    "            analyze_character_object_continuity(json_output_dir, clusters, entity_type=\"objects\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"Error: {clusters_file} does not exist. Skipping scene comparison.\")\n",
    "            percentage_place_changes = None\n",
    "\n",
    "        # Ensure save_additional_stats gets the expected data\n",
    "        print(\"Saving additional stats...\", flush=True)\n",
    "        save_additional_stats(\n",
    "            video_path, scenes_output_dir, start_time, end_time, video_output_dir, json_output_dir,\n",
    "            summary_statistics, scenes, edges_output_dir, flicker_and_short_scene_stats, edge_density_stats,\n",
    "            percentage_place_changes  # Make sure place changes are passed\n",
    "        )\n",
    "        print(f\"Finished processing video: {video_file}\", flush=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video {video_file}: {e}\", flush=True)\n",
    "\n",
    "async def process_videos_in_directory(directory_path, output_base_dir, api_key):\n",
    "    video_files = [f for f in os.listdir(directory_path) if f.endswith(('.mp4', '.avi', '.mkv'))]\n",
    "\n",
    "    if not video_files:\n",
    "        print(\"No video files found in the directory.\", flush=True)\n",
    "        return\n",
    "\n",
    "    with tqdm(total=len(video_files), desc=\"Processing Videos\", unit=\"video\") as pbar:\n",
    "        for video_file in video_files:\n",
    "            await process_video(video_file, directory_path, output_base_dir, api_key)\n",
    "            pbar.update(1)\n",
    "\n",
    "# Ensure the main script has appropriate paths\n",
    "video_directory = '/Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/02_Video_DB'\n",
    "output_base_directory = '/Users/santiagowon/Dropbox/Santiago/01. Maestria/Tesis/11_Project_Analysed_DB'\n",
    "\n",
    "# Run the async function (sequential processing)\n",
    "asyncio.run(process_videos_in_directory(video_directory, output_base_directory, api_key))\n",
    "print(\"FINISHED PROCESSING ALL VIDEOS.\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test for Chars and Objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "10_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
